"""
Report generator for TDXAgent.

This module generates comprehensive markdown reports from processed
social media data and LLM analysis results.
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone
import logging

from utils.logger import TDXLogger
from utils.helpers import ensure_directory_async, format_timestamp, sanitize_filename
from processors.batch_processor import BatchResult


class ReportGenerator:
    """
    Comprehensive report generator for TDXAgent.
    
    Features:
    - Markdown report generation
    - Platform-specific sections
    - Summary statistics
    - Media file references
    - Customizable templates
    - Multi-language support
    """
    
    def __init__(self, output_directory: str = "TDXAgent_Data/reports", start_time: Optional[datetime] = None, hours_back: Optional[int] = None, config_manager=None):
        """
        Initialize report generator.
        
        Args:
            output_directory: Directory to save reports
            start_time: Program start time for filename generation
            hours_back: Hours of data collected for filename generation
            config_manager: Configuration manager instance (optional)
        """
        self.output_directory = Path(output_directory)
        # Use UTC+8 timezone for start_time
        from datetime import timedelta
        utc_plus_8 = timezone(timedelta(hours=8))
        self.start_time = start_time or datetime.now(utc_plus_8)
        self.hours_back = hours_back
        self.logger = TDXLogger.get_logger("tdxagent.processors.reports")
        
        # Store config manager for accessing configuration
        self.config_manager = config_manager
        
        # Report templates
        self.templates = {
            'summary': self._get_summary_template(),
            'detailed': self._get_detailed_template(),
            'platform_specific': self._get_platform_template()
        }
        
        self.logger.info(f"Initialized report generator: {self.output_directory}")
    
    
    def _generate_filename(self, prefix: str) -> str:
        """
        Generate human-readable filename.
        
        Args:
            prefix: File prefix (e.g., "TDXAgentç»¼åˆæŠ¥å‘Š", "TwitteræŠ¥å‘Š")
            
        Returns:
            Generated filename with human-readable format
        """
        # Format date for human readability
        date_str = self.start_time.strftime("%Yå¹´%mæœˆ%dæ—¥")
        time_str = self.start_time.strftime("%Hæ—¶%Måˆ†")
        
        # Generate hours description
        if self.hours_back:
            if self.hours_back == 1:
                duration_str = "1å°æ—¶"
            elif self.hours_back == 24:
                duration_str = "1å¤©"  
            elif self.hours_back == 168:  # 7*24
                duration_str = "1å‘¨"
            elif self.hours_back < 24:
                duration_str = f"{self.hours_back}å°æ—¶"
            elif self.hours_back % 24 == 0:
                days = self.hours_back // 24
                duration_str = f"{days}å¤©"
            else:
                duration_str = f"{self.hours_back}å°æ—¶"
        else:
            duration_str = "æœªçŸ¥æ—¶é•¿"
        
        # Generate human-readable filename
        filename = f"{prefix}_{date_str}_{time_str}_{duration_str}.md"
        return filename
    
    def _write_file(self, file_path: Path, content: str) -> None:
        """Write content to file synchronously."""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    async def generate_unified_report(self, 
                                    batch_result: BatchResult,
                                    platforms_included: List[str],
                                    total_messages: int,
                                    start_time: Optional[datetime] = None,
                                    hours_back: Optional[int] = None,
                                    data_file_paths: Optional[Dict[str, str]] = None) -> str:
        """
        Generate a unified multi-platform report from unified analysis results.
        
        Args:
            batch_result: Unified analysis batch processing result
            platforms_included: List of platforms included in analysis
            total_messages: Total number of messages analyzed
            start_time: Program start time (overrides instance value if provided)
            hours_back: Hours of data collected (overrides instance value if provided)
            data_file_paths: Dict mapping platform to absolute data file path
            
        Returns:
            Path to generated unified report file
        """
        try:
            # Update timing parameters if provided
            if start_time:
                self.start_time = start_time
            if hours_back is not None:
                self.hours_back = hours_back
                
            await ensure_directory_async(self.output_directory)
            
            # Generate unified report content
            report_content = self._generate_unified_content(batch_result, platforms_included, total_messages, data_file_paths)
            
            # Create filename with unified format
            filename = self._generate_filename("TDXAgentåˆ†ææŠ¥å‘Š")
            
            # Create date-based subdirectory
            report_date = datetime.now()
            date_folder = report_date.strftime("%Y%m%d")
            date_directory = self.output_directory / date_folder
            await ensure_directory_async(date_directory)
            
            report_path = date_directory / filename
            
            # Write report
            await asyncio.to_thread(self._write_file, report_path, report_content)
            
            self.logger.info(f"Generated unified multi-platform report: {report_path}")
            return str(report_path)
            
        except Exception as e:
            self.logger.error(f"Failed to generate unified report: {e}")
            raise
    
    async def generate_error_report(self, 
                                  error_message: str,
                                  platforms_attempted: List[str],
                                  total_messages: int,
                                  start_time: Optional[datetime] = None,
                                  hours_back: Optional[int] = None,
                                  analysis_result: Optional = None) -> str:
        """
        Generate an error report when analysis fails.
        
        Args:
            error_message: The error that occurred
            platforms_attempted: List of platforms that were attempted
            total_messages: Total number of messages that were collected
            start_time: Program start time (overrides instance value if provided)
            hours_back: Hours of data collected (overrides instance value if provided)
            analysis_result: Any partial analysis result for debugging (optional)
            
        Returns:
            Path to generated error report file
        """
        try:
            # Update timing parameters if provided
            if start_time:
                self.start_time = start_time
            if hours_back is not None:
                self.hours_back = hours_back
                
            await ensure_directory_async(self.output_directory)
            
            # Generate error report content
            report_content = self._generate_error_content(
                error_message, platforms_attempted, total_messages, analysis_result
            )
            
            # Create filename with error format
            filename = self._generate_filename("åˆ†æé”™è¯¯æŠ¥å‘Š")
            
            # Create date-based subdirectory
            report_date = datetime.now()
            date_folder = report_date.strftime("%Y%m%d")
            date_directory = self.output_directory / date_folder
            await ensure_directory_async(date_directory)
            
            report_path = date_directory / filename
            
            # Write report
            await asyncio.to_thread(self._write_file, report_path, report_content)
            
            self.logger.info(f"Generated error report: {report_path}")
            return str(report_path)
            
        except Exception as e:
            self.logger.error(f"Failed to generate error report: {e}")
            # Don't raise here - we don't want to fail the error report generation
            return None
    
    def _generate_unified_content(self, 
                                 batch_result: BatchResult,
                                 platforms_included: List[str],
                                 total_messages: int,
                                 data_file_paths: Optional[Dict[str, str]] = None) -> str:
        """Generate unified multi-platform report content."""
        
        report_time = format_timestamp(format_type="human")
        
        # Get platform display names
        platform_names = [self._get_platform_display_name(p) for p in platforms_included]
        platforms_display = "ã€".join(platform_names)
        
        # Generate prompt files section - è·å–æç¤ºè¯æ–‡ä»¶è·¯å¾„
        prompt_files_section = self._generate_prompt_files_section(platforms_included, batch_result)
        
        content = f"""
**ç”Ÿæˆæ—¶é—´**: {report_time}  
**æ•°æ®èŒƒå›´**: æœ€è¿‘ {self.hours_back or 'N/A'} å°æ—¶  
**åˆ†ææ¨¡å¼**: ç»Ÿä¸€å¤šå¹³å°åˆ†æ

## ğŸ“Š å¤„ç†ç»Ÿè®¡

- **æ¶‰åŠå¹³å°**: {platforms_display}
- **æ€»æ¶ˆæ¯æ•°**: {total_messages:,}
- **æˆåŠŸå¤„ç†**: {batch_result.processed_messages:,} ({batch_result.success_rate:.1f}%)
- **æ‰¹æ¬¡å¤„ç†**: {batch_result.successful_batches} æˆåŠŸ / {batch_result.failed_batches} å¤±è´¥
- **Token ä½¿ç”¨**: {batch_result.total_tokens_used:,} (å¹³å‡ {batch_result.average_tokens_per_batch:.0f}/æ‰¹æ¬¡)
- **å¤„ç†æ—¶é—´**: {batch_result.processing_time:.1f} ç§’
- **ä¼°ç®—æˆæœ¬**: ${batch_result.total_cost:.4f}

### ğŸ“„ æç¤ºè¯æ–‡ä»¶è·¯å¾„
{prompt_files_section}

## ğŸ¤– ç»Ÿä¸€AIåˆ†æç»“æœ

"""
        
        # Add unified AI analysis results
        if batch_result.summaries:
            content += f"""### ç»Ÿä¸€å¤šå¹³å°æŠ•èµ„åˆ†æ

"""
            # æ˜¾ç¤ºç»Ÿä¸€åˆ†ææ‘˜è¦
            for summary in batch_result.summaries:
                content += f"{summary}\n\n"
        else:
            content += "æš‚æ—  AI åˆ†æç»“æœã€‚\n\n"
        
        
        # Add simple footer
        content += f"""*æŠ¥å‘Šç”± TDXAgent è‡ªåŠ¨ç”Ÿæˆ - {report_time}*
"""
        
        return content
    
    def _generate_prompt_files_section(self, platforms_included: List[str], batch_result=None) -> str:
        """ç”Ÿæˆæç¤ºè¯æ–‡ä»¶è·¯å¾„ç« èŠ‚"""
        try:
            # ç›´æ¥ä»BatchResultè·å–æ”¶é›†å¥½çš„æç¤ºè¯æ–‡ä»¶è·¯å¾„
            prompt_file_paths = getattr(batch_result, 'prompt_file_paths', [])
            
            if prompt_file_paths:
                # ç”Ÿæˆç»Ÿä¸€æ•°æ®ä½ç½®è¯´æ˜
                platform_names = [self._get_platform_display_name(p) for p in platforms_included]
                platforms_display = "ã€".join(platform_names)
                
                sections = []
                sections.append("**æœ¬æ¬¡åˆ†æä½¿ç”¨çš„æç¤ºè¯æ•°æ®æ–‡ä»¶**:")
                sections.append("")
                
                # æŒ‰å¹³å°åˆ†ç»„å’Œæ–‡ä»¶ååˆ†ç±»æ˜¾ç¤ºï¼šå¹³å°ç‹¬ç«‹åˆ†æ + æ•´åˆåˆ†æ
                platform_files_grouped = {}  # {platform: [paths]}
                integration_files = []
                
                for path in prompt_file_paths:
                    filename = os.path.basename(path)
                    if 'integration_analysis' in filename:
                        integration_files.append(path)
                    else:
                        # ä»æ–‡ä»¶åæå–å¹³å°ä¿¡æ¯
                        if 'twitter_analysis' in filename:
                            platform = 'twitter'
                            platform_name = 'ğŸ¦ Twitter/X'
                        elif 'telegram_analysis' in filename:
                            platform = 'telegram'
                            platform_name = 'âœˆï¸ Telegram'
                        elif 'discord_analysis' in filename:
                            platform = 'discord'
                            platform_name = 'ğŸ’¬ Discord'
                        elif 'gmail_analysis' in filename:
                            platform = 'gmail'
                            platform_name = 'ğŸ“§ Gmail'
                        else:
                            platform = 'unknown'
                            platform_name = 'ğŸ“„ æœªçŸ¥å¹³å°'
                        
                        if platform not in platform_files_grouped:
                            platform_files_grouped[platform] = {'name': platform_name, 'paths': []}
                        platform_files_grouped[platform]['paths'].append(path)
                
                # å¯¹æ¯ä¸ªå¹³å°çš„æ–‡ä»¶æŒ‰è·¯å¾„åæ’åºï¼ˆç¡®ä¿æ‰¹æ¬¡é¡ºåºæ­£ç¡®ï¼‰
                for platform_data in platform_files_grouped.values():
                    platform_data['paths'].sort()
                
                # æ˜¾ç¤ºå„å¹³å°ç‹¬ç«‹åˆ†æçš„æç¤ºè¯æ–‡ä»¶ï¼ˆæŒ‰å¹³å°åˆ†ç»„ï¼‰
                if platform_files_grouped:
                    sections.append("**å„å¹³å°ç‹¬ç«‹åˆ†ææ–‡ä»¶**:")
                    for platform, data in platform_files_grouped.items():
                        platform_name = data['name']
                        paths = data['paths']
                        
                        if len(paths) == 1:
                            # å•æ‰¹æ¬¡ï¼šç›´æ¥æ˜¾ç¤º
                            sections.append(f"- {platform_name}: `{paths[0]}`")
                        else:
                            # å¤šæ‰¹æ¬¡ï¼šåˆ†æ‰¹æ¬¡æ˜¾ç¤º
                            sections.append(f"- {platform_name}:")
                            for i, path in enumerate(paths, 1):
                                sections.append(f"  - æ‰¹æ¬¡{i}: `{path}`")
                    sections.append("")
                
                # æ˜¾ç¤ºæ•´åˆåˆ†æçš„æç¤ºè¯æ–‡ä»¶
                if integration_files:
                    sections.append("**ç»Ÿä¸€æ•´åˆåˆ†ææ–‡ä»¶**:")
                    for path in integration_files:
                        sections.append(f"- ğŸ¤– æ•´åˆåˆ†æ: `{path}`")
                    sections.append("")
                
                sections.append("**ğŸ¯ åŸå§‹æ•°æ®å¼•ç”¨è§£ææŒ‡å¼•**:")
                sections.append("AIå¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå·¥å…·ç²¾ç¡®å®šä½å’Œæå–åŸå§‹æ•°æ®å†…å®¹ï¼š")
                sections.append("")
                sections.append("**1. Telegramç¾¤ç»„å®Œæ•´æå–**:")
                sections.append("- AIå¼•ç”¨æ ¼å¼: `xxxç¾¤ç»„ @ç”¨æˆ·å 06:00` æˆ– `xxxç¾¤ç»„ å¤šç”¨æˆ·è®¨è®º 06:28-06:33`")
                sections.append("- å®Œæ•´ç¾¤ç»„æå–: `sed -n '/^## ç¾¤ç»„å.*ç¾¤ç»„è®¨è®º$/,/^## /p' æç¤ºè¯æ–‡ä»¶è·¯å¾„`")
                sections.append("")
                sections.append("**2. æ•°æ®åŒºåŸŸè¯†åˆ«**:")
                sections.append("- æ•°æ®ä½ç½®: åœ¨æç¤ºè¯æ–‡ä»¶ä¸­æœç´¢ `<analysis_data>` æ ‡ç­¾")
                sections.append("- ç¾¤ç»„ç»“æ„: æ•°æ®æŒ‰ `## ç¾¤ç»„å ç¾¤ç»„è®¨è®º` åˆ†ç»„ç»„ç»‡")
                sections.append("- åºå·ç¼–æ’: æ¯ä¸ªç¾¤ç»„å†…æ¶ˆæ¯ä»1å¼€å§‹æŒ‰åºå·ç¼–æ’")
                
                return "\n".join(sections)
            else:
                return "æ— æ³•è·å–æç¤ºè¯æ–‡ä»¶è·¯å¾„ä¿¡æ¯"
                
        except Exception as e:
            return f"è·å–æç¤ºè¯æ–‡ä»¶è·¯å¾„å¤±è´¥: {str(e)}"
    
    
    def _get_platform_display_name(self, platform: str) -> str:
        """Get display name for platform."""
        names = {
            'twitter': 'ğŸ¦ Twitter/X',
            'telegram': 'âœˆï¸ Telegram',
            'discord': 'ğŸ’¬ Discord',
            'gmail': 'ğŸ“§ Gmail'
        }
        return names.get(platform.lower(), platform.title())
    
    def _generate_error_content(self, 
                               error_message: str,
                               platforms_attempted: List[str],
                               total_messages: int,
                               analysis_result = None) -> str:
        """Generate error report content."""
        
        report_time = format_timestamp(format_type="human")
        
        # Get platform display names
        if platforms_attempted:
            platform_names = [self._get_platform_display_name(p) for p in platforms_attempted]
            platforms_display = "ã€".join(platform_names)
        else:
            platforms_display = "æ— "
        
        content = f"""# TDXAgent åˆ†æé”™è¯¯æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {report_time}

## æ‰§è¡Œä¿¡æ¯
- **åˆ†æå¹³å°**: {platforms_display}
- **æ”¶é›†æ¶ˆæ¯**: {total_messages:,} æ¡
- **æ•°æ®çŠ¶æ€**: âœ… å·²æ”¶é›†å®Œæˆ

## é”™è¯¯æ—¥å¿—
```
{error_message}
```

## å¤„ç†ç»“æœ
- **æ•°æ®æ”¶é›†**: âœ… æˆåŠŸ ({total_messages:,} æ¡æ¶ˆæ¯å·²ä¿å­˜)
- **AIåˆ†æ**: âŒ å¤±è´¥
- **æŠ¥å‘Šç”Ÿæˆ**: âŒ æœªå®Œæˆ

## é‡æ–°åˆ†æ
è§£å†³é”™è¯¯åè¿è¡Œ: `python src/main.py analyze --hours {self.hours_back or 12}`

"""
        
        
        content += f"*{report_time} - TDXAgent*"
        
        return content
    
    def _get_summary_template(self) -> str:
        """Get summary report template."""
        return """# {title}

**ç”Ÿæˆæ—¶é—´**: {timestamp}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

{overall_stats}

## ğŸŒ å¹³å°æ¦‚è§ˆ

{platform_sections}

## âš ï¸ é”™è¯¯æŠ¥å‘Š

{error_summary}

---

*æŠ¥å‘Šç”± TDXAgent è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    def _get_detailed_template(self) -> str:
        """Get detailed report template."""
        return """# {platform} è¯¦ç»†åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}

## ğŸ“ˆ å¤„ç†ç»Ÿè®¡

{processing_stats}

## ğŸ¤– AI åˆ†æç»“æœ

{ai_summaries}

## ğŸ“ æ¶ˆæ¯æ ·æœ¬

{message_samples}

## âš ï¸ é”™è¯¯è¯¦æƒ…

{error_details}

---

*æŠ¥å‘Šç”± TDXAgent è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    def _get_platform_template(self) -> str:
        """Get platform-specific template."""
        return self._get_detailed_template()
    
    async def list_reports(self) -> List[Dict[str, Any]]:
        """
        List all generated reports.
        
        Returns:
            List of report information
        """
        reports = []
        
        try:
            if not self.output_directory.exists():
                return reports
            
            for report_file in self.output_directory.glob("*.md"):
                try:
                    stat = report_file.stat()
                    reports.append({
                        'filename': report_file.name,
                        'path': str(report_file),
                        'size': stat.st_size,
                        'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                    })
                except Exception as e:
                    self.logger.warning(f"Failed to get info for {report_file}: {e}")
            
            # Sort by creation time (newest first)
            reports.sort(key=lambda x: x['created'], reverse=True)
            
        except Exception as e:
            self.logger.error(f"Failed to list reports: {e}")
        
        return reports
    
    async def cleanup_old_reports(self, days_old: int = 30) -> int:
        """
        Clean up old report files.
        
        Args:
            days_old: Remove reports older than this many days
            
        Returns:
            Number of files removed
        """
        removed_count = 0
        cutoff_time = datetime.now().timestamp() - (days_old * 24 * 60 * 60)
        
        try:
            if not self.output_directory.exists():
                return 0
            
            for report_file in self.output_directory.glob("*.md"):
                try:
                    if report_file.stat().st_mtime < cutoff_time:
                        report_file.unlink()
                        removed_count += 1
                        self.logger.debug(f"Removed old report: {report_file}")
                except Exception as e:
                    self.logger.warning(f"Failed to remove {report_file}: {e}")
            
            self.logger.info(f"Cleaned up {removed_count} old reports")
            
        except Exception as e:
            self.logger.error(f"Failed to cleanup reports: {e}")
        
        return removed_count
    
    def _add_batch_execution_details(self, content: str, platform_results: Dict[str, Any]) -> str:
        """
        æ·»åŠ æ‰¹æ¬¡æ‰§è¡Œè¯¦æƒ…åˆ°æŠ¥å‘Šå†…å®¹ä¸­
        
        Args:
            content: ç°æœ‰çš„æŠ¥å‘Šå†…å®¹
            platform_results: å¹³å°å¤„ç†ç»“æœå­—å…¸
            
        Returns:
            æ›´æ–°åçš„æŠ¥å‘Šå†…å®¹
        """
        # å¯¼å…¥BatchProcessorä»¥ä½¿ç”¨get_execution_summaryæ–¹æ³•
        from processors.batch_processor import BatchProcessor
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ‰é—®é¢˜çš„æ‰¹æ¬¡
        all_problematic_batches = []
        platform_summaries = {}
        
        for platform, result_data in platform_results.items():
            if isinstance(result_data, dict) and 'batch_result' in result_data:
                batch_result = result_data['batch_result']
                
                # æ£€æŸ¥BatchResultæ˜¯å¦æœ‰batch_detailså±æ€§ï¼ˆæ–°ç‰ˆæœ¬æ‰æœ‰ï¼‰
                if hasattr(batch_result, 'batch_details') and batch_result.batch_details:
                    problematic_batches = batch_result.problematic_batches
                    if problematic_batches:
                        all_problematic_batches.extend(problematic_batches)
                        # ç›´æ¥è°ƒç”¨BatchProcessorçš„é™æ€æ–¹æ³•æˆ–åˆ›å»ºç®€åŒ–çš„summary
                        platform_summaries[platform] = self._create_execution_summary(batch_result)
        
        # å¦‚æœæ²¡æœ‰é—®é¢˜æ‰¹æ¬¡ï¼Œè¿”å›åŸå†…å®¹
        if not all_problematic_batches:
            return content
        
        # æ·»åŠ æ‰¹æ¬¡æ‰§è¡Œè¯¦æƒ…ç« èŠ‚
        execution_details = f"""

## ğŸ” æ‰¹æ¬¡æ‰§è¡Œè¯¦æƒ…

æ£€æµ‹åˆ° {len(all_problematic_batches)} ä¸ªæœ‰é—®é¢˜çš„æ‰¹æ¬¡ï¼Œè¯¦æƒ…å¦‚ä¸‹ï¼š

"""
        
        for platform, summary in platform_summaries.items():
            platform_name = self._get_platform_display_name(platform)
            execution_details += f"### {platform_name}\n\n"
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            execution_details += f"""**æ‰§è¡Œç»Ÿè®¡**:
- æ€»æ‰¹æ¬¡æ•°: {summary['total_batches']}
- æˆåŠŸæ‰¹æ¬¡: {summary['successful_batches']}
- å¤±è´¥æ‰¹æ¬¡: {summary['failed_batches']}
- é—®é¢˜æ‰¹æ¬¡: {summary['problematic_count']}

"""
            
            # å¤±è´¥æ‰¹æ¬¡è¯¦æƒ…
            if summary['failed_details']:
                execution_details += "**ğŸš¨ å¤±è´¥æ‰¹æ¬¡è¯¦æƒ…**:\n\n"
                for detail in summary['failed_details']:
                    execution_details += f"""- **æ‰¹æ¬¡ {detail['batch_number']}**: 
  - LLMå‘½ä»¤: `{detail['llm_command']}`
  - æ¶ˆæ¯æ•°: {detail['message_count']} æ¡
  - å¤„ç†æ—¶é—´: {detail['processing_time']:.2f}s
  - é”™è¯¯ä¿¡æ¯: {detail['error_message']}

"""
            
            # æ— æ•ˆç»“æœæ‰¹æ¬¡è¯¦æƒ…
            if summary['empty_result_details']:
                execution_details += "**âš ï¸ æ— æœ‰æ•ˆç»“æœæ‰¹æ¬¡**:\n\n"
                for detail in summary['empty_result_details']:
                    execution_details += f"""- **æ‰¹æ¬¡ {detail['batch_number']}**: 
  - LLMå‘½ä»¤: `{detail['llm_command']}`
  - æ¶ˆæ¯æ•°: {detail['message_count']} æ¡
  - Tokenä½¿ç”¨: {detail['tokens_used']}
  - å¤„ç†æ—¶é—´: {detail['processing_time']:.2f}s
  - å“åº”é¢„è§ˆ: {detail['response_preview']}

"""
            
            execution_details += "---\n\n"
        
        # æ·»åŠ è°ƒè¯•å»ºè®®
        execution_details += """**ğŸ› ï¸ è°ƒè¯•å»ºè®®**:

1. **å¤±è´¥æ‰¹æ¬¡**: æ£€æŸ¥ç½‘ç»œè¿æ¥ã€APIé…é¢æˆ–æ¨¡å‹å¯ç”¨æ€§
2. **æ— æ•ˆç»“æœæ‰¹æ¬¡**: 
   - æ£€æŸ¥è¾“å…¥æ•°æ®è´¨é‡å’Œç›¸å…³æ€§
   - è€ƒè™‘è°ƒæ•´æç¤ºè¯æ¨¡æ¿
   - éªŒè¯æ¶ˆæ¯æ—¶é—´èŒƒå›´æ˜¯å¦åŒ…å«ç›¸å…³å†…å®¹
3. **é‡æ–°è¿è¡Œ**: å¯ä»¥å°è¯•é‡æ–°è¿è¡Œç›¸åŒæ—¶é—´èŒƒå›´çš„åˆ†æ

"""
        
        return content + execution_details
    
    def _has_execution_details_support(self, batch_result) -> bool:
        """
        æ£€æŸ¥BatchResultæ˜¯å¦æ”¯æŒbatch_detailsï¼ˆå‘åå…¼å®¹æ€§æ£€æŸ¥ï¼‰
        
        Args:
            batch_result: BatchResultå®ä¾‹
            
        Returns:
            æ˜¯å¦æ”¯æŒè¯¦ç»†æ‰§è¡Œä¿¡æ¯
        """
        return hasattr(batch_result, 'batch_details') and batch_result.batch_details is not None
    
    def _create_execution_summary(self, batch_result) -> dict:
        """
        åˆ›å»ºæ‰¹æ¬¡æ‰§è¡Œæ±‡æ€»ä¿¡æ¯ï¼ˆç‹¬ç«‹äºBatchProcessorçš„å®ç°ï¼‰
        
        Args:
            batch_result: BatchResultå®ä¾‹
            
        Returns:
            åŒ…å«æ‰§è¡Œæ±‡æ€»çš„å­—å…¸
        """
        if not hasattr(batch_result, 'batch_details') or not batch_result.batch_details:
            return {}
        
        problematic_batches = batch_result.problematic_batches
        failed_batches = batch_result.failed_batch_details
        empty_batches = batch_result.empty_result_batches
        
        return {
            'platform': batch_result.platform,
            'total_batches': batch_result.successful_batches + batch_result.failed_batches,
            'successful_batches': batch_result.successful_batches,
            'failed_batches': batch_result.failed_batches,
            'problematic_count': len(problematic_batches),
            'failed_details': [
                {
                    'batch_number': detail.batch_number,
                    'llm_command': getattr(detail, 'llm_command', detail.command_type),
                    'message_count': detail.message_count,
                    'error_message': detail.error_message,
                    'processing_time': detail.processing_time
                }
                for detail in failed_batches
            ],
            'empty_result_details': [
                {
                    'batch_number': detail.batch_number,
                    'llm_command': getattr(detail, 'llm_command', detail.command_type),
                    'message_count': detail.message_count,
                    'tokens_used': detail.tokens_used,
                    'processing_time': detail.processing_time,
                    'response_preview': detail.response_content[:100] + "..." if len(detail.response_content) > 100 else detail.response_content
                }
                for detail in empty_batches
            ]
        }
    
