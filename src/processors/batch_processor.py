"""
Intelligent batch processor for LLM requests in TDXAgent.

This module provides efficient batch processing of messages for LLM analysis,
with intelligent token management and optimization strategies.
"""

import asyncio
import math
import re
import time
from typing import List, Dict, Any, Optional, Callable, Tuple
from datetime import datetime
from dataclasses import dataclass
import logging

from utils.logger import TDXLogger, log_async_function_call
from llm.base_provider import BaseLLMProvider, LLMResponse
from processors.prompt_manager import PromptTemplate


@dataclass
class BatchExecutionDetail:
    """è¯¦ç»†çš„æ‰¹æ¬¡æ‰§è¡Œä¿¡æ¯"""
    batch_number: int
    message_count: int
    tokens_used: int
    processing_time: float
    success: bool
    error_message: str = ""
    response_content: str = ""
    command_type: str = ""  # å‡½æ•°åï¼Œå¦‚ "process_messages_with_template"
    llm_command: str = ""   # å…·ä½“çš„LLMè°ƒç”¨å‘½ä»¤ï¼Œå¦‚ "gemini -y < xxx_prompt.txt" æˆ– "APIè°ƒç”¨: gpt-4"
    llm_provider: str = ""  # LLMæä¾›å•†ï¼Œå¦‚ "claude_cli", "openai", "gemini_cli"
    model_name: str = ""    # æ¨¡å‹åç§°ï¼Œå¦‚ "gpt-4", "claude-3-sonnet"
    
    @property
    def has_meaningful_content(self) -> bool:
        """åŸºäºæ¨¡æ¿æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹"""
        if not self.success or not self.response_content:
            return False
        
        # ä½¿ç”¨åŸºäºæ¨¡æ¿çš„è´¨é‡æ£€æµ‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        try:
            from processors.template_quality_checker import get_quality_checker
            
            # è·å–è´¨é‡æ£€æµ‹å™¨ï¼Œä¸éœ€è¦PromptManager
            quality_checker = get_quality_checker()
            
            # ç›´æ¥è¿›è¡ŒåŒæ­¥è´¨é‡æ£€æŸ¥
            result = quality_checker.is_valid_response(
                content=self.response_content,
                template_name='pure_investment_analysis',
                min_sections_required=1  # è‡³å°‘åŒ…å«ä¸€ä¸ªç« èŠ‚å°±è®¤ä¸ºæœ‰æ•ˆ
            )
            
            return result.is_valid
            
        except Exception as e:
            # å¦‚æœè´¨é‡æ£€æµ‹å‡ºé”™ï¼Œä½¿ç”¨å¢å¼ºçš„fallbackæ£€æµ‹
            import logging
            logging.getLogger("tdxagent.batch_processor").warning(
                f"Template-based quality check failed, using enhanced fallback: {e}"
            )
            
            # å¢å¼ºçš„fallback: æ£€æŸ¥å†…å®¹ç‰¹å¾ï¼Œé¿å…ç³»ç»Ÿæ¶ˆæ¯è¢«è¯¯åˆ¤
            content = self.response_content.strip()
            
            # åŸºæœ¬é•¿åº¦æ£€æŸ¥
            if len(content) < 50:
                return False
            
            # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç³»ç»Ÿæ¶ˆæ¯
            system_message_patterns = [
                r'^Loaded cached credentials\.',
                r'^Loading\s+.*\.\.\.',
                r'^Authenticating\s+.*\.\.\.',
                r'^Connected to\s+.*',
                r'^Error:\s+',
                r'^Warning:\s+'
            ]
            
            # å¦‚æœå†…å®¹ä¸»è¦æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼Œè®¤ä¸ºæ— æ•ˆ
            for pattern in system_message_patterns:
                if re.match(pattern, content, re.IGNORECASE):
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æœ‰æ„ä¹‰çš„å†…å®¹
                    lines = content.split('\n')
                    non_empty_lines = [line.strip() for line in lines if line.strip()]
                    if len(non_empty_lines) <= 3:  # åªæœ‰å¾ˆå°‘å‡ è¡Œï¼Œå¯èƒ½éƒ½æ˜¯ç³»ç»Ÿæ¶ˆæ¯
                        return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„åˆ†æç»“æ„
            has_section_headers = bool(re.search(r'^##\s+', content, re.MULTILINE))
            has_analysis_content = any(keyword in content.lower() for keyword in [
                'åˆ†æ', 'å»ºè®®', 'æ€»ç»“', 'ç»“è®º', 'è§‚å¯Ÿ', 'å‘ç°', 'è¶‹åŠ¿', 'æœºä¼š', 'é£é™©'
            ])
            
            return has_section_headers or has_analysis_content


@dataclass
class BatchResult:
    """Result of a batch processing operation."""
    platform: str
    total_messages: int
    processed_messages: int
    successful_batches: int
    failed_batches: int
    total_tokens_used: int
    total_cost: float
    processing_time: float
    summaries: List[str]
    errors: List[str]
    # æ–°å¢å­—æ®µï¼šè¯¦ç»†çš„æ‰¹æ¬¡æ‰§è¡Œä¿¡æ¯
    batch_details: List[BatchExecutionDetail] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.batch_details is None:
            self.batch_details = []
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate as percentage."""
        if self.total_messages == 0:
            return 0.0
        return (self.processed_messages / self.total_messages) * 100
    
    @property
    def average_tokens_per_batch(self) -> float:
        """Calculate average tokens per successful batch."""
        if self.successful_batches == 0:
            return 0.0
        return self.total_tokens_used / self.successful_batches
    
    @property
    def failed_batch_details(self) -> List[BatchExecutionDetail]:
        """è·å–å¤±è´¥çš„æ‰¹æ¬¡è¯¦æƒ…"""
        return [detail for detail in self.batch_details if not detail.success]
    
    @property
    def empty_result_batches(self) -> List[BatchExecutionDetail]:
        """è·å–æˆåŠŸä½†æ²¡æœ‰æœ‰æ„ä¹‰å†…å®¹çš„æ‰¹æ¬¡"""
        return [detail for detail in self.batch_details 
                if detail.success and not detail.has_meaningful_content]
    
    @property
    def problematic_batches(self) -> List[BatchExecutionDetail]:
        """è·å–æ‰€æœ‰æœ‰é—®é¢˜çš„æ‰¹æ¬¡ï¼ˆå¤±è´¥ + æ— æ„ä¹‰å†…å®¹ï¼‰"""
        return self.failed_batch_details + self.empty_result_batches


@dataclass  
class IntegrationResponse:
    """æ•´åˆå¤„ç†å“åº”ï¼ŒåŒ…å«LLMå“åº”å’Œæ‰¹æ¬¡è¯¦æƒ…"""
    llm_response: 'LLMResponse'
    batch_details: List[BatchExecutionDetail]
    skipped_batches: List[int] = None  # è¢«è·³è¿‡çš„æ‰¹æ¬¡ç´¢å¼•
    
    @property
    def success(self) -> bool:
        return self.llm_response.success
    
    @property
    def content(self) -> str:
        return self.llm_response.content
    
    @property 
    def error_message(self) -> str:
        return self.llm_response.error_message


@dataclass
class BatchConfig:
    """Configuration for batch processing."""
    max_messages_per_batch: int = 200
    max_tokens_per_batch: int = 60000  # å……åˆ†åˆ©ç”¨å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    max_concurrent_batches: int = 2  # é™ä½å¹¶å‘ä»¥é¿å…RPMé™åˆ¶
    retry_failed_batches: bool = True
    max_retries: int = 2
    delay_between_batches: float = 2.0  # ç»Ÿä¸€å¤šå¹³å°åˆ†ææ¶æ„çš„æ‰¹æ¬¡é—´å»¶è¿Ÿ
    token_buffer: int = 8000  # ä¸ºå“åº”ä¿ç•™æ›´å¤štokens
    
    # è´¨é‡é‡è¯•é…ç½®
    quality_retry_enabled: bool = True  # å¯ç”¨è´¨é‡é‡è¯•
    max_quality_retries: int = 3  # æœ€å¤§è´¨é‡é‡è¯•æ¬¡æ•°
    quality_retry_delay: float = 10.0  # åˆå§‹é‡è¯•å»¶è¿Ÿ(ç§’)
    quality_retry_backoff_factor: float = 1.5  # å»¶è¿Ÿé€’å¢å€æ•°
    
    def validate(self) -> bool:
        """Validate configuration parameters."""
        return (
            self.max_messages_per_batch > 0 and
            self.max_tokens_per_batch > 100 and
            self.max_concurrent_batches > 0 and
            self.max_retries >= 0 and
            self.delay_between_batches >= 0 and
            self.token_buffer >= 0 and
            self.max_quality_retries >= 0 and
            self.quality_retry_delay >= 0 and
            self.quality_retry_backoff_factor > 0
        )


class BatchProcessor:
    """
    Intelligent batch processor for LLM requests.
    
    Features:
    - Smart batching based on token limits
    - Concurrent processing with rate limiting
    - Automatic retry for failed batches
    - Token usage optimization
    - Progress tracking and reporting
    """
    
    def __init__(self, llm_provider: BaseLLMProvider, config: Optional[BatchConfig] = None, llm_config=None):
        """
        Initialize batch processor.
        
        Args:
            llm_provider: LLM provider instance
            config: Batch processing configuration
            llm_config: LLM configuration from config manager
        """
        self.llm_provider = llm_provider
        
        # Create batch config from LLM config if provided
        if config is None and llm_config is not None:
            config = BatchConfig(
                max_messages_per_batch=llm_config.batch_size,
                max_tokens_per_batch=999999,  # ä¸é™åˆ¶Tokenï¼ŒåªæŒ‰æ¶ˆæ¯æ•°é‡åˆ†æ‰¹
                max_concurrent_batches=2,
                retry_failed_batches=True,
                max_retries=llm_config.max_retries,
                delay_between_batches=llm_config.delay_between_batches,
                token_buffer=0  # ä¸éœ€è¦Tokenç¼“å†²
            )
        
        self.config = config or BatchConfig()
        self.logger = TDXLogger.get_logger("tdxagent.processors.batch")
        
        if not self.config.validate():
            raise ValueError("Invalid batch configuration")
        
        # Initialize integration manager
        
        # Processing state
        self._processing_stats = {
            'total_batches_processed': 0,
            'total_messages_processed': 0,
            'total_tokens_used': 0,
            'total_cost': 0.0,
            'total_processing_time': 0.0
        }
        
        # Integration manager for progressive analysis
        
        
        self.logger.info(f"Initialized batch processor with progressive integration support")
    
    async def process_messages(self, messages: List[Dict[str, Any]], 
                              prompt_template: str,
                              platform: str = "",
                              progress_callback: Optional[Callable[[int, int], None]] = None) -> BatchResult:
        """
        Process messages in intelligent batches.
        
        Args:
            messages: List of message dictionaries
            prompt_template: Template with {data} placeholder
            platform: Platform name for context
            progress_callback: Optional callback for progress updates
            
        Returns:
            BatchResult with processing results
        """
        start_time = datetime.now()
        
        if not messages:
            return BatchResult(
                platform=platform,
                total_messages=0,
                processed_messages=0,
                successful_batches=0,
                failed_batches=0,
                total_tokens_used=0,
                total_cost=0.0,
                processing_time=0.0,
                summaries=[],
                errors=[]
            )
        
        # ğŸ”„ å¯¹è¾“å…¥çš„æ¶ˆæ¯è¿›è¡Œå»é‡å¤„ç†
        deduplicated_messages = self._deduplicate_messages(messages, platform)
        
        self.logger.info(f"Starting batch processing of {len(deduplicated_messages)} messages (after deduplication)")
        
        # Create intelligent batches - ä½¿ç”¨å»é‡åçš„æ¶ˆæ¯
        batches = self._create_intelligent_batches(deduplicated_messages, prompt_template)
        self.logger.info(f"Created {len(batches)} batches for processing")
        
        # Process batches using simplified approach (without integration manager)
        final_result = await self._process_batches_simple(
            batches, prompt_template.template, platform, progress_callback
        )
        
        # Compile final results
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # Count successful and failed batches from integration result
        successful_batches = len(batches) if final_result.success else 0
        failed_batches = len(batches) - successful_batches
        
        batch_result = BatchResult(
            platform=platform,
            total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
            processed_messages=len(deduplicated_messages) if final_result.success else 0,  # ä½¿ç”¨å»é‡åçš„æ•°é‡
            successful_batches=successful_batches,
            failed_batches=failed_batches,
            total_tokens_used=final_result.token_count if final_result.success else 0,
            total_cost=self.llm_provider.calculate_cost(final_result.usage) if (
                final_result.success and hasattr(self.llm_provider, 'calculate_cost')
            ) else 0.0,
            processing_time=processing_time,
            summaries=[final_result.content] if final_result.success else [],
            errors=[final_result.error_message] if not final_result.success else []
        )
        
        # Update global stats
        self._update_stats(batch_result)
        
        self.logger.info(
            f"Batch processing complete: {batch_result.processed_messages}/{batch_result.total_messages} "
            f"messages processed in {processing_time:.2f}s "
            f"({batch_result.success_rate:.1f}% success rate)"
        )
        
        return batch_result
    
    def _create_intelligent_batches(self, messages: List[Dict[str, Any]], 
                                   prompt_template: str) -> List[List[Dict[str, Any]]]:
        """
        Create simple batches based on message count only.
        
        Args:
            messages: List of messages to batch
            prompt_template: Prompt template (unused, kept for compatibility)
            
        Returns:
            List of message batches
        """
        batches = []
        current_batch = []
        
        for message in messages:
            # Check if batch is full
            if len(current_batch) >= self.config.max_messages_per_batch:
                # Start new batch
                batches.append(current_batch)
                current_batch = []
            
            # Add message to current batch
            current_batch.append(message)
        
        # Add final batch if not empty
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _format_message_for_estimation(self, message: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ¶ˆæ¯ä»¥è¿›è¡Œtokenä¼°ç®—ï¼ˆé€‚é…ä¼˜åŒ–æ•°æ®ç»“æ„ï¼‰ã€‚"""
        author = message.get('author', {}).get('name', 'Unknown')
        content = message.get('content', {}).get('text', '')
        timestamp = message.get('metadata', {}).get('posted_at', '')
        platform = message.get('platform', '')
        
        return f"[{timestamp}] {author} ({platform}): {content}"
    

    async def _process_batches_simple(self, batches: List[List[Dict[str, Any]]], 
                                     prompt_template: str,
                                     platform: str = "",
                                     progress_callback: Optional[Callable[[int, int], None]] = None) -> LLMResponse:
        """
        ç®€åŒ–çš„æ‰¹æ¬¡å¤„ç†æ–¹æ³• - ä¸ä½¿ç”¨IntegrationManager
        
        Args:
            batches: æ¶ˆæ¯æ‰¹æ¬¡åˆ—è¡¨
            prompt_template: æç¤ºè¯æ¨¡æ¿å­—ç¬¦ä¸²
            platform: å¹³å°åç§°
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            æœ€åä¸€ä¸ªæ‰¹æ¬¡çš„LLMå“åº”
        """
        if not batches:
            return LLMResponse(
                content="",
                usage={},
                model=self.llm_provider.default_model,
                provider=self.llm_provider.provider_name,
                timestamp=datetime.now(),
                success=False,
                error_message="No batches to process"
            )
        
        self.logger.info(f"Starting simple batch processing of {len(batches)} batches")
        
        current_analysis = ""
        total_tokens_used = 0
        accumulated_usage = {}
        
        # Process each batch
        for batch_index, batch in enumerate(batches):
            try:
                # Update progress
                if progress_callback:
                    progress_callback(batch_index + 1, len(batches))
                
                # Import here to avoid circular dependency
                from utils.link_generator import LinkGenerator
                
                # Format messages using unified method
                link_generator = LinkGenerator()
                formatted_messages = link_generator.format_messages_unified(batch)
                
                # Create prompt with integration context if not first batch
                prompt = prompt_template.format(
                    data=formatted_messages,
                    integration_context=current_analysis if batch_index > 0 else ""
                )
                
                # Process batch
                response = await self.llm_provider.generate_response(prompt, platform=platform)
                
                if not response.success:
                    self.logger.error(f"Batch {batch_index + 1} failed: {response.error_message}")
                    return response
                
                # Update current analysis
                current_analysis = response.content
                total_tokens_used += response.token_count
                if response.usage:
                    for key, value in response.usage.items():
                        accumulated_usage[key] = accumulated_usage.get(key, 0) + value
                
                # Add delay between batches
                if batch_index < len(batches) - 1:
                    delay = getattr(self.llm_config, 'delay_between_batches', 2)
                    await asyncio.sleep(delay)
                
                self.logger.info(f"Completed batch {batch_index + 1}/{len(batches)}")
                
            except Exception as e:
                self.logger.error(f"Error processing batch {batch_index + 1}: {e}")
                return LLMResponse(
                    content="",
                    usage=accumulated_usage,
                    model=self.llm_provider.default_model,
                    provider=self.llm_provider.provider_name,
                    timestamp=datetime.now(),
                    success=False,
                    error_message=f"Simple batch processing failed: {str(e)}"
                )
        
        # Return final result
        return LLMResponse(
            content=current_analysis or "",
            usage=accumulated_usage,
            model=self.llm_provider.default_model,
            provider=self.llm_provider.provider_name,
            timestamp=datetime.now(),
            success=True,
            error_message=None
        )

    async def _process_batches_concurrent(self, batches: List[List[Dict[str, Any]]], 
                                        prompt_template: str,
                                        progress_callback: Optional[Callable[[int, int], None]] = None) -> List[LLMResponse]:
        """
        Process batches with controlled concurrency.
        
        Args:
            batches: List of message batches
            prompt_template: Prompt template
            progress_callback: Progress callback function
            
        Returns:
            List of LLM responses
        """
        semaphore = asyncio.Semaphore(self.config.max_concurrent_batches)
        results = []
        completed = 0
        
        async def process_single_batch(batch_index: int, batch: List[Dict[str, Any]]) -> LLMResponse:
            async with semaphore:
                nonlocal completed
                
                try:
                    # Add delay between batches to respect rate limits
                    if batch_index > 0:
                        await asyncio.sleep(self.config.delay_between_batches)
                    
                    # Process the batch
                    result = await self._process_single_batch(batch, prompt_template)
                    
                    # Update progress
                    completed += 1
                    if progress_callback:
                        progress_callback(completed, len(batches))
                    
                    self.logger.debug(f"Completed batch {batch_index + 1}/{len(batches)}")
                    return result
                    
                except Exception as e:
                    self.logger.error(f"Batch {batch_index + 1} failed: {e}")
                    return LLMResponse(
                        content="",
                        usage={},
                        model=self.llm_provider.default_model,
                        provider=self.llm_provider.provider_name,
                        timestamp=datetime.now(),
                        success=False,
                        error_message=str(e)
                    )
        
        # Create tasks for all batches
        tasks = [
            process_single_batch(i, batch) 
            for i, batch in enumerate(batches)
        ]
        
        # Execute all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle any exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"Batch {i + 1} raised exception: {result}")
                processed_results.append(LLMResponse(
                    content="",
                    usage={},
                    model=self.llm_provider.default_model,
                    provider=self.llm_provider.provider_name,
                    timestamp=datetime.now(),
                    success=False,
                    error_message=str(result)
                ))
            else:
                processed_results.append(result)
        
        return processed_results
    
    
    async def _process_single_batch(self, batch: List[Dict[str, Any]], 
                                   prompt_template: str, 
                                   platform: str = "") -> LLMResponse:
        """
        Process a single batch of messages.
        
        Args:
            batch: List of messages in the batch
            prompt_template: Prompt template
            
        Returns:
            LLM response
        """
        # Import here to avoid circular dependency
        from utils.link_generator import LinkGenerator
        
        # Format messages using unified method - platform agnostic
        link_generator = LinkGenerator()
        formatted_messages = link_generator.format_messages_unified(batch)
        
        # Create the prompt with all required variables
        prompt = prompt_template.format(
            data=formatted_messages,
            integration_context=""  # Empty for regular analysis
        )
        
        # é¢„ä¼°tokenæ•°é‡ç”¨äºæ—¥å¿—
        estimated_tokens = self.llm_provider.estimate_tokens(prompt)
        
        self.logger.info(f"å¤„ç†å•æ‰¹æ¬¡ {len(batch)} æ¡æ¶ˆæ¯ (é¢„ä¼° {estimated_tokens} tokens)")
        
        # Make the request with retry logic
        for attempt in range(self.config.max_retries + 1):
            try:
                response = await self.llm_provider.generate_response(prompt, platform=platform)
                
                if response.success:
                    return response
                elif attempt < self.config.max_retries:
                    self.logger.warning(f"Batch attempt {attempt + 1} failed, retrying...")
                    await asyncio.sleep(self.config.delay_between_batches * (attempt + 1))
                
            except Exception as e:
                if attempt < self.config.max_retries:
                    self.logger.warning(f"Batch attempt {attempt + 1} error: {e}, retrying...")
                    await asyncio.sleep(self.config.delay_between_batches * (attempt + 1))
                else:
                    raise
        
        # If we get here, all retries failed
        return LLMResponse(
            content="",
            usage={},
            model=self.llm_provider.default_model,
            provider=self.llm_provider.provider_name,
            timestamp=datetime.now(),
            success=False,
            error_message="All retry attempts failed"
        )
    
    async def _process_batch_with_quality_retry(self, batch: List[Dict[str, Any]], 
                                               prompt_template: str, 
                                               platform: str = "",
                                               batch_index: int = 0) -> LLMResponse:
        """
        Process a batch with intelligent quality retry mechanism.
        
        Args:
            batch: List of messages in the batch
            prompt_template: Prompt template
            platform: Platform name for logging
            batch_index: Index of the batch for logging
            
        Returns:
            LLM response with quality validation
        """
        if not self.config.quality_retry_enabled:
            # å¦‚æœè´¨é‡é‡è¯•æœªå¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            self.logger.info(f"æ‰¹æ¬¡ {batch_index + 1} è´¨é‡é‡è¯•è¢«ç¦ç”¨ï¼Œä½¿ç”¨ç›´æ¥å¤„ç†")
            return await self._process_single_batch(batch, prompt_template, platform)
        
        self.logger.info(f"æ‰¹æ¬¡ {batch_index + 1} å¼€å§‹è´¨é‡é‡è¯•å¤„ç†ï¼Œæœ€å¤§é‡è¯•æ¬¡æ•°: {self.config.max_quality_retries}")
        
        retry_delay = self.config.quality_retry_delay
        
        for retry_attempt in range(self.config.max_quality_retries + 1):
            # æ‰§è¡Œæ‰¹æ¬¡å¤„ç†
            response = await self._process_single_batch(batch, prompt_template, platform)
            
            # å¦‚æœLLMè°ƒç”¨æœ¬èº«å¤±è´¥ï¼Œä¸è¿›è¡Œè´¨é‡é‡è¯•ï¼Œç›´æ¥è¿”å›å¤±è´¥
            if not response.success:
                self.logger.warning(f"æ‰¹æ¬¡ {batch_index + 1} LLMè°ƒç”¨å¤±è´¥ï¼Œä¸è¿›è¡Œè´¨é‡é‡è¯•: {response.error_message}")
                return response
            
            # åˆ›å»ºBatchExecutionDetailæ¥æ£€æŸ¥å†…å®¹è´¨é‡
            batch_detail = BatchExecutionDetail(
                batch_number=batch_index + 1,
                message_count=len(batch),
                success=response.success,
                tokens_used=response.usage.get('total_tokens', 0),
                processing_time=0.0,  # è¿™é‡Œä¸è®¡ç®—æ—¶é—´ï¼Œä¸»è¦ç”¨äºè´¨é‡æ£€æµ‹
                response_content=response.content,
                llm_command=getattr(response, 'call_command', 'command not available'),
                command_type="process_batch_with_quality_retry"
            )
            
            # æ£€æŸ¥å“åº”è´¨é‡
            if batch_detail.has_meaningful_content:
                if retry_attempt > 0:
                    self.logger.info(f"æ‰¹æ¬¡ {batch_index + 1} è´¨é‡é‡è¯•ç¬¬ {retry_attempt} æ¬¡æˆåŠŸ")
                return response
            
            # å†…å®¹è´¨é‡ä¸ä½³ï¼Œè€ƒè™‘é‡è¯•
            if retry_attempt < self.config.max_quality_retries:
                self.logger.warning(f"æ‰¹æ¬¡ {batch_index + 1} è´¨é‡æ£€æµ‹å¤±è´¥ï¼Œå°†åœ¨ {retry_delay:.1f} ç§’åè¿›è¡Œç¬¬ {retry_attempt + 1} æ¬¡é‡è¯•")
                
                # è¯¦ç»†è®°å½•é—®é¢˜å“åº”å†…å®¹ç”¨äºè°ƒè¯•
                if len(response.content) > 1000:
                    self.logger.warning(f"é—®é¢˜å“åº”å†…å®¹ (å‰1000å­—ç¬¦): {response.content[:1000]}...")
                    self.logger.warning(f"é—®é¢˜å“åº”å†…å®¹ (å500å­—ç¬¦): ...{response.content[-500:]}")
                else:
                    self.logger.warning(f"é—®é¢˜å“åº”å®Œæ•´å†…å®¹: {response.content}")
                
                # è®°å½•å“åº”å…ƒæ•°æ®
                if hasattr(response, 'error_message') and response.error_message:
                    self.logger.warning(f"å“åº”é”™è¯¯ä¿¡æ¯: {response.error_message}")
                self.logger.warning(f"LLMå‘½ä»¤: {getattr(response, 'call_command', 'unknown')}")
                
                # ç­‰å¾…é‡è¯•å»¶è¿Ÿ
                await asyncio.sleep(retry_delay)
                
                # é€’å¢å»¶è¿Ÿæ—¶é—´
                retry_delay *= self.config.quality_retry_backoff_factor
            else:
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
                self.logger.error(f"æ‰¹æ¬¡ {batch_index + 1} ç»è¿‡ {self.config.max_quality_retries} æ¬¡è´¨é‡é‡è¯•ä»ç„¶å¤±è´¥")
                
                # è®°å½•å®Œæ•´çš„æœ€ç»ˆå“åº”å†…å®¹
                if len(response.content) > 1000:
                    self.logger.error(f"æœ€ç»ˆå“åº”å†…å®¹ (å‰1000å­—ç¬¦): {response.content[:1000]}...")
                    self.logger.error(f"æœ€ç»ˆå“åº”å†…å®¹ (å500å­—ç¬¦): ...{response.content[-500:]}")
                else:
                    self.logger.error(f"æœ€ç»ˆå“åº”å®Œæ•´å†…å®¹: {response.content}")
                
                # è®°å½•å“åº”å…ƒæ•°æ®
                if hasattr(response, 'error_message') and response.error_message:
                    self.logger.error(f"æœ€ç»ˆå“åº”é”™è¯¯ä¿¡æ¯: {response.error_message}")
                self.logger.error(f"æœ€ç»ˆLLMå‘½ä»¤: {getattr(response, 'call_command', 'unknown')}")
                
                # è¿”å›ä¸€ä¸ªæ ‡è®°ä¸ºå¤±è´¥çš„å“åº”
                return LLMResponse(
                    content="",
                    usage=response.usage,
                    model=response.model,
                    provider=response.provider,
                    timestamp=response.timestamp,
                    success=False,
                    error_message=f"è´¨é‡é‡è¯•å¤±è´¥: å“åº”å†…å®¹è´¨é‡ä¸ä½³ï¼Œç»è¿‡ {self.config.max_quality_retries} æ¬¡é‡è¯•ä»æ— æ³•è·å¾—æœ‰æ•ˆç»“æœ"
                )
        
        # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
        return response
    
    def _update_stats(self, batch_result: BatchResult) -> None:
        """Update global processing statistics."""
        self._processing_stats['total_batches_processed'] += (
            batch_result.successful_batches + batch_result.failed_batches
        )
        self._processing_stats['total_messages_processed'] += batch_result.processed_messages
        self._processing_stats['total_tokens_used'] += batch_result.total_tokens_used
        self._processing_stats['total_cost'] += batch_result.total_cost
        self._processing_stats['total_processing_time'] += batch_result.processing_time
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get overall processing statistics."""
        return self._processing_stats.copy()
    
    def get_collected_conversations(self) -> List[Dict[str, Any]]:
        """Get collected LLM conversations for report appendix.
        
        This method retrieves complete LLM conversations that were automatically
        collected at the provider interface level. Each conversation includes
        request prompt, response content, and full metadata.
        
        Returns:
            List of conversation dictionaries with complete request-response data
        """
        conversations = self.llm_provider.get_collected_conversations()
        return [conv.to_dict() for conv in conversations]
    
    def clear_collected_conversations(self) -> None:
        """Clear collected conversations after report generation.
        
        Clears the conversation collection in the LLM provider to free memory
        and prepare for the next analysis cycle. Should be called after
        generating reports to avoid accumulating conversations across sessions.
        """
        self.llm_provider.clear_collected_conversations()
    
    def enable_conversation_collection(self) -> None:
        """Enable LLM conversation collection for debugging.
        
        Activates the interface-level conversation recording in the LLM provider.
        This captures every request-response pair with full metadata including
        tokens, cost, timing, and success status for debugging purposes.
        """
        self.llm_provider.enable_conversation_collection()
    
    def disable_conversation_collection(self) -> None:
        """Disable LLM conversation collection."""
        self.llm_provider.disable_conversation_collection()
    
    def estimate_processing_cost(self, messages: List[Dict[str, Any]], 
                                prompt_template: str) -> Dict[str, Any]:
        """
        Estimate the cost and time for processing messages.
        
        Args:
            messages: List of messages to process
            prompt_template: Prompt template
            
        Returns:
            Dictionary with cost and time estimates
        """
        if not messages:
            return {
                'estimated_batches': 0,
                'estimated_tokens': 0,
                'estimated_cost': 0.0,
                'estimated_time_seconds': 0.0
            }
        
        # Create batches to get accurate estimates
        batches = self._create_intelligent_batches(messages, prompt_template)
        
        # Estimate tokens for each batch
        total_tokens = 0
        for batch in batches:
            formatted_messages = self.llm_provider.format_messages_for_prompt(batch)
            prompt = prompt_template.format(
                data=formatted_messages,
                integration_context=""  # Empty for estimation
            )
            batch_tokens = self.llm_provider.estimate_tokens(prompt)
            batch_tokens += self.config.token_buffer  # Add response tokens
            total_tokens += batch_tokens
        
        # Estimate cost if provider supports it
        estimated_cost = 0.0
        if hasattr(self.llm_provider, 'calculate_cost'):
            # Rough estimate assuming 80% input, 20% output tokens
            input_tokens = int(total_tokens * 0.8)
            output_tokens = int(total_tokens * 0.2)
            usage = {
                'prompt_tokens': input_tokens,
                'completion_tokens': output_tokens,
                'total_tokens': total_tokens
            }
            estimated_cost = self.llm_provider.calculate_cost(usage)
        
        # Estimate processing time
        estimated_time = len(batches) * self.config.delay_between_batches
        estimated_time += len(batches) * 5  # Rough estimate of 5 seconds per batch
        
        return {
            'estimated_batches': len(batches),
            'estimated_tokens': total_tokens,
            'estimated_cost': estimated_cost,
            'estimated_time_seconds': estimated_time
        }
    
    def _deduplicate_messages(self, messages: List[Dict[str, Any]], platform: str = "unknown") -> List[Dict[str, Any]]:
        """
        å¯¹æ¶ˆæ¯åˆ—è¡¨è¿›è¡Œå»é‡å¤„ç†
        
        å»é‡è§„åˆ™ï¼š
        1. æœ‰IDçš„æ¶ˆæ¯ï¼šæŒ‰IDå»é‡ï¼Œç›¸åŒIDåªä¿ç•™ç¬¬ä¸€æ¡
        2. æ— IDçš„æ¶ˆæ¯ï¼šå…¨éƒ¨ä¿ç•™ï¼ˆæ— æ³•åˆ¤æ–­æ˜¯å¦é‡å¤ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            platform: å¹³å°åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•
            
        Returns:
            å»é‡åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not messages:
            return messages
            
        seen_ids = set()
        unique_messages = []
        duplicate_count = 0
        no_id_count = 0
        
        for msg in messages:
            msg_id = msg.get('id')
            
            if msg_id:
                # æœ‰IDçš„æ¶ˆæ¯ï¼šæ£€æŸ¥æ˜¯å¦é‡å¤
                if msg_id not in seen_ids:
                    seen_ids.add(msg_id)
                    unique_messages.append(msg)
                else:
                    duplicate_count += 1
                    self.logger.debug(f"è·³è¿‡é‡å¤æ¶ˆæ¯ ID: {msg_id} (å¹³å°: {platform})")
            else:
                # æ— IDçš„æ¶ˆæ¯ï¼šå…¨éƒ¨ä¿ç•™ï¼Œä½†è®°å½•æ•°é‡
                unique_messages.append(msg)
                no_id_count += 1
                self.logger.debug(f"ä¿ç•™æ— IDæ¶ˆæ¯ (å¹³å°: {platform}) - æ— æ³•åˆ¤æ–­é‡å¤æ€§")
        
        # è®°å½•å»é‡ç»Ÿè®¡
        if duplicate_count > 0 or no_id_count > 0:
            status_parts = []
            if duplicate_count > 0:
                status_parts.append(f"ç§»é™¤ {duplicate_count} æ¡é‡å¤æ¶ˆæ¯")
            if no_id_count > 0:
                status_parts.append(f"ä¿ç•™ {no_id_count} æ¡æ— IDæ¶ˆæ¯")
            
            self.logger.info(f"å¹³å° {platform} å»é‡: {', '.join(status_parts)}ï¼Œæœ€ç»ˆä¿ç•™ {len(unique_messages)} æ¡æ¶ˆæ¯")
        else:
            self.logger.debug(f"å¹³å° {platform}: {len(unique_messages)} æ¡æ¶ˆæ¯æ— é‡å¤")
            
        return unique_messages
    
    def _deduplicate_platform_data(self, all_platform_data: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        å¯¹å„å¹³å°æ•°æ®åˆ†åˆ«è¿›è¡Œå»é‡ï¼ˆæ¯ä¸ªå¹³å°å•ç‹¬å»é‡ï¼‰
        
        æ³¨æ„ï¼šä¸éœ€è¦è·¨å¹³å°å»é‡ï¼Œå› ä¸ºä¸åŒå¹³å°çš„æ¶ˆæ¯IDå‰ç¼€ä¸åŒï¼Œæ°¸è¿œä¸ä¼šå†²çª
        
        Args:
            all_platform_data: æ‰€æœ‰å¹³å°æ•°æ®å­—å…¸ {platform: messages}
            
        Returns:
            å»é‡åçš„å¹³å°æ•°æ®å­—å…¸
        """
        deduplicated_data = {}
        total_original = 0
        total_deduplicated = 0
        
        for platform, messages in all_platform_data.items():
            original_count = len(messages)
            # æ¯ä¸ªå¹³å°å•ç‹¬å»é‡
            deduplicated_messages = self._deduplicate_messages(messages, platform)
            deduplicated_count = len(deduplicated_messages)
            
            deduplicated_data[platform] = deduplicated_messages
            total_original += original_count
            total_deduplicated += deduplicated_count
        
        # è®°å½•æ•´ä½“å»é‡ç»Ÿè®¡
        total_removed = total_original - total_deduplicated
        if total_removed > 0:
            removal_percentage = (total_removed / total_original) * 100
            self.logger.info(f"ğŸ“Š å„å¹³å°å»é‡å®Œæˆ: åŸå§‹æ¶ˆæ¯ {total_original} æ¡ â†’ å»é‡å {total_deduplicated} æ¡")
            self.logger.info(f"âœ… å»é‡æ•ˆæœ: ç§»é™¤ {total_removed} æ¡é‡å¤æ¶ˆæ¯ ({removal_percentage:.1f}%)")
        else:
            self.logger.info(f"âœ… å„å¹³å°æ•°æ®æ£€æŸ¥: {total_deduplicated} æ¡æ¶ˆæ¯æ— é‡å¤")
        
        return deduplicated_data
    
    async def process_unified_multi_platform_messages(
        self, 
        all_platform_data: Dict[str, List[Dict[str, Any]]], 
        prompt_template: PromptTemplate,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> BatchResult:
        """
        ç»Ÿä¸€å¤šå¹³å°åˆ†æ - ä½¿ç”¨å¹³å°æ ‡ç­¾åŒ–æ•°æ®å¤„ç†
        
        Args:
            all_platform_data: æ‰€æœ‰å¹³å°æ•°æ®å­—å…¸ {platform: messages}
            prompt_template: æç¤ºè¯æ¨¡æ¿
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            ç»Ÿä¸€çš„BatchResult
        """
        if not all_platform_data or not any(all_platform_data.values()):
            self.logger.warning("No platform data to process")
            return self._create_empty_batch_result("unified")
        
        start_time = time.time()
        
        # ğŸ”„ å¯¹å„å¹³å°æ•°æ®åˆ†åˆ«å»é‡
        self.logger.info("ğŸ”„ å¼€å§‹å¯¹å„å¹³å°æ•°æ®åˆ†åˆ«å»é‡...")
        deduplicated_data = self._deduplicate_platform_data(all_platform_data)
        
        # ä½¿ç”¨å»é‡åçš„æ•°æ®è®¡ç®—æ€»æ¶ˆæ¯æ•°
        total_messages = sum(len(msgs) for msgs in deduplicated_data.values())
        
        self.logger.info(f"Starting unified multi-platform analysis: {total_messages} total messages (after deduplication)")
        
        # åˆ›å»ºå¹³å°æ ‡ç­¾åŒ–çš„æ‰¹æ¬¡æ•°æ® - ä½¿ç”¨å»é‡åçš„æ•°æ®
        tagged_data_batches = self._create_platform_tagged_batches(deduplicated_data, self.config.max_messages_per_batch)
        
        if len(tagged_data_batches) == 1:
            # å•æ‰¹æ¬¡ï¼šç›´æ¥å¤„ç†
            self.logger.info("Single batch processing for unified analysis")
            return await self._process_single_tagged_batch(tagged_data_batches[0], prompt_template, total_messages, start_time)
        else:
            # å¤šæ‰¹æ¬¡ï¼šæ¸è¿›å¼å¤„ç†
            self.logger.info(f"Multi-batch processing for unified analysis: {len(tagged_data_batches)} batches")
            return await self._process_multi_tagged_batches(tagged_data_batches, prompt_template, total_messages, start_time, progress_callback)

    async def _process_single_tagged_batch(self, batch_data: str, template: PromptTemplate, 
                                         total_messages: int, start_time: float) -> BatchResult:
        """å¤„ç†å•ä¸ªæ ‡ç­¾åŒ–æ‰¹æ¬¡"""
        try:
            # å¡«å……æ¨¡æ¿
            prompt_content = template.template.format(
                data=batch_data,
                integration_context=""  # å•æ‰¹æ¬¡ä¸éœ€è¦å‰åºç»“æœ
            )
            
            # LLMå¤„ç†
            response = await self.llm_provider.generate_response(prompt_content)
            
            processing_time = time.time() - start_time
            
            if response.success:
                return BatchResult(
                    platform="unified",
                    total_messages=total_messages,
                    processed_messages=total_messages,
                    successful_batches=1,
                    failed_batches=0,
                    total_tokens_used=response.token_count,
                    total_cost=response.cost,
                    processing_time=processing_time,
                    summaries=[response.content],
                    errors=[]
                )
            else:
                return BatchResult(
                    platform="unified", 
                    total_messages=total_messages,
                    processed_messages=0,
                    successful_batches=0,
                    failed_batches=1,
                    total_tokens_used=0,
                    total_cost=0.0,
                    processing_time=processing_time,
                    summaries=[],
                    errors=[response.error_message or "LLM processing failed"]
                )
                
        except Exception as e:
            self.logger.error(f"Single tagged batch processing failed: {e}")
            processing_time = time.time() - start_time
            return BatchResult(
                platform="unified",
                total_messages=total_messages,
                processed_messages=0,
                successful_batches=0,
                failed_batches=1,
                total_tokens_used=0,
                total_cost=0.0,
                processing_time=processing_time,
                summaries=[],
                errors=[str(e)]
            )

    async def _process_multi_tagged_batches(self, tagged_batches: List[str], template: PromptTemplate,
                                          total_messages: int, start_time: float,
                                          progress_callback: Optional[Callable[[int, int], None]] = None) -> BatchResult:
        """å¤šæ‰¹æ¬¡æ¸è¿›å¼å¤„ç†"""
        total_tokens = 0
        total_cost = 0.0
        integration_context = ""  # ç”¨äºprevious_analysis
        successful_batches = 0
        failed_batches = 0
        errors = []
        
        for batch_idx, batch_data in enumerate(tagged_batches, 1):
            try:
                self.logger.info(f"Processing unified batch {batch_idx}/{len(tagged_batches)}")
                
                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(batch_idx, len(tagged_batches))
                
                # å¡«å……æ¨¡æ¿ï¼ˆä½¿ç”¨ç°æœ‰çš„å˜é‡åï¼‰
                prompt_content = template.template.format(
                    data=batch_data,
                    integration_context=integration_context
                )
                
                # LLMå¤„ç†
                response = await self.llm_provider.generate_response(prompt_content)
                
                if response.success:
                    integration_context = response.content  # ä¸‹ä¸€æ‰¹æ¬¡çš„previous_analysis
                    total_tokens += response.token_count
                    total_cost += response.cost
                    successful_batches += 1
                    self.logger.info(f"Batch {batch_idx} completed successfully")
                else:
                    failed_batches += 1
                    error_msg = response.error_message or f"Batch {batch_idx} LLM processing failed"
                    errors.append(error_msg)
                    self.logger.error(f"Batch {batch_idx} failed: {error_msg}")
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if batch_idx < len(tagged_batches):
                    delay = getattr(self.llm_config, 'delay_between_batches', 2)
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                failed_batches += 1
                error_msg = f"Batch {batch_idx} exception: {str(e)}"
                errors.append(error_msg)
                self.logger.error(error_msg)
        
        processing_time = time.time() - start_time
        
        # è¿”å›æœ€ç»ˆç»“æœï¼ˆæœ€åä¸€æ‰¹çš„åˆ†æç»“æœåŒ…å«äº†æ‰€æœ‰å‰æ‰¹æ¬¡çš„æ•´åˆï¼‰
        final_summary = integration_context if integration_context else ""
        
        return BatchResult(
            platform="unified",
            total_messages=total_messages,
            processed_messages=total_messages if successful_batches > 0 else 0,
            successful_batches=successful_batches,
            failed_batches=failed_batches,
            total_tokens_used=total_tokens,
            total_cost=total_cost,
            processing_time=processing_time,
            summaries=[final_summary] if final_summary else [],
            errors=errors
        )

    def _create_platform_tagged_batches(self, all_platform_data: Dict[str, List], batch_size: int) -> List[str]:
        """
        åˆ›å»ºå¹³å°æ ‡ç­¾åŒ–çš„æ•°æ®æ‰¹æ¬¡
        
        æŒ‰å¹³å°é¡ºåºå¡«å……æ•°æ®ï¼Œè¾¾åˆ°batch_sizeå°±æˆªæ–­ï¼Œä¸‹æ‰¹æ¬¡ä»æˆªæ–­ç‚¹ç»§ç»­
        """
        # æŒ‰å¹³å°é¡ºåºåˆå¹¶æ‰€æœ‰æ¶ˆæ¯
        all_messages_with_platform = []
        platform_order = ['twitter', 'telegram', 'gmail', 'discord']
        
        for platform in platform_order:
            if platform in all_platform_data and all_platform_data[platform]:
                for msg in all_platform_data[platform]:
                    all_messages_with_platform.append((platform, msg))
        
        if not all_messages_with_platform:
            return []
        
        # æŒ‰batch_sizeåˆ†æ‰¹
        batches = []
        for i in range(0, len(all_messages_with_platform), batch_size):
            batch_messages = all_messages_with_platform[i:i + batch_size]
            
            # æŒ‰å¹³å°ç»„ç»‡å½“å‰æ‰¹æ¬¡çš„æ¶ˆæ¯
            current_batch_by_platform = {}
            for platform, msg in batch_messages:
                if platform not in current_batch_by_platform:
                    current_batch_by_platform[platform] = []
                current_batch_by_platform[platform].append(msg)
            
            # ç”Ÿæˆå¹³å°æ ‡ç­¾åŒ–çš„æ•°æ®æ ¼å¼
            batch_data = self._format_batch_with_platform_tags(current_batch_by_platform)
            batches.append(batch_data)
        
        return batches

    def _format_batch_with_platform_tags(self, platform_messages: Dict[str, List]) -> str:
        """ä¸ºæ‰¹æ¬¡ç”Ÿæˆå¸¦å¹³å°æ ‡ç­¾çš„æ•°æ®æ ¼å¼"""
        from utils.link_generator import LinkGenerator
        
        platform_sections = []
        platform_order = ['twitter', 'telegram', 'gmail', 'discord'] 
        
        # ä¸éœ€è¦å¼•ç”¨æ ¼å¼ç¤ºä¾‹ - ç»Ÿä¸€æ ¼å¼åŒ–å·²å¤„ç†
        
        link_generator = LinkGenerator()
        
        for platform in platform_order:
            messages = platform_messages.get(platform, [])
            
            if messages:
                # æ ¼å¼åŒ–æ¶ˆæ¯æ•°æ®
                formatted_data = link_generator.format_messages_unified(messages)
                platform_section = f"""<{platform}_data>
=== {platform.title()} æ•°æ® ===

{formatted_data}
</{platform}_data>"""
            else:
                platform_section = f"""<{platform}_data>
æš‚æ— {platform.title()}æ•°æ®
</{platform}_data>"""
            
            platform_sections.append(platform_section)
        
        return "\n\n".join(platform_sections)

    def _create_empty_batch_result(self, platform: str) -> BatchResult:
        """åˆ›å»ºç©ºçš„æ‰¹æ¬¡ç»“æœ"""
        return BatchResult(
            platform=platform,
            total_messages=0,
            processed_messages=0,
            successful_batches=0,
            failed_batches=0,
            total_tokens_used=0,
            total_cost=0.0,
            processing_time=0.0,
            summaries=[],
            errors=["No messages provided"]
        )

    async def process_messages_with_template(self, 
                                           messages: List[Dict[str, Any]], 
                                           prompt_template: PromptTemplate,
                                           platform: str = "",
                                           formatted_messages: str = "",
                                           progress_callback: Optional[Callable[[int, int], None]] = None) -> BatchResult:
        """
        Process messages using PromptTemplate and formatted message citations.
        
        Args:
            messages: List of message dictionaries
            prompt_template: PromptTemplate instance from PromptManager
            platform: Platform name for integration templates
            formatted_messages: Pre-formatted messages with citation references
            progress_callback: Optional callback for progress updates
            
        Returns:
            BatchResult with analysis results
        """
        if not messages:
            self.logger.warning("No messages to process")
            return BatchResult(
                platform=platform,
                total_messages=0,
                processed_messages=0,
                successful_batches=0,
                failed_batches=0,
                total_tokens_used=0,
                total_cost=0.0,
                processing_time=0.0,
                summaries=[],
                errors=["No messages provided"]
            )
        
        start_time = time.time()
        
        # ğŸ”„ å¯¹è¾“å…¥çš„æ¶ˆæ¯è¿›è¡Œå»é‡å¤„ç†
        deduplicated_messages = self._deduplicate_messages(messages, platform)
        
        # Use the template string from PromptTemplate
        template_string = prompt_template.template
        
        # If formatted_messages is provided, use it directly; otherwise format normally
        if formatted_messages:
            # Use the formatted messages with citations
            final_prompt = template_string.format(
                data=formatted_messages,
                integration_context=""  # Empty for formatted messages
            )
            
            # Process as single batch since messages are already formatted with citations
            batch_detail = BatchExecutionDetail(
                batch_number=1,
                message_count=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                tokens_used=0,
                processing_time=0,
                success=False,
                command_type="process_messages_with_template"
            )
            
            try:
                response = await self.llm_provider.generate_response(final_prompt, platform=platform)
                
                processing_time = time.time() - start_time
                batch_detail.processing_time = processing_time
                batch_detail.tokens_used = response.usage.get('total_tokens', 0)
                batch_detail.response_content = response.content
                batch_detail.llm_command = getattr(response, 'call_command', 'command not available')
                
                if response.success:
                    batch_detail.success = True
                    
                    # æ£€æŸ¥å“åº”å†…å®¹è´¨é‡å¹¶è®°å½•
                    if not batch_detail.has_meaningful_content:
                        self.logger.warning(f"å¹³å° {platform} æ‰¹æ¬¡ 1 è¿”å›å†…å®¹è´¨é‡ä¸ä½³æˆ–æ— æœ‰æ•ˆç»“æœ")
                        batch_detail.error_message = "AIè¿”å›å†…å®¹æ— æœ‰æ•ˆç»“æœæˆ–è´¨é‡ä¸ä½³"
                    
                    result = BatchResult(
                        platform=platform,
                        total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        processed_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        successful_batches=1,
                        failed_batches=0,
                        total_tokens_used=response.usage.get('total_tokens', 0),
                        total_cost=self.llm_provider.calculate_cost(response.usage),
                        processing_time=processing_time,
                        summaries=[response.content],
                        errors=[],
                        batch_details=[batch_detail]
                    )
                    
                    # è®°å½•è¯¦ç»†çš„æ‰§è¡Œç»“æœ
                    self._log_batch_execution_results(result)
                    return result
                else:
                    batch_detail.success = False
                    batch_detail.error_message = response.error_message
                    batch_detail.llm_command = getattr(response, 'call_command', 'command not available')
                    
                    self.logger.error(f"å¹³å° {platform} æ‰¹æ¬¡ 1 å¤„ç†å¤±è´¥: {response.error_message}")
                    
                    result = BatchResult(
                        platform=platform,
                        total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        processed_messages=0,
                        successful_batches=0,
                        failed_batches=1,
                        total_tokens_used=0,
                        total_cost=0.0,
                        processing_time=processing_time,
                        summaries=[],
                        errors=[f"LLM processing failed: {response.error_message}"],
                        batch_details=[batch_detail]
                    )
                    
                    self._log_batch_execution_results(result)
                    return result
                    
            except Exception as e:
                processing_time = time.time() - start_time
                batch_detail.processing_time = processing_time
                batch_detail.success = False
                batch_detail.error_message = str(e)
                batch_detail.llm_command = "exception occurred before command execution"
                
                self.logger.error(f"å¹³å° {platform} æ‰¹æ¬¡ 1 å¤„ç†å¼‚å¸¸: {e}")
                
                result = BatchResult(
                    platform=platform,
                    total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                    processed_messages=0,
                    successful_batches=0,
                    failed_batches=1,
                    total_tokens_used=0,
                    total_cost=0.0,
                    processing_time=processing_time,
                    summaries=[],
                    errors=[f"Processing error: {str(e)}"],
                    batch_details=[batch_detail]
                )
                
                self._log_batch_execution_results(result)
                return result
        else:
            # No formatted messages provided - need to do batch processing with platform formatting
            # Import here to avoid circular dependency
            from utils.link_generator import LinkGenerator
            
            # Create LinkGenerator for platform-specific formatting
            link_generator = LinkGenerator()
            
            # Create intelligent batches - ä½¿ç”¨å»é‡åçš„æ¶ˆæ¯
            batches = self._create_intelligent_batches(deduplicated_messages, template_string)
            self.logger.info(f"Created {len(batches)} batches for {platform} processing")
            
            # If only one batch, process directly without integration
            if len(batches) == 1:
                # Single batch - process without integration
                batch_messages = batches[0]
                try:
                    # Format this batch of messages using unified method - platform agnostic
                    formatted_batch = link_generator.format_messages_unified(batch_messages)
                    
                    # Create prompt for this batch
                    batch_prompt = template_string.format(
                        data=formatted_batch,
                        integration_context=""
                    )
                    
                    # æ·»åŠ å•æ‰¹æ¬¡å¤„ç†çš„è¯¦ç»†æ—¥å¿—
                    estimated_tokens = self.llm_provider.estimate_tokens(batch_prompt)
                    self.logger.info(f"å¤„ç†å•æ‰¹æ¬¡ {len(batch_messages)} æ¡æ¶ˆæ¯ (é¢„ä¼° {estimated_tokens} tokens)")
                    
                    # ä½¿ç”¨è´¨é‡é‡è¯•æœºåˆ¶å¤„ç†å•æ‰¹æ¬¡
                    if self.config.quality_retry_enabled:
                        retry_delay = self.config.quality_retry_delay
                        
                        for retry_attempt in range(self.config.max_quality_retries + 1):
                            # Process this batch
                            response = await self.llm_provider.generate_response(batch_prompt, platform=platform)
                            
                            # å¦‚æœLLMè°ƒç”¨æœ¬èº«å¤±è´¥ï¼Œä¸è¿›è¡Œè´¨é‡é‡è¯•
                            if not response.success:
                                self.logger.warning(f"å•æ‰¹æ¬¡ LLMè°ƒç”¨å¤±è´¥ï¼Œä¸è¿›è¡Œè´¨é‡é‡è¯•: {response.error_message}")
                                break
                            
                            # åˆ›å»ºBatchExecutionDetailæ¥æ£€æµ‹è´¨é‡
                            batch_detail = BatchExecutionDetail(
                                batch_number=1,
                                message_count=len(batch_messages),
                                success=response.success,
                                tokens_used=response.usage.get('total_tokens', 0),
                                processing_time=0.0,
                                response_content=response.content,
                                llm_command=getattr(response, 'call_command', 'command not available'),
                                command_type="process_messages_with_template"
                            )
                            
                            # æ£€æŸ¥å“åº”è´¨é‡
                            if batch_detail.has_meaningful_content:
                                if retry_attempt > 0:
                                    self.logger.info(f"å•æ‰¹æ¬¡è´¨é‡é‡è¯•ç¬¬ {retry_attempt} æ¬¡æˆåŠŸ")
                                break
                            
                            # å†…å®¹è´¨é‡ä¸ä½³ï¼Œè€ƒè™‘é‡è¯•
                            if retry_attempt < self.config.max_quality_retries:
                                self.logger.warning(f"å•æ‰¹æ¬¡è´¨é‡æ£€æµ‹å¤±è´¥ï¼Œå°†åœ¨ {retry_delay:.1f} ç§’åè¿›è¡Œç¬¬ {retry_attempt + 1} æ¬¡é‡è¯•")
                                self.logger.warning(f"é—®é¢˜å“åº”å†…å®¹é¢„è§ˆ: {response.content[:200]}...")
                                
                                # ç­‰å¾…é‡è¯•å»¶è¿Ÿ
                                await asyncio.sleep(retry_delay)
                                
                                # é€’å¢å»¶è¿Ÿæ—¶é—´
                                retry_delay *= self.config.quality_retry_backoff_factor
                            else:
                                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
                                self.logger.error(f"å•æ‰¹æ¬¡ç»è¿‡ {self.config.max_quality_retries} æ¬¡è´¨é‡é‡è¯•ä»ç„¶å¤±è´¥")
                                self.logger.error(f"æœ€ç»ˆå“åº”å†…å®¹: {response.content[:500]}...")
                                
                                # æ ‡è®°ä¸ºå¤±è´¥
                                response = LLMResponse(
                                    content="",
                                    usage=response.usage,
                                    model=response.model,
                                    provider=response.provider,
                                    timestamp=response.timestamp,
                                    success=False,
                                    error_message=f"è´¨é‡é‡è¯•å¤±è´¥: å“åº”å†…å®¹è´¨é‡ä¸ä½³ï¼Œç»è¿‡ {self.config.max_quality_retries} æ¬¡é‡è¯•ä»æ— æ³•è·å¾—æœ‰æ•ˆç»“æœ"
                                )
                                break
                    else:
                        # å¦‚æœè´¨é‡é‡è¯•æœªå¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                        response = await self.llm_provider.generate_response(batch_prompt, platform=platform)
                    
                    processing_time = time.time() - start_time
                    
                    if response.success:
                        return BatchResult(
                            platform=platform,
                            total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                            processed_messages=len(batch_messages),
                            successful_batches=1,
                            failed_batches=0,
                            total_tokens_used=getattr(response, 'tokens_used', response.token_count if hasattr(response, 'token_count') else 0),
                            total_cost=getattr(response, 'cost', 0.0),
                            processing_time=processing_time,
                            summaries=[response.content],
                            errors=[]
                        )
                    else:
                        return BatchResult(
                            platform=platform,
                            total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                            processed_messages=0,
                            successful_batches=0,
                            failed_batches=1,
                            total_tokens_used=0,
                            total_cost=0.0,
                            processing_time=processing_time,
                            summaries=[],
                            errors=[f"LLM processing failed: {response.error_message}"]
                        )
                        
                except Exception as e:
                    processing_time = time.time() - start_time
                    self.logger.error(f"Error processing single batch: {e}")
                    return BatchResult(
                        platform=platform,
                        total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        processed_messages=0,
                        successful_batches=0,
                        failed_batches=1,
                        total_tokens_used=0,
                        total_cost=0.0,
                        processing_time=processing_time,
                        summaries=[],
                        errors=[f"Processing error: {str(e)}"]
                    )
            else:
                # Multiple batches - use simplified processing
                integration_response = await self._process_batches_simple(
                    batches, template_string, platform, progress_callback
                )
                
                # Convert LLMResponse to BatchResult
                processing_time = time.time() - start_time
                
                if integration_response.success:
                    # åˆ›å»ºæˆåŠŸçš„æ‰¹æ¬¡è¯¦æƒ… - è¿™æ ·æŠ¥å‘Šç”Ÿæˆå™¨èƒ½æ£€æµ‹åˆ°è¢«è·³è¿‡çš„æ— æ•ˆæ‰¹æ¬¡
                    # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„å®ç°ï¼Œå®é™…çš„æ‰¹æ¬¡å¤„ç†è¯¦æƒ…åœ¨_process_batches_with_integrationä¸­
                    successful_batch_details = []
                    for i, batch in enumerate(batches):
                        batch_detail = BatchExecutionDetail(
                            batch_number=i + 1,
                            message_count=len(batch),
                            tokens_used=getattr(integration_response, 'tokens_used', 0) // len(batches),  # å¹³å‡åˆ†é…
                            processing_time=processing_time / len(batches),  # å¹³å‡åˆ†é…æ—¶é—´
                            success=True,  # æ•´ä½“æˆåŠŸ
                            response_content=integration_response.content if i == len(batches) - 1 else "",  # åªæœ‰æœ€åä¸€ä¸ªæ‰¹æ¬¡æœ‰å®Œæ•´å†…å®¹
                            command_type="process_batches_with_integration",
                            llm_command=getattr(integration_response, 'call_command', 'integration processing')
                        )
                        successful_batch_details.append(batch_detail)
                    
                    return BatchResult(
                        platform=platform,
                        total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        processed_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        successful_batches=len(batches),
                        failed_batches=0,
                        total_tokens_used=getattr(integration_response, 'tokens_used', integration_response.token_count if hasattr(integration_response, 'token_count') else 0),
                        total_cost=getattr(integration_response, 'cost', 0.0),
                        processing_time=processing_time,
                        summaries=[integration_response.content],
                        errors=[],
                        batch_details=successful_batch_details  # æ·»åŠ æ‰¹æ¬¡è¯¦æƒ…
                    )
                else:
                    # åˆ›å»ºå¤±è´¥çš„æ‰¹æ¬¡è¯¦æƒ… - ç¡®ä¿æŠ¥å‘Šä¸­èƒ½æ˜¾ç¤ºå¤±è´¥ä¿¡æ¯
                    failed_batch_details = []
                    for i, batch in enumerate(batches):
                        batch_detail = BatchExecutionDetail(
                            batch_number=i + 1,
                            message_count=len(batch),
                            tokens_used=0,
                            processing_time=processing_time / len(batches),  # å¹³å‡åˆ†é…æ—¶é—´
                            success=False,
                            response_content="",
                            command_type="process_batches_with_integration",
                            llm_command=getattr(integration_response, 'call_command', 'integration failed'),
                            error_message=integration_response.error_message
                        )
                        failed_batch_details.append(batch_detail)
                    
                    return BatchResult(
                        platform=platform,
                        total_messages=len(deduplicated_messages),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
                        processed_messages=0,
                        successful_batches=0,
                        failed_batches=len(batches),
                        total_tokens_used=0,
                        total_cost=0.0,
                        processing_time=processing_time,
                        summaries=[],
                        errors=[f"Integration processing failed: {integration_response.error_message}"],
                        batch_details=failed_batch_details  # æ·»åŠ è¯¦ç»†çš„æ‰¹æ¬¡ä¿¡æ¯
                    )
    
    async def process_cross_platform_analysis(self, 
                                            all_platform_data: Dict[str, List[Dict[str, Any]]], 
                                            prompt_template: PromptTemplate,
                                            formatted_all_data: str,
                                            progress_callback: Optional[Callable[[int, int], None]] = None) -> BatchResult:
        """
        æ‰§è¡Œè·¨å¹³å°ä¸»é¢˜æ•´åˆåˆ†æã€‚
        
        Args:
            all_platform_data: æ‰€æœ‰å¹³å°çš„æ¶ˆæ¯æ•°æ®
            prompt_template: æç¤ºè¯æ¨¡æ¿
            formatted_all_data: æ ¼å¼åŒ–åçš„è·¨å¹³å°æ•°æ®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            BatchResult è·¨å¹³å°åˆ†æç»“æœ
        """
        # è®¡ç®—æ€»æ¶ˆæ¯æ•°
        total_messages = sum(len(messages) for messages in all_platform_data.values())
        
        if total_messages == 0:
            self.logger.warning("No messages to process for cross-platform analysis")
            return BatchResult(
                platform="cross_platform",
                total_messages=0,
                processed_messages=0,
                successful_batches=0,
                failed_batches=0,
                total_tokens_used=0,
                total_cost=0.0,
                processing_time=0.0,
                summaries=[],
                errors=["No messages provided for cross-platform analysis"]
            )
        
        start_time = time.time()
        
        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        template_string = prompt_template.template
        
        self.logger.info(f"å¼€å§‹è·¨å¹³å°ä¸»é¢˜æ•´åˆåˆ†æ: {total_messages} æ¡æ¶ˆæ¯æ¥è‡ª {len(all_platform_data)} ä¸ªå¹³å°")
        
        # å‡†å¤‡æ¨¡æ¿å˜é‡
        template_vars = {
            'data': formatted_all_data,
            'integration_context': ''  # æš‚æ—¶ä¸ºç©ºï¼Œæœªæ¥å¯æ”¯æŒæ‰¹æ¬¡é—´æ•´åˆ
        }
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„ LLM å¤„ç†æ–¹æ³•
            filled_template = template_string.format(**template_vars)
            
            # è°ƒç”¨ LLM è¿›è¡Œè·¨å¹³å°åˆ†æ
            response = await self.llm_provider.generate_response(filled_template, platform="cross_platform")
            
            if response.success:
                processing_time = time.time() - start_time
                
                result = BatchResult(
                    platform="cross_platform",
                    total_messages=total_messages,
                    processed_messages=total_messages,
                    successful_batches=1,
                    failed_batches=0,
                    total_tokens_used=response.token_count,
                    total_cost=0.0,  # Cost calculation moved to provider level
                    processing_time=processing_time,
                    summaries=[response.content],
                    errors=[]
                )
                
                self.logger.info(
                    f"è·¨å¹³å°åˆ†æå®Œæˆ: {result.total_tokens_used} tokens, "
                    f"è€—æ—¶ {result.processing_time:.1f}s"
                )
                
                return result
            else:
                error_msg = f"LLM analysis failed: {response.error_message}"
                self.logger.error(error_msg)
                
                return BatchResult(
                    platform="cross_platform",
                    total_messages=total_messages,
                    processed_messages=0,
                    successful_batches=0,
                    failed_batches=1,
                    total_tokens_used=0,
                    total_cost=0.0,
                    processing_time=time.time() - start_time,
                    summaries=[],
                    errors=[error_msg]
                )
                
        except Exception as e:
            error_msg = f"Cross-platform analysis error: {str(e)}"
            self.logger.error(error_msg)
            
            return BatchResult(
                platform="cross_platform",
                total_messages=total_messages,
                processed_messages=0,
                successful_batches=0,
                failed_batches=1,
                total_tokens_used=0,
                total_cost=0.0,
                processing_time=time.time() - start_time,
                summaries=[],
                errors=[error_msg]
            )
    
    def _log_batch_execution_results(self, result: BatchResult) -> None:
        """
        è®°å½•æ‰¹æ¬¡æ‰§è¡Œç»“æœçš„è¯¦ç»†æ—¥å¿—
        
        Args:
            result: æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        # åŸºæœ¬æ‰§è¡Œä¿¡æ¯
        self.logger.info(f"å¹³å° {result.platform} æ‰§è¡Œå®Œæˆ - "
                        f"æˆåŠŸæ‰¹æ¬¡: {result.successful_batches}, "
                        f"å¤±è´¥æ‰¹æ¬¡: {result.failed_batches}, "
                        f"å¤„ç†æ¶ˆæ¯: {result.processed_messages}/{result.total_messages}")
        
        # æ£€æŸ¥å¹¶è®°å½•æœ‰é—®é¢˜çš„æ‰¹æ¬¡
        problematic_batches = result.problematic_batches
        if problematic_batches:
            self.logger.warning(f"å¹³å° {result.platform} å‘ç° {len(problematic_batches)} ä¸ªæœ‰é—®é¢˜çš„æ‰¹æ¬¡:")
            
            for batch_detail in problematic_batches:
                if not batch_detail.success:
                    # å®Œå…¨å¤±è´¥çš„æ‰¹æ¬¡
                    self.logger.error(f"  æ‰¹æ¬¡ {batch_detail.batch_number} æ‰§è¡Œå¤±è´¥ - "
                                    f"LLMå‘½ä»¤: {batch_detail.llm_command}, "
                                    f"æ¶ˆæ¯æ•°: {batch_detail.message_count}, "
                                    f"é”™è¯¯: {batch_detail.error_message}")
                elif not batch_detail.has_meaningful_content:
                    # æˆåŠŸä½†æ— æœ‰æ•ˆå†…å®¹çš„æ‰¹æ¬¡
                    self.logger.warning(f"  æ‰¹æ¬¡ {batch_detail.batch_number} æ— æœ‰æ•ˆç»“æœ - "
                                      f"LLMå‘½ä»¤: {batch_detail.llm_command}, "
                                      f"æ¶ˆæ¯æ•°: {batch_detail.message_count}, "
                                      f"Tokenä½¿ç”¨: {batch_detail.tokens_used}, "
                                      f"å¤„ç†æ—¶é—´: {batch_detail.processing_time:.2f}s")
                    # è®°å½•å“åº”å†…å®¹æ‘˜è¦ç”¨äºè°ƒè¯•
                    content_preview = batch_detail.response_content[:200] + "..." if len(batch_detail.response_content) > 200 else batch_detail.response_content
                    self.logger.debug(f"    å“åº”å†…å®¹é¢„è§ˆ: {content_preview}")
        else:
            self.logger.info(f"å¹³å° {result.platform} æ‰€æœ‰æ‰¹æ¬¡æ‰§è¡Œæ­£å¸¸ï¼Œå†…å®¹è´¨é‡è‰¯å¥½")
    
    def get_execution_summary(self, result: BatchResult) -> dict:
        """
        ç”Ÿæˆæ‰¹æ¬¡æ‰§è¡Œçš„æ±‡æ€»ä¿¡æ¯ï¼Œç”¨äºæŠ¥å‘Šç”Ÿæˆ
        
        Args:
            result: æ‰¹æ¬¡å¤„ç†ç»“æœ
            
        Returns:
            åŒ…å«æ‰§è¡Œæ±‡æ€»çš„å­—å…¸
        """
        problematic_batches = result.problematic_batches
        failed_batches = result.failed_batch_details
        empty_batches = result.empty_result_batches
        
        return {
            'platform': result.platform,
            'total_batches': result.successful_batches + result.failed_batches,
            'successful_batches': result.successful_batches,
            'failed_batches': result.failed_batches,
            'problematic_count': len(problematic_batches),
            'failed_details': [
                {
                    'batch_number': detail.batch_number,
                    'command_type': detail.command_type,
                    'message_count': detail.message_count,
                    'error_message': detail.error_message,
                    'processing_time': detail.processing_time
                }
                for detail in failed_batches
            ],
            'empty_result_details': [
                {
                    'batch_number': detail.batch_number,
                    'command_type': detail.command_type,
                    'message_count': detail.message_count,
                    'tokens_used': detail.tokens_used,
                    'processing_time': detail.processing_time,
                    'response_preview': detail.response_content[:100] + "..." if len(detail.response_content) > 100 else detail.response_content
                }
                for detail in empty_batches
            ]
        }
