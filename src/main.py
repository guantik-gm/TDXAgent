"""
TDXAgent - Personal Information AI Assistant

Main application entry point that orchestrates data collection,
processing, and report generation across multiple platforms.
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta
import logging
import click
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from rich.panel import Panel

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from config.config_manager import ConfigManager
from utils.logger import TDXLogger
from storage.jsonl_storage import JSONLStorage
from storage.media_storage import MediaStorage
from llm.openai_provider import OpenAIProvider
from llm.gemini_provider import GeminiProvider
from llm.claude_provider import ClaudeProvider
from llm.gemini_cli_provider import GeminiCliProvider
from processors.batch_processor import BatchProcessor, BatchConfig
from processors.report_generator import ReportGenerator
from processors.prompt_manager import PromptManager
from utils.link_generator import LinkGenerator
from scrapers.telegram_scraper import TelegramScraper
from scrapers.discord_scraper import DiscordScraper
from scrapers.twitter_scraper import TwitterScraper
from scrapers.gmail_scraper import GmailScraper


class TDXAgent:
    """
    Main TDXAgent application orchestrator.
    
    Coordinates data collection, processing, and reporting across
    Twitter/X, Telegram, and Discord platforms.
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize TDXAgent.
        
        Args:
            config_path: Path to configuration file
        """
        self.console = Console()
        self.config_manager = ConfigManager(config_path)
        
        # Initialize logging
        self.logger = TDXLogger.setup_application_logging(
            self.config_manager.app.data_directory,
            self.config_manager.app.log_level
        )
        
        # Initialize components  
        self.storage = JSONLStorage(self.config_manager.app.data_directory)
        self.media_storage = MediaStorage(self.config_manager.app.data_directory)
        
        # Get reports directory from config (fallback to default)
        reports_dir = self.config_manager.output.get('reports_directory', f"{self.config_manager.app.data_directory}/reports")
        self.report_generator = ReportGenerator(reports_dir)
        
        # Track execution start time for reports (UTC+8)
        from datetime import timedelta
        utc_plus_8 = timezone(timedelta(hours=8))
        self.execution_start_time = datetime.now(utc_plus_8)
        
        # Initialize LLM provider
        self.llm_provider = self._initialize_llm_provider()
        
        # Initialize prompt manager and link generator
        self.prompt_manager = PromptManager()
        self.link_generator = LinkGenerator()
        
        self.batch_processor = BatchProcessor(
            self.llm_provider, 
            llm_config=self.config_manager.llm
        ) if self.llm_provider else None
        
        # Initialize scrapers
        self.scrapers = self._initialize_scrapers()
        
        self.logger.info("TDXAgent initialized successfully")
    
    def _initialize_llm_provider(self):
        """Initialize LLM provider based on configuration."""
        try:
            provider_name = self.config_manager.llm.provider
            
            if provider_name == 'openai':
                # Merge provider-specific config with general LLM config
                openai_config = {
                    **self.config_manager.llm.openai,
                    'max_requests_per_minute': self.config_manager.llm.max_requests_per_minute,
                    'max_tokens_per_minute': self.config_manager.llm.max_tokens_per_minute,
                    'timeout': self.config_manager.llm.timeout,
                    'max_retries': self.config_manager.llm.max_retries,
                    'retry_delay': self.config_manager.llm.retry_delay,
                    'max_tokens': self.config_manager.llm.max_tokens
                }
                return OpenAIProvider(openai_config, data_directory=str(self.config_manager.get_data_directory()))
            elif provider_name == 'gemini':
                # Merge provider-specific config with general LLM config
                gemini_config = {
                    **self.config_manager.llm.gemini,
                    'max_requests_per_minute': self.config_manager.llm.max_requests_per_minute,
                    'max_tokens_per_minute': self.config_manager.llm.max_tokens_per_minute,
                    'timeout': self.config_manager.llm.timeout,
                    'max_retries': self.config_manager.llm.max_retries,
                    'retry_delay': self.config_manager.llm.retry_delay,
                    'max_tokens': self.config_manager.llm.max_tokens
                }
                return GeminiProvider(gemini_config, data_directory=str(self.config_manager.get_data_directory()))
            elif provider_name == 'claude_cli':
                # Merge provider-specific config with general LLM config
                claude_config = {
                    **self.config_manager.llm.claude_cli,
                    'max_requests_per_minute': self.config_manager.llm.max_requests_per_minute,
                    'max_tokens_per_minute': self.config_manager.llm.max_tokens_per_minute,
                    'timeout': self.config_manager.llm.claude_cli.get('timeout', 120),
                    'max_retries': self.config_manager.llm.max_retries,
                    'retry_delay': self.config_manager.llm.retry_delay,
                    'max_tokens': self.config_manager.llm.max_tokens,
                    'enable_prompt_files': getattr(self.config_manager.llm, 'enable_prompt_files', True)
                }
                return ClaudeProvider(claude_config, data_directory=str(self.config_manager.get_data_directory()))
            elif provider_name == 'gemini_cli':
                # Merge provider-specific config with general LLM config
                gemini_cli_config = {
                    **self.config_manager.llm.gemini_cli,
                    'max_requests_per_minute': self.config_manager.llm.max_requests_per_minute,
                    'max_tokens_per_minute': self.config_manager.llm.max_tokens_per_minute,
                    'timeout': self.config_manager.llm.gemini_cli.get('timeout', 120),
                    'max_retries': self.config_manager.llm.max_retries,
                    'retry_delay': self.config_manager.llm.retry_delay,
                    'max_tokens': self.config_manager.llm.max_tokens,
                    'enable_prompt_files': getattr(self.config_manager.llm, 'enable_prompt_files', True)
                }
                return GeminiCliProvider(gemini_cli_config, data_directory=str(self.config_manager.get_data_directory()))
            else:
                self.logger.error(f"Unknown LLM provider: {provider_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"Failed to initialize LLM provider: {e}")
            return None
    
    def _initialize_scrapers(self) -> Dict[str, Any]:
        """Initialize platform scrapers."""
        scrapers = {}
        
        try:
            if self.config_manager.telegram.enabled:
                scrapers['telegram'] = TelegramScraper(
                    self.config_manager.telegram.__dict__, 
                    self.config_manager.app.data_directory
                )
            
            if self.config_manager.discord.enabled:
                scrapers['discord'] = DiscordScraper(self.config_manager.discord.__dict__)
            
            if self.config_manager.twitter.enabled:
                scrapers['twitter'] = TwitterScraper(self.config_manager.twitter.__dict__)
            
            if hasattr(self.config_manager, 'gmail') and self.config_manager.gmail.enabled:
                scrapers['gmail'] = GmailScraper(self.config_manager.gmail.__dict__)
                
        except Exception as e:
            self.logger.error(f"Failed to initialize scrapers: {e}")
        
        return scrapers
    
    async def run_collection(self, hours_back: int = None, platforms: List[str] = None) -> Dict[str, Any]:
        """
        Run data collection from specified platforms.
        
        Args:
            hours_back: Hours back to collect data (uses config default if None)
            platforms: List of platforms to collect from (all enabled if None)
            
        Returns:
            Dictionary of collection results
        """
        hours_back = hours_back or self.config_manager.app.default_hours_to_fetch
        platforms = platforms or list(self.scrapers.keys())
        
        results = {}
        
        for platform in platforms:
            if platform not in self.scrapers:
                self.logger.warning(f"Platform {platform} not available")
                continue
            
            try:
                scraper = self.scrapers[platform]
                
                # Authenticate without progress bar interference
                self.console.print(f"üîë Ê≠£Âú®ËÆ§ËØÅ {platform}...")
                auth_success = await scraper.authenticate()
                
                if not auth_success:
                    self.logger.error(f"Failed to authenticate {platform}")
                    results[platform] = {'error': 'Authentication failed'}
                    continue
                
                # Now show progress for data collection
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    console=self.console
                ) as progress:
                    task = progress.add_task(f"Êî∂ÈõÜ {platform} Êï∞ÊçÆ...", total=None)
                    
                    # Scrape data
                    scraping_result = await scraper.scrape_with_monitoring(hours_back)
                    
                    # Store data
                    if scraping_result.messages:
                        success_count, failed_count = await self.storage.store_messages(
                            platform, scraping_result.messages
                        )
                        
                        self.logger.info(
                            f"Stored {success_count} {platform} messages "
                            f"({failed_count} failed)"
                        )
                    
                    results[platform] = {
                        'scraping_result': scraping_result,
                        'stored_messages': len(scraping_result.messages)
                    }
                    
                    # Cleanup
                    await scraper.cleanup()
                    
            except Exception as e:
                self.logger.error(f"Failed to collect {platform} data: {e}")
                results[platform] = {'error': str(e)}
        
        return results
    
    async def run_analysis(self, platforms: List[str] = None, hours_back: int = None) -> Dict[str, Any]:
        """
        Run AI analysis on collected data.
        
        Args:
            platforms: Platforms to analyze (all if None)
            hours_back: Hours back to analyze (uses config default if None)
            
        Returns:
            Dictionary of analysis results
        """
        if not self.batch_processor:
            self.logger.error("No LLM provider available for analysis")
            return {}
        
        hours_back = hours_back or self.config_manager.app.default_hours_to_fetch
        platforms = platforms or list(self.scrapers.keys())
        
        # Calculate time range for analysis (more precise than date-based)
        from datetime import datetime, timedelta, timezone
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(hours=hours_back)
        
        # Also calculate date range for logging
        start_date = start_time.date()
        end_date = end_time.date()
        
        self.logger.info(f"ÂàÜÊûêÊó∂Èó¥ËåÉÂõ¥: {start_time.strftime('%Y-%m-%d %H:%M')} Âà∞ {end_time.strftime('%Y-%m-%d %H:%M')} ({hours_back}Â∞èÊó∂)")
        
        # üéØ ÊåâÂπ≥Âè∞Áã¨Á´ãÂàÜÊûê - ÂçïÁã¨Â§ÑÁêÜÊØè‰∏™Âπ≥Âè∞ÁöÑÊï∞ÊçÆ
        platform_results = {}
        total_messages = 0
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=self.console
        ) as progress:
            
            # Get pure investment analysis template (platform-agnostic)
            prompt_template = await self.prompt_manager.get_template('pure_investment_analysis')
            
            if not prompt_template:
                self.logger.error("Pure investment analysis template not found")
                return {'error': 'Pure investment analysis template not available'}
            
            # ÊåâÂπ≥Âè∞Áã¨Á´ãÂ§ÑÁêÜ
            for platform in platforms:
                platform_enabled = self.config_manager.is_platform_enabled(platform)
                self.logger.info(f"Âπ≥Âè∞ {platform} ÂêØÁî®Áä∂ÊÄÅ: {platform_enabled}")
                
                if not platform_enabled:
                    self.logger.warning(f"Âπ≥Âè∞ {platform} Ë¢´Á¶ÅÁî®ÔºåË∑≥ËøáÂàÜÊûê")
                    continue
                
                platform_task = progress.add_task(f"ÂàÜÊûê {platform} Âπ≥Âè∞Êï∞ÊçÆ...", total=None)
                
                try:
                    # ‰ΩøÁî®Á≤æÁ°ÆÊó∂Èó¥ËåÉÂõ¥Êî∂ÈõÜÂΩìÂâçÂπ≥Âè∞Êï∞ÊçÆ
                    messages = await self.storage.get_messages_by_time_range(
                        platform,
                        start_time=start_time,
                        end_time=end_time
                    )
                    
                    if not messages:
                        self.logger.info(f"No messages found for {platform}")
                        platform_results[platform] = {
                            'batch_result': None,
                            'total_messages_analyzed': 0,
                            'error': f'No data available for {platform}'
                        }
                        continue
                    
                    total_messages += len(messages)
                    self.logger.info(f"Êî∂ÈõÜÂà∞ {len(messages)} Êù° {platform} Ê∂àÊÅØ")
                    
                    # ü§ñ ÊâßË°åÂçïÂπ≥Âè∞ÂàÜÊûê - ‰ΩøÁî®Ëá™Âä®ÂàÜÊâπÂ§ÑÁêÜ
                    batch_result = await self.batch_processor.process_messages_with_template(
                        messages=messages,
                        prompt_template=prompt_template,
                        platform=platform,
                        formatted_messages=""  # ËÆ©BatchProcessorÂÜÖÈÉ®Â§ÑÁêÜÂàÜÊâπÂíåÊ†ºÂºèÂåñ
                    )
                    
                    platform_results[platform] = {
                        'batch_result': batch_result,
                        'total_messages_analyzed': len(messages),
                        'platform': platform
                    }
                    
                    self.logger.info(f"ÂÆåÊàê {platform} Âπ≥Âè∞ÂàÜÊûê: {len(messages)} Êù°Ê∂àÊÅØ")
                    
                    # Âπ≥Âè∞Èó¥ÁºìÂÜ≤Á≠âÂæÖÊó∂Èó¥ÔºàÈô§‰∫ÜÊúÄÂêé‰∏Ä‰∏™Âπ≥Âè∞Ôºâ
                    current_platform_index = list(platforms).index(platform)
                    if current_platform_index < len(list(platforms)) - 1:  # ‰∏çÊòØÊúÄÂêé‰∏Ä‰∏™Âπ≥Âè∞
                        delay = self.config_manager.llm.multi_platform_delay
                        self.logger.info(f"Âπ≥Âè∞Èó¥Á≠âÂæÖ {delay} ÁßíÁºìÂÜ≤Êó∂Èó¥...")
                        await asyncio.sleep(delay)
                    
                except Exception as e:
                    self.logger.error(f"Failed to analyze {platform} data: {e}")
                    platform_results[platform] = {
                        'batch_result': None,
                        'total_messages_analyzed': 0,
                        'error': str(e)
                    }
                
                progress.advance(platform_task)
            
            if not platform_results:
                self.logger.warning("No data collected from any platform")
                return {'error': 'No data available for analysis'}
            
            results = {
                'platform_analysis': platform_results,
                'total_messages_analyzed': total_messages,
                'platforms_included': list(platform_results.keys())
            }
            
            self.logger.info(
                f"ÂÆåÊàêÊåâÂπ≥Âè∞Áã¨Á´ãÂàÜÊûê: {total_messages} Êù°Ê∂àÊÅØÊù•Ëá™ {len(platform_results)} ‰∏™Âπ≥Âè∞"
            )
        
        return results
    
    async def _format_platform_data(self, messages: List[Dict[str, Any]], platform: str) -> str:
        """
        Ê†ºÂºèÂåñÂçï‰∏™Âπ≥Âè∞Êï∞ÊçÆ‰∏∫AIÂàÜÊûêÁöÑËæìÂÖ•Ê†ºÂºè - ÂÆåÂÖ®Âπ≥Âè∞Êó†ÂÖ≥„ÄÇ
        
        Args:
            messages: Âπ≥Âè∞Ê∂àÊÅØÊï∞ÊçÆ
            platform: Âπ≥Âè∞ÂêçÁß∞Ôºà‰øùÁïôÂèÇÊï∞Áî®‰∫éÂÖºÂÆπÊÄßÔºå‰ΩÜ‰∏çÂÜç‰ΩøÁî®Ôºâ
            
        Returns:
            Ê†ºÂºèÂåñÂêéÁöÑÁªü‰∏ÄÊï∞ÊçÆÂ≠óÁ¨¶‰∏≤
        """
        if not messages:
            return ""
        
        try:
            # ‰ΩøÁî®Êñ∞ÁöÑÁªü‰∏ÄÊ†ºÂºèÂåñÊñπÊ≥ï - AIÂÆåÂÖ®Âπ≥Âè∞Êó†ÂÖ≥
            return self.link_generator.format_messages_unified(messages)
                
        except Exception as e:
            self.logger.error(f"Failed to format platform data: {e}")
            # Fallback to basic unified formatting
            formatted_lines = []
            for i, msg in enumerate(messages, 1):
                author = msg.get('author', {}).get('name', 'Unknown')
                content = msg.get('content', {}).get('text', '')
                timestamp = msg.get('metadata', {}).get('posted_at', '')[:16]
                platform_name = msg.get('platform', '').title()
                if content.strip():
                    formatted_lines.append(f"{i}. [{timestamp}] {author}@{platform_name}: {content}")
            return '\n'.join(formatted_lines)
    
    async def _format_cross_platform_data(self, all_platform_data: Dict[str, List]) -> str:
        """
        Ê†ºÂºèÂåñË∑®Âπ≥Âè∞Êï∞ÊçÆ‰∏∫AIÂàÜÊûêÁöÑÁªü‰∏ÄËæìÂÖ•Ê†ºÂºè - ÂÆåÂÖ®Âπ≥Âè∞Êó†ÂÖ≥„ÄÇ
        
        Args:
            all_platform_data: ÂêÑÂπ≥Âè∞ÁöÑÊ∂àÊÅØÊï∞ÊçÆ
            
        Returns:
            Ê†ºÂºèÂåñÂêéÁöÑÁªü‰∏ÄÊï∞ÊçÆÂ≠óÁ¨¶‰∏≤
        """
        # ÂêàÂπ∂ÊâÄÊúâÂπ≥Âè∞ÁöÑÊ∂àÊÅØ
        all_messages = []
        for platform, messages in all_platform_data.items():
            if messages:
                all_messages.extend(messages)
        
        # ‰ΩøÁî®Áªü‰∏ÄÊ†ºÂºèÂåñÊñπÊ≥ï - AIÂÆåÂÖ®Âπ≥Âè∞Êó†ÂÖ≥
        return self.link_generator.format_messages_unified(all_messages)
    
    def _get_platform_display_name(self, platform: str) -> str:
        """Ëé∑ÂèñÂπ≥Âè∞ÊòæÁ§∫ÂêçÁß∞"""
        platform_names = {
            'twitter': 'üê¶ Twitter/X',
            'telegram': '‚úàÔ∏è Telegram', 
            'discord': 'üí¨ Discord',
            'gmail': 'üìß Gmail'
        }
        return platform_names.get(platform, f"üì± {platform.title()}")
    
    async def generate_reports(self, analysis_results: Dict[str, Any], hours_back: int = None) -> List[str]:
        """
        Generate unified comprehensive report from cross-platform analysis results.
        
        Args:
            analysis_results: Results from run_analysis (now contains cross_platform_analysis)
            hours_back: Hours back that were processed (for filename generation)
            
        Returns:
            List containing the unified report file path
        """
        report_paths = []
        
        try:
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÊåâÂπ≥Âè∞ÂàÜÊûêÁªìÊûú
            if 'platform_analysis' not in analysis_results:
                self.logger.error("No platform analysis results found")
                return []
            
            platform_results = analysis_results['platform_analysis']
            
            
            # Generate individual platform reports
            self.logger.debug(f"ÂºÄÂßãÁîüÊàê {len(platform_results)} ‰∏™Âπ≥Âè∞ÁöÑÊä•Âëä")
            for platform, platform_result in platform_results.items():
                if not platform_result.get('batch_result') or not platform_result['batch_result'].summaries:
                    self.logger.warning(f"No analysis summaries available for {platform}")
                    continue
                
                self.logger.debug(f"Ê≠£Âú®ÁîüÊàê {platform} Âπ≥Âè∞Êä•Âëä...")
                
                # Generate platform-specific report
                platform_path = await self.report_generator.generate_platform_report(
                    platform=platform,
                    batch_result=platform_result['batch_result'],
                    messages=platform_result.get('messages', []),
                    start_time=self.execution_start_time,
                    hours_back=hours_back
                )
                
                if platform_path:
                    report_paths.append(platform_path)
                    self.logger.debug(f"‚úÖ {platform} Âπ≥Âè∞Êä•ÂëäÂ∑≤ÁîüÊàê: {Path(platform_path).name}")
                    # Note: ReportGenerator already logs the generation success
            
            # Generate consolidated report if multiple platforms have reports
            if len(report_paths) > 1:
                self.logger.info(f"ÁîüÊàêÂ§öÂπ≥Âè∞Ê±áÊÄªÊä•ÂëäÔºåÊï¥Âêà {len(report_paths)} ‰∏™Âπ≥Âè∞Êä•Âëä...")
                
                # ÁîüÊàêÊ±áÊÄªÊä•ÂëäÔºöÁÆÄÂçïÂú∞Áî® --- ÂàÜÂâ≤ÂêÑÂπ≥Âè∞Êä•ÂëäÂÜÖÂÆπ
                consolidated_path = await self._generate_consolidated_report(report_paths, hours_back)
                
                if consolidated_path:
                    # Ê†πÊçÆÈÖçÁΩÆÂÜ≥ÂÆöÊòØÂê¶Âà†Èô§ÂçïÁã¨ÁöÑÂπ≥Âè∞Êä•ÂëäÊñá‰ª∂
                    multi_platform_config = self.config_manager.output.get('multi_platform_reports', {})
                    keep_individual_reports = multi_platform_config.get('keep_individual_reports', False)
                    
                    if not keep_individual_reports:
                        # Âà†Èô§ÂçïÁã¨ÁöÑÂπ≥Âè∞Êä•ÂëäÊñá‰ª∂
                        for platform_path in report_paths:
                            try:
                                Path(platform_path).unlink()
                                self.logger.debug(f"Â∑≤Âà†Èô§ÂçïÁã¨Âπ≥Âè∞Êä•Âëä: {Path(platform_path).name}")
                            except Exception as e:
                                self.logger.warning(f"Âà†Èô§Âπ≥Âè∞Êä•ÂëäÂ§±Ë¥• {platform_path}: {e}")
                        
                        # Âè™‰øùÁïôÊ±áÊÄªÊä•ÂëäË∑ØÂæÑ
                        report_paths = [consolidated_path]
                        self.logger.info(f"Â∑≤ÁîüÊàêÊ±áÊÄªÊä•ÂëäÂπ∂Ê∏ÖÁêÜÂçïÁã¨Êä•Âëä: {Path(consolidated_path).name}")
                    else:
                        # ‰øùÁïôÂàÜÂπ≥Âè∞Êä•ÂëäÔºåÊ∑ªÂä†Ê±áÊÄªÊä•ÂëäÂà∞Ë∑ØÂæÑÂàóË°®
                        report_paths.append(consolidated_path)
                        self.logger.info(f"Â∑≤ÁîüÊàêÊ±áÊÄªÊä•ÂëäÂπ∂‰øùÁïôÂàÜÂπ≥Âè∞Êä•Âëä: {Path(consolidated_path).name}")
            else:
                self.logger.info(f"Ë∑≥ËøáÊ±áÊÄªÊä•ÂëäÁîüÊàê: Âè™Êúâ {len(report_paths)} ‰∏™Âπ≥Âè∞Êä•Âëä")
            
        except Exception as e:
            self.logger.error(f"Failed to generate reports: {e}")
        
        return report_paths
    
    async def _generate_consolidated_report(self, platform_report_paths: List[str], hours_back: int = None) -> str:
        """
        ÁîüÊàêÁÆÄÂçïÁöÑÊ±áÊÄªÊä•ÂëäÔºöÂ∞ÜÂêÑÂπ≥Âè∞Êä•ÂëäÁî® --- ÂàÜÂâ≤Êï¥ÂêàÂà∞‰∏Ä‰∏™Êñá‰ª∂
        
        Args:
            platform_report_paths: ÂêÑÂπ≥Âè∞Êä•ÂëäÊñá‰ª∂Ë∑ØÂæÑÂàóË°®
            hours_back: Â∞èÊó∂Êï∞ÔºàÁî®‰∫éÊñá‰ª∂ÂêçÔºâ
            
        Returns:
            Ê±áÊÄªÊä•ÂëäÊñá‰ª∂Ë∑ØÂæÑ
        """
        try:
            # ËØªÂèñÊâÄÊúâÂπ≥Âè∞Êä•ÂëäÂÜÖÂÆπ
            all_contents = []
            
            for report_path in platform_report_paths:
                try:
                    with open(report_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        all_contents.append(content)
                except Exception as e:
                    self.logger.warning(f"ËØªÂèñÊä•ÂëäÂ§±Ë¥• {report_path}: {e}")
                    continue
            
            if not all_contents:
                self.logger.error("Ê≤°ÊúâÂèØÁî®ÁöÑÂπ≥Âè∞Êä•ÂëäÂÜÖÂÆπ")
                return None
            
            # Áî® --- ÂàÜÂâ≤Á¨¶Êï¥ÂêàÊâÄÊúâÊä•Âëä
            consolidated_content = "\n\n---\n\n".join(all_contents)
            
            # ÁîüÊàêÊ±áÊÄªÊä•ÂëäÊñá‰ª∂Âêç
            timestamp = datetime.now().strftime("%YÂπ¥%mÊúà%dÊó•_%HÊó∂%MÂàÜ")
            
            if hours_back:
                if hours_back == 1:
                    period_desc = "1Â∞èÊó∂"
                elif hours_back == 12:
                    period_desc = "12Â∞èÊó∂"
                elif hours_back == 24:
                    period_desc = "1Â§©"
                elif hours_back < 24:
                    period_desc = f"{hours_back}Â∞èÊó∂"
                elif hours_back % 24 == 0:
                    days = hours_back // 24
                    period_desc = f"{days}Â§©"
                else:
                    period_desc = f"{hours_back}Â∞èÊó∂"
            else:
                period_desc = "Â§öÂ∞èÊó∂"
            
            filename = f"TDXAgent_Â§öÂπ≥Âè∞Ê±áÊÄªÊä•Âëä_{timestamp}_{period_desc}.md"
            consolidated_path = self.report_generator.output_directory / filename
            
            # ÂÜôÂÖ•Ê±áÊÄªÊä•Âëä
            with open(consolidated_path, 'w', encoding='utf-8') as f:
                f.write(consolidated_content)
            
            return str(consolidated_path)
            
        except Exception as e:
            self.logger.error(f"ÁîüÊàêÊ±áÊÄªÊä•ÂëäÂ§±Ë¥•: {e}")
            return None
    
    async def run_full_pipeline(self, hours_back: int = None, platforms: List[str] = None) -> Dict[str, Any]:
        """
        Run the complete TDXAgent pipeline: collect -> analyze -> report.
        
        Args:
            hours_back: Hours back to process
            platforms: Platforms to process
            
        Returns:
            Complete pipeline results
        """
        self.console.print(Panel.fit("üöÄ ÂêØÂä® TDXAgent ÂÆåÊï¥ÊµÅÁ®ã", style="bold blue"))
        
        # Step 1: Data Collection
        self.console.print("\nüì• Ê≠•È™§ 1: Êï∞ÊçÆÊî∂ÈõÜ")
        collection_results = await self.run_collection(hours_back, platforms)
        
        # Step 2: AI Analysis
        self.console.print("\nü§ñ Ê≠•È™§ 2: AI ÂàÜÊûê")
        analysis_results = await self.run_analysis(platforms, hours_back)
        
        # Step 3: Report Generation
        self.console.print("\nüìä Ê≠•È™§ 3: ÁîüÊàêÊä•Âëä")
        report_paths = await self.generate_reports(analysis_results, hours_back)
        
        # Display summary
        self._display_pipeline_summary(collection_results, analysis_results, report_paths)
        
        return {
            'collection_results': collection_results,
            'analysis_results': analysis_results,
            'report_paths': report_paths
        }
    
    def _display_pipeline_summary(self, collection_results: Dict[str, Any], 
                                 analysis_results: Dict[str, Any], 
                                 report_paths: List[str]) -> None:
        """Display pipeline execution summary."""
        
        # Create summary table
        table = Table(title="TDXAgent ÊâßË°åÊëòË¶Å")
        table.add_column("Âπ≥Âè∞", style="cyan")
        table.add_column("Êî∂ÈõÜÊ∂àÊÅØ", style="green")
        table.add_column("ÂàÜÊûêÊ∂àÊÅØ", style="yellow")
        table.add_column("Áä∂ÊÄÅ", style="magenta")
        
        for platform in set(list(collection_results.keys()) + list(analysis_results.keys())):
            # Collection info
            collection_info = collection_results.get(platform, {})
            collected = collection_info.get('stored_messages', 0)
            
            # Analysis info
            analysis_info = analysis_results.get(platform, {})
            if 'batch_result' in analysis_info:
                analyzed = analysis_info['batch_result'].processed_messages
                status = f"‚úÖ {analysis_info['batch_result'].success_rate:.1f}%"
            else:
                analyzed = 0
                status = "‚ùå Â§±Ë¥•"
            
            table.add_row(
                platform.title(),
                str(collected),
                str(analyzed),
                status
            )
        
        self.console.print(table)
        
        # Report info
        if report_paths:
            if len(report_paths) == 1:
                self.console.print(f"\nüìÑ ÁîüÊàêÁªºÂêàÊä•Âëä:")
            else:
                self.console.print(f"\nüìÑ ÁîüÊàê‰∫Ü {len(report_paths)} ‰∏™Êä•Âëä:")
            
            for path in report_paths:
                self.console.print(f"  ‚Ä¢ {Path(path).name}")
        
        self.console.print(Panel.fit("‚ú® TDXAgent ÊµÅÁ®ãÊâßË°åÂÆåÊàê!", style="bold green"))


# CLI Interface
@click.group()
@click.option('--config', default='config.yaml', help='ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ')
@click.pass_context
def cli(ctx, config):
    """TDXAgent - ‰∏™‰∫∫‰ø°ÊÅØ AI Âä©ÁêÜ"""
    ctx.ensure_object(dict)
    ctx.obj['config'] = config


@cli.command()
@click.option('--hours', default=12, help='Êî∂ÈõÜÂ§öÂ∞ëÂ∞èÊó∂ÂâçÁöÑÊï∞ÊçÆ')
@click.option('--platforms', help='ÊåáÂÆöÂπ≥Âè∞ (ÈÄóÂè∑ÂàÜÈöî)')
@click.pass_context
def collect(ctx, hours, platforms):
    """Êî∂ÈõÜÁ§æ‰∫§Â™í‰ΩìÊï∞ÊçÆ"""
    async def run():
        agent = TDXAgent(ctx.obj['config'])
        platform_list = platforms.split(',') if platforms else None
        results = await agent.run_collection(hours, platform_list)
        return results
    
    asyncio.run(run())


@cli.command()
@click.option('--hours', default=12, help='ÂàÜÊûêÂ§öÂ∞ëÂ∞èÊó∂ÂâçÁöÑÊï∞ÊçÆ')
@click.option('--platforms', help='ÊåáÂÆöÂπ≥Âè∞ (ÈÄóÂè∑ÂàÜÈöî)')
@click.pass_context
def analyze(ctx, hours, platforms):
    """ÂàÜÊûêÊî∂ÈõÜÁöÑÊï∞ÊçÆÂπ∂ÁîüÊàêÊä•Âëä"""
    async def run():
        agent = TDXAgent(ctx.obj['config'])
        platform_list = platforms.split(',') if platforms else None
        
        # ËøêË°åÂàÜÊûê
        analysis_results = await agent.run_analysis(platform_list, hours)
        
        # ÁîüÊàêÊä•Âëä
        report_paths = await agent.generate_reports(analysis_results, hours)
        
        # ÊòæÁ§∫ÁªìÊûú
        if report_paths:
            agent.console.print(f"\nüìÑ ÁîüÊàê‰∫Ü {len(report_paths)} ‰∏™Êä•Âëä:")
            for path in report_paths:
                agent.console.print(f"  ‚Ä¢ {Path(path).name}")
        else:
            agent.console.print("‚ùå Ê≤°ÊúâÁîüÊàêÊä•Âëä")
        
        return {'analysis_results': analysis_results, 'report_paths': report_paths}
    
    asyncio.run(run())


@cli.command()
@click.option('--hours', default=12, help='Â§ÑÁêÜÂ§öÂ∞ëÂ∞èÊó∂ÂâçÁöÑÊï∞ÊçÆ')
@click.option('--platforms', help='ÊåáÂÆöÂπ≥Âè∞ (ÈÄóÂè∑ÂàÜÈöî)')
@click.pass_context
def run(ctx, hours, platforms):
    """ËøêË°åÂÆåÊï¥ÁöÑ TDXAgent ÊµÅÁ®ã"""
    async def run_pipeline():
        agent = TDXAgent(ctx.obj['config'])
        platform_list = platforms.split(',') if platforms else None
        results = await agent.run_full_pipeline(hours, platform_list)
        return results
    
    asyncio.run(run_pipeline())


@cli.command()
@click.pass_context
def status(ctx):
    """ÊòæÁ§∫ TDXAgent Áä∂ÊÄÅ"""
    async def show_status():
        agent = TDXAgent(ctx.obj['config'])
        
        # Show configuration status
        console = Console()
        console.print("üìã TDXAgent Áä∂ÊÄÅ", style="bold blue")
        
        # Platform status
        table = Table(title="Âπ≥Âè∞ÈÖçÁΩÆ")
        table.add_column("Âπ≥Âè∞", style="cyan")
        table.add_column("Áä∂ÊÄÅ", style="green")
        
        platforms = ['twitter', 'telegram', 'discord']
        for platform in platforms:
            enabled = agent.config_manager.is_platform_enabled(platform)
            status = "‚úÖ ÂêØÁî®" if enabled else "‚ùå Á¶ÅÁî®"
            table.add_row(platform.title(), status)
        
        console.print(table)
        
        # Storage stats
        stats = await agent.storage.get_storage_stats()
        console.print(f"\nüíæ Â≠òÂÇ®ÁªüËÆ°: {stats['total_files']} Êñá‰ª∂")
    
    asyncio.run(show_status())


def main():
    """Main entry point."""
    cli()


if __name__ == '__main__':
    main()
