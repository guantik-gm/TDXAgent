"""
Twitter/X scraper for TDXAgent using Playwright.

This module provides Twitter data collection using Playwright with advanced
anti-detection measures and human-like behavior simulation.
"""

import asyncio
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone
import logging

from playwright.async_api import async_playwright, Browser, Page, BrowserContext
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from scrapers.base_scraper import BaseScraper, ScrapingResult
from utils.logger import TDXLogger, log_async_function_call
from storage.optimized_message_converter import OptimizedMessageConverter
from utils.helpers import ensure_directory, sanitize_filename
from utils.retry_handler import retry_on_failure, NetworkError, RateLimitError


class TwitterScraper(BaseScraper):
    """
    Twitter/X scraper using Playwright with anti-detection.
    
    Features:
    - Advanced anti-detection measures
    - Human-like behavior simulation
    - Cookie-based authentication
    - Following and For You timeline scraping
    - Media content extraction
    - Rate limiting compliance
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize Twitter scraper.
        
        Args:
            config: Twitter configuration dictionary
        """
        super().__init__(config, "twitter")
        
        # Browser configuration
        self.headless = config.get('headless', False)
        self.delay_range = config.get('delay_range', [2, 5])
        # æ»šåŠ¨æ¬¡æ•°é™åˆ¶å·²ç§»é™¤ï¼Œä½†ä¿ç•™å˜é‡é¿å…é”™è¯¯ï¼ˆè®¾ä¸ºæå¤§å€¼ï¼‰
        self.max_scrolls = config.get('max_scrolls', 999999)
        self.cookie_file = config.get('cookie_file', 'twitter_cookies.json')
        
        # æ»šåŠ¨æ€§èƒ½é…ç½®
        scroll_settings = config.get('scroll_settings', {})
        self.scroll_distance_range = scroll_settings.get('distance_range', [1500, 2500])
        self.content_wait_range = scroll_settings.get('content_wait_range', [2, 4])
        self.bottom_wait_range = scroll_settings.get('bottom_wait_range', [3, 6])
        self.scroll_mode = scroll_settings.get('mode', 'balanced')
        
        # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
        self._adjust_scroll_params_by_mode()
        self.login_timeout = config.get('login_timeout', 600)  # é»˜è®¤10åˆ†é’Ÿè¶…æ—¶
        
        # æŒä¹…åŒ–ç”¨æˆ·æ•°æ®ç›®å½•
        self.user_data_dir = config.get('user_data_dir', 'twitter_user_data')
        self.use_persistent_browser = config.get('use_persistent_browser', True)
        
        # Browser instances
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        # ä¼˜åŒ–ç‰ˆè½¬æ¢å™¨
        self.converter = OptimizedMessageConverter()
        
        # Anti-detection configuration
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°é…ç½®å†…å®¹
        self.logger.info(f"Twitter scraper initialized with collection_strategy: {config.get('collection_strategy', 'NOT_FOUND')}")
        self.logger.debug(f"Available config keys: {list(config.keys())}")
        self.logger.info("Initialized Twitter scraper")
    
    @retry_on_failure(max_retries=3, base_delay=2.0, max_delay=30.0)
    async def authenticate(self) -> bool:
        """
        Authenticate with Twitter using cookies or interactive login.
        
        Returns:
            True if authentication successful
        """
        try:
            # Initialize Playwright
            self.logger.info("Starting Playwright...")
            self.playwright = await async_playwright().start()
            self.logger.info("Playwright started successfully")
            
            # Get proxy configuration from config
            from config.config_manager import ConfigManager
            config_manager = ConfigManager()
            
            # Prepare browser launch args with SSL fixes
            launch_args = [
                '--no-blink-features=AutomationControlled',
                '--disable-blink-features=AutomationControlled', 
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--ignore-ssl-errors',
                '--ignore-certificate-errors',
                '--ignore-certificate-errors-spki-list',
                '--disable-ssl-false-start',
                '--disable-tls13-early-data',
                '--disable-features=VizDisplayCompositor'
            ]
            
            # Get proxy configuration (will be handled differently for persistent vs non-persistent)
            twitter_proxy = self.config.get('proxy', {})
            
            if self.use_persistent_browser:
                # ä½¿ç”¨æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡
                from pathlib import Path
                user_data_path = Path(self.user_data_dir)
                user_data_path.mkdir(exist_ok=True)
                
                self.logger.info(f"Launching persistent browser context with user data: {user_data_path}")
                
                # ä½¿ç”¨ launch_persistent_context è€Œä¸æ˜¯ launch + user-data-dir
                # æ„å»º proxy å‚æ•°
                proxy_config = None
                if twitter_proxy.get('enabled', False):
                    proxy_config = {
                        "server": f"{twitter_proxy.get('type', 'socks5')}://{twitter_proxy.get('host', '127.0.0.1')}:{twitter_proxy.get('port', 7890)}"
                    }
                    if twitter_proxy.get('username'):
                        proxy_config["username"] = twitter_proxy['username']
                        proxy_config["password"] = twitter_proxy['password']
                    self.logger.info(f"Using Twitter-specific proxy: {proxy_config['server']}")
                elif config_manager.proxy.enabled:
                    proxy_config = {
                        "server": f"{config_manager.proxy.type}://{config_manager.proxy.host}:{config_manager.proxy.port}"
                    }
                    if config_manager.proxy.username:
                        proxy_config["username"] = config_manager.proxy.username
                        proxy_config["password"] = config_manager.proxy.password
                    self.logger.info(f"Using global proxy: {proxy_config['server']}")
                else:
                    self.logger.info("No proxy configured for Twitter")
                
                self.context = await self.playwright.chromium.launch_persistent_context(
                    user_data_dir=str(user_data_path),
                    headless=self.headless,
                    args=launch_args,
                    proxy=proxy_config,
                    user_agent=random.choice(self.user_agents),
                    viewport={'width': 1920, 'height': 1080},
                    locale='en-US',
                    timezone_id='America/New_York'
                )
                self.logger.info("Persistent browser context created successfully")
                
                # è·å–å·²å­˜åœ¨çš„é¡µé¢æˆ–åˆ›å»ºæ–°é¡µé¢
                if self.context.pages:
                    self.page = self.context.pages[0]
                    self.logger.info("Using existing page from persistent context")
                else:
                    self.page = await self.context.new_page()
                    self.logger.info("Created new page in persistent context")
                
            else:
                # ä¼ ç»Ÿæ— ç—•æ¨¡å¼
                # Add proxy args for non-persistent mode
                if twitter_proxy.get('enabled', False):
                    proxy_url = f"{twitter_proxy.get('type', 'socks5')}://"
                    if twitter_proxy.get('username'):
                        proxy_url += f"{twitter_proxy['username']}:{twitter_proxy['password']}@"
                    proxy_url += f"{twitter_proxy.get('host', '127.0.0.1')}:{twitter_proxy.get('port', 7890)}"
                    launch_args.append(f'--proxy-server={proxy_url}')
                    self.logger.info(f"Using Twitter-specific proxy: {twitter_proxy.get('type')}://{twitter_proxy.get('host')}:{twitter_proxy.get('port')}")
                elif config_manager.proxy.enabled:
                    proxy_url = f"{config_manager.proxy.type}://"
                    if config_manager.proxy.username:
                        proxy_url += f"{config_manager.proxy.username}:{config_manager.proxy.password}@"
                    proxy_url += f"{config_manager.proxy.host}:{config_manager.proxy.port}"
                    launch_args.append(f'--proxy-server={proxy_url}')
                    self.logger.info(f"Using global proxy for Twitter: {config_manager.proxy.type}://{config_manager.proxy.host}:{config_manager.proxy.port}")
                else:
                    self.logger.info("No proxy configured for Twitter")
                
                self.logger.info(f"Launching browser (headless={self.headless})...")
                self.browser = await self.playwright.chromium.launch(
                    headless=self.headless,
                    args=launch_args
                )
                self.logger.info("Browser launched successfully")
                
                # Create context with anti-detection
                self.logger.info("Creating browser context...")
                self.context = await self.browser.new_context(
                    user_agent=random.choice(self.user_agents),
                    viewport={'width': 1920, 'height': 1080},
                    locale='en-US',
                    timezone_id='America/New_York',
                    permissions=['geolocation'],
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Cache-Control': 'max-age=0'
                    }
                )
                
                # Create page for non-persistent mode
                self.page = await self.context.new_page()
                
            self.logger.info("Browser context ready")
            
            # Page should already be created in both modes above
            if not self.page:
                self.logger.info("Creating new page...")
                self.page = await self.context.new_page()
                self.logger.info("Page created successfully")
            
            # Add stealth scripts
            self.logger.info("Adding stealth scripts...")
            await self._add_stealth_scripts()
            self.logger.info("Stealth scripts added")
            
            # Load cookies after page creation (ä»…åœ¨éæŒä¹…åŒ–æ¨¡å¼ä¸‹)
            if not self.use_persistent_browser:
                self.logger.info("Loading cookies from file...")
                await self._load_cookies()
            else:
                self.logger.info("Using persistent browser - cookies automatically loaded")
            
            # Navigate to Twitter with shorter timeout
            self.logger.info("Navigating to Twitter homepage...")
            try:
                await self.page.goto('https://x.com/home', wait_until='domcontentloaded', timeout=30000)  # 30ç§’è¶…æ—¶ï¼Œä½¿ç”¨domcontentloaded
                self.logger.info("Successfully navigated to Twitter")
                
                # Wait for basic content to load
                await asyncio.sleep(5)
                
            except Exception as e:
                self.logger.warning(f"Navigation issue: {e}, trying to continue...")
                # Try simpler navigation
                try:
                    await self.page.goto('https://x.com/home', timeout=15000)
                    await asyncio.sleep(3)
                    self.logger.info("Fallback navigation successful")
                except Exception as e2:
                    self.logger.error(f"Fallback navigation failed: {e2}")
            
            # Check if logged in
            self.logger.info("Checking login status...")
            await asyncio.sleep(3)
            
            if await self._is_logged_in():
                self.logger.info("Already authenticated with existing cookies")
                self._is_authenticated = True
                return True
            else:
                self.logger.info("Not logged in, manual authentication required")
                return await self._interactive_login()
                
        except Exception as e:
            self.logger.error(f"Twitter authentication failed: {e}")
            return False
    
    async def _load_cookies(self) -> None:
        """Load cookies from file."""
        cookie_path = Path(self.cookie_file)
        
        if cookie_path.exists():
            try:
                self.logger.info(f"Found cookie file: {cookie_path}")
                with open(cookie_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    self.logger.info(f"Cookie file size: {len(content)} characters")
                    
                # Parse JSON with better error handling
                try:
                    cookies = json.loads(content)
                    self.logger.info(f"Parsed {len(cookies)} cookies")
                except json.JSONDecodeError as e:
                    self.logger.error(f"JSON decode error at position {e.pos}: {e.msg}")
                    self.logger.error(f"Content around error: '{content[max(0, e.pos-20):e.pos+20]}'")
                    return
                
                # Validate cookie format
                for i, cookie in enumerate(cookies):
                    if not isinstance(cookie, dict) or 'name' not in cookie or 'value' not in cookie:
                        self.logger.error(f"Invalid cookie format at index {i}: {cookie}")
                        return
                
                await self.context.add_cookies(cookies)
                self.logger.info(f"Successfully loaded {len(cookies)} cookies")
                
                # Debug: Show which cookies were loaded
                cookie_names = [c['name'] for c in cookies]
                self.logger.info(f"Loaded cookies: {cookie_names}")
                
                # Check for important login cookies
                important_cookies = ['auth_token', 'ct0', 'twid']
                missing_cookies = [c for c in important_cookies if c not in cookie_names]
                if missing_cookies:
                    self.logger.warning(f"Missing important login cookies: {missing_cookies}")
                else:
                    self.logger.info("All important login cookies present")
                
            except Exception as e:
                self.logger.warning(f"Failed to load cookies: {e}")
        else:
            self.logger.info(f"No cookie file found at: {cookie_path}")
    
    async def _save_cookies(self) -> None:
        """Save cookies to file."""
        try:
            cookies = await self.context.cookies()
            
            with open(self.cookie_file, 'w') as f:
                json.dump(cookies, f, indent=2)
            
            self.logger.info("Saved cookies")
            
        except Exception as e:
            self.logger.warning(f"Failed to save cookies: {e}")
    
    async def _add_stealth_scripts(self) -> None:
        """Add stealth scripts to avoid detection."""
        stealth_script = """
        // Remove webdriver property
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
        });
        
        // Mock languages and plugins
        Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en'],
        });
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5],
        });
        
        // Mock permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
        """
        
        await self.page.add_init_script(stealth_script)
    
    async def _is_logged_in(self) -> bool:
        """Check if user is logged in."""
        try:
            self.logger.info("Checking login status...")
            
            # Wait for page to load
            await asyncio.sleep(3)
            
            # Check current URL first
            current_url = self.page.url
            self.logger.info(f"Current URL: {current_url}")
            
            # If we're on login page, definitely not logged in
            if '/login' in current_url or '/i/flow/login' in current_url:
                self.logger.info("On login page - not logged in")
                return False
            
            # Look for login indicators (these mean NOT logged in)
            login_selectors = [
                '[data-testid="loginButton"]',
                '[href="/login"]',
                'text=Log in',
                'text=Sign up'
            ]
            
            for selector in login_selectors:
                count = await self.page.locator(selector).count()
                if count > 0:
                    self.logger.info(f"Found login indicator: {selector}")
                    return False
            
            # Look for logged-in indicators
            logged_in_selectors = [
                '[data-testid="SideNav_AccountSwitcher_Button"]',
                '[data-testid="AppTabBar_Profile_Link"]',
                '[aria-label="Profile"]',
                '[data-testid="primaryColumn"]',
                '[data-testid="tweet"]'
            ]
            
            for selector in logged_in_selectors:
                count = await self.page.locator(selector).count()
                if count > 0:
                    self.logger.info(f"Found logged-in indicator: {selector}")
                    return True
            
            # Final check: if we're on home page and no login buttons, assume logged in
            if '/home' in current_url:
                self.logger.info("On home page with no login indicators - assuming logged in")
                return True
            
            self.logger.info("No clear indicators found - not logged in")
            return False
            
        except Exception as e:
            self.logger.warning(f"Failed to check login status: {e}")
            return False
    
    async def _interactive_login(self) -> bool:
        """Handle interactive login process."""
        try:
            self.logger.info("Please log in manually in the browser window")
            
            if self.headless:
                self.logger.error("Cannot perform interactive login in headless mode")
                return False
            
            # Wait for user to log in
            print("\n" + "="*50)
            print("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨ç™»å½• Twitter/X")
            print("ç™»å½•å®Œæˆåï¼Œè¯·æŒ‰ Enter é”®ç»§ç»­...")
            print("æˆ–è€…ç­‰å¾… 30 ç§’è‡ªåŠ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            print("="*50)
            
            try:
                # ä½¿ç”¨ timeout å¤„ç†ç”¨æˆ·è¾“å…¥
                import select
                import sys
                
                if select.select([sys.stdin], [], [], 30) == ([sys.stdin], [], []):
                    input()  # ç”¨æˆ·æŒ‰äº†Enter
                else:
                    self.logger.info("30ç§’è¶…æ—¶ï¼Œè‡ªåŠ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            except:
                # å¦‚æœselectä¸å¯ç”¨ï¼ˆWindowsï¼‰ï¼Œä½¿ç”¨ç®€å•çš„input
                input()
            
            # Check if now logged in
            if await self._is_logged_in():
                # ä»…åœ¨éæŒä¹…åŒ–æ¨¡å¼ä¸‹ä¿å­˜Cookieåˆ°æ–‡ä»¶
                if not self.use_persistent_browser:
                    await self._save_cookies()
                else:
                    self.logger.info("Persistent browser - cookies automatically saved")
                self._is_authenticated = True
                self.logger.info("Interactive login successful")
                return True
            else:
                self.logger.error("Login verification failed")
                return False
                
        except Exception as e:
            self.logger.error(f"Interactive login failed: {e}")
            return False
    
    async def scrape(self, hours_back: int = 12, **kwargs) -> ScrapingResult:
        """
        Scrape Twitter data from Following and For You timelines.
        
        Args:
            hours_back: Number of hours back to scrape
            **kwargs: Additional parameters
            
        Returns:
            ScrapingResult with collected tweets
        """
        if not self.page or not self._is_authenticated:
            error_msg = "Twitter scraper not authenticated"
            self.logger.error(error_msg)
            return self.create_scraping_result([], [error_msg])
        
        messages = []
        errors = []
        
        try:
            # Scrape Following timeline
            self.logger.info("Scraping Following timeline...")
            following_tweets = await self._scrape_timeline('following', hours_back)
            messages.extend(following_tweets)
            
            # Random delay between timelines
            await asyncio.sleep(random.uniform(3, 7))
            
            # Scrape For You timeline
            self.logger.info("Scraping For You timeline...")
            for_you_tweets = await self._scrape_timeline('for_you', hours_back)
            messages.extend(for_you_tweets)
            
            # Convert to unified format
            converted_messages = []
            for raw_tweet in messages:
                try:
                    converted = self.converter.convert_message(raw_tweet, 'twitter')
                    if converted:
                        converted_messages.append(converted)
                except Exception as e:
                    self.logger.warning(f"Failed to convert tweet: {e}")
            
            self.logger.info(f"Scraped {len(converted_messages)} tweets from Twitter")
            return self.create_scraping_result(converted_messages, errors)
            
        except Exception as e:
            error_msg = f"Twitter scraping failed: {e}"
            self.logger.error(error_msg)
            return self.create_scraping_result([], [error_msg])
    
    async def _scrape_timeline(self, timeline_type: str, hours_back: int) -> List[Dict[str, Any]]:
        """
        Scrape a specific timeline using count-based or time-based strategy.
        
        Args:
            timeline_type: 'following' or 'for_you'
            hours_back: Number of hours back to scrape (only used for time_based strategy)
            
        Returns:
            List of tweet dictionaries
        """
        tweets = []
        
        try:
            # Navigate to appropriate timeline
            self.logger.info(f"Navigating to {timeline_type} timeline...")
            if timeline_type == 'following':
                navigation_success = await self._navigate_to_following_timeline()
                if not navigation_success:
                    self.logger.warning("Following timeline navigation failed, continuing with current page")
            else:  # for_you
                navigation_success = await self._navigate_to_for_you_timeline()
                if not navigation_success:
                    self.logger.warning("For You timeline navigation failed, continuing with current page")
            
            # æ£€æŸ¥æ”¶é›†ç­–ç•¥
            collection_strategy = self.config.get('collection_strategy', 'time_based')
            self.logger.info(f"ä½¿ç”¨æ”¶é›†ç­–ç•¥: {collection_strategy}")
            self.logger.debug(f"Twitteré…ç½®å†…å®¹: {list(self.config.keys())}")
            
            if collection_strategy == 'count_based':
                # ä¸åŒé¡µé¢ä½¿ç”¨ä¸åŒç­–ç•¥
                if timeline_type == 'following':
                    # Followingé¡µé¢ï¼šæ—¶é—´ä¼˜å…ˆç­–ç•¥
                    self.logger.info(f"ğŸ“… Followingé¡µé¢ä½¿ç”¨æ—¶é—´ä¼˜å…ˆç­–ç•¥ï¼ˆæ—¶é—´é™åˆ¶>{hours_back}hï¼Œæ•°é‡é™åˆ¶ä½œä¸ºå¤‡é€‰ï¼‰")
                    tweets = await self._scrape_timeline_time_first(timeline_type, hours_back)
                else:  # for_you
                    # For Youé¡µé¢ï¼šçº¯æ•°é‡ç­–ç•¥ï¼ˆå¿½ç•¥æ—¶é—´ï¼‰
                    self.logger.info(f"ğŸ”¢ For Youé¡µé¢ä½¿ç”¨çº¯æ•°é‡ç­–ç•¥ï¼ˆå¿½ç•¥æ—¶é—´é™åˆ¶ï¼Œåªæ”¶é›†æŒ‡å®šæ•°é‡ï¼‰")
                    tweets = await self._scrape_timeline_by_count(timeline_type, hours_back=None)
            else:
                tweets = await self._scrape_timeline_by_time(timeline_type, hours_back)
                
        except Exception as e:
            self.logger.error(f"Error scraping {timeline_type} timeline: {e}")
            
        return tweets
    
    async def _scrape_timeline_by_count(self, timeline_type: str, hours_back: int = None) -> List[Dict[str, Any]]:
        """åŸºäºæ¨æ–‡æ•°é‡çš„æ”¶é›†ç­–ç•¥ï¼ˆä»ç„¶éµå®ˆæ—¶é—´è¾¹ç•Œï¼‰"""
        tweets = []
        max_tweets = self.config.get('max_tweets_per_run', 100)
        
        # è®¡ç®—æ—¶é—´è¾¹ç•Œï¼ˆå¦‚æœæä¾›äº†hours_backå‚æ•°ï¼‰
        cutoff_time = None
        if hours_back is not None:
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            self.logger.info(f"Starting count-based tweet collection for {timeline_type} timeline with time boundary...")
            self.logger.info(f"Target: {max_tweets} tweets (within {hours_back} hours from {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')})")
        else:
            self.logger.info(f"Starting count-based tweet collection for {timeline_type} timeline...")
            self.logger.info(f"Target: {max_tweets} tweets (no time boundary)")
        
        scroll_count = 0
        collected_tweet_ids = set()  # ç”¨äºå»é‡
        no_new_tweets_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡çš„æ¬¡æ•°
        consecutive_old_tweets = 0  # è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•°ï¼ˆä»…åœ¨æœ‰æ—¶é—´è¾¹ç•Œæ—¶ä½¿ç”¨ï¼‰
        max_consecutive_old = 10  # è¿ç»­è¶…æ—¶æ¨æ–‡é™åˆ¶
        
        while len(tweets) < max_tweets:
            self.logger.info(f"Scroll {scroll_count + 1} - Extracting tweets from current view...")
            
            # Extract tweets from current view
            current_tweets = await self._extract_tweets_from_page(timeline_type)
            
            # å»é‡å¤„ç†å’Œæ—¶é—´è¿‡æ»¤
            new_tweets = []
            valid_tweets = []  # åœ¨æ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡
            old_tweets_in_batch = 0  # æœ¬æ‰¹æ¬¡ä¸­çš„è¶…æ—¶æ¨æ–‡æ•°é‡
            oldest_tweet_time = None
            
            for tweet in current_tweets:
                tweet_id = tweet.get('id', f"tweet_{random.randint(1000000, 9999999)}")
                if tweet_id not in collected_tweet_ids:
                    collected_tweet_ids.add(tweet_id)
                    new_tweets.append(tweet)
                    
                    # æ—¶é—´è¿‡æ»¤ï¼ˆå¦‚æœè®¾ç½®äº†æ—¶é—´è¾¹ç•Œï¼‰
                    if cutoff_time is not None:
                        try:
                            tweet_time_str = tweet.get('created_at', '')
                            is_retweet = tweet.get('is_retweet', False)
                            tweet_time = self._parse_tweet_timestamp(tweet_time_str)
                            
                            # cutoff_time ç¡®ä¿æ˜¯ naive datetimeï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼‰
                            if cutoff_time.tzinfo is not None:
                                cutoff_time = cutoff_time.replace(tzinfo=None)
                            
                            # è·Ÿè¸ªæœ€æ—§çš„æ¨æ–‡æ—¶é—´
                            if oldest_tweet_time is None or tweet_time < oldest_tweet_time:
                                oldest_tweet_time = tweet_time
                            
                            # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                            if tweet_time >= cutoff_time:
                                if is_retweet:
                                    self.logger.debug(f"Count-based: Retweet {tweet_id} within time range (retweet time: {tweet_time})")
                                else:
                                    self.logger.debug(f"Count-based: Original tweet {tweet_id} within time range (created: {tweet_time})")
                                valid_tweets.append(tweet)
                            else:
                                old_tweets_in_batch += 1
                                if is_retweet:
                                    self.logger.debug(f"Count-based: Retweet {tweet_id} too old (retweet time: {tweet_time}), not collecting")
                                else:
                                    self.logger.debug(f"Count-based: Tweet {tweet_id} too old (created: {tweet_time}), not collecting")
                        except Exception as e:
                            # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼ŒåŒ…å«è¿™æ¡æ¨æ–‡ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                            self.logger.debug(f"Failed to parse tweet time: {e}, including tweet")
                            valid_tweets.append(tweet)
                    else:
                        # æ²¡æœ‰æ—¶é—´è¾¹ç•Œï¼Œæ·»åŠ æ‰€æœ‰æ–°æ¨æ–‡
                        valid_tweets.append(tweet)
            
            # æ·»åŠ æœ‰æ•ˆæ¨æ–‡åˆ°ç»“æœåˆ—è¡¨
            tweets.extend(valid_tweets)
            
            # å¦‚æœè¶…å‡ºç›®æ ‡æ•°é‡ï¼Œæˆªæ–­åˆ°ç›®æ ‡æ•°
            if len(tweets) > max_tweets:
                tweets = tweets[:max_tweets]
            
            new_tweets_count = len(new_tweets)
            valid_tweets_count = len(valid_tweets)
            total_tweets = len(tweets)
            
            # æ›´æ–°è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•°ï¼ˆä»…åœ¨æœ‰æ—¶é—´è¾¹ç•Œæ—¶ï¼‰
            if cutoff_time is not None:
                if old_tweets_in_batch > 0 and valid_tweets_count == 0:
                    consecutive_old_tweets += old_tweets_in_batch
                else:
                    consecutive_old_tweets = 0  # é‡ç½®è®¡æ•°
                
                # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯
                if oldest_tweet_time:
                    time_info = f", oldest: {oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S')}"
                else:
                    time_info = ""
                
                self.logger.info(f"å‘ç° {len(current_tweets)} æ¡æ¨æ–‡, {new_tweets_count} æ¡æ–°æ¨æ–‡, {valid_tweets_count} æ¡æœ‰æ•ˆæ¨æ–‡, æ€»è®¡: {total_tweets}/{max_tweets}{time_info}")
                
                if old_tweets_in_batch > 0:
                    self.logger.info(f"æœ¬æ‰¹æ¬¡ {old_tweets_in_batch} æ¡æ¨æ–‡è¶…å‡ºæ—¶é—´é™åˆ¶, è¿ç»­è¶…æ—¶: {consecutive_old_tweets}/{max_consecutive_old}")
                
                # æ£€æŸ¥æ˜¯å¦è¿ç»­å¤ªå¤šè¶…æ—¶æ¨æ–‡
                if consecutive_old_tweets >= max_consecutive_old:
                    self.logger.info(f"â° æ—¶é—´é™åˆ¶è§¦å‘ï¼šè¿ç»­å‘ç° {consecutive_old_tweets} æ¡è¶…å‡ºæ—¶é—´é™åˆ¶çš„æ¨æ–‡")
                    self.logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€ï¼šæ—¶é—´è¾¹ç•Œ={cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}, æœ€æ—§æ¨æ–‡æ—¶é—´={oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S') if oldest_tweet_time else 'None'}")
                    self.logger.info(f"ğŸ›‘ åœæ­¢æ”¶é›† - å·²è¾¾åˆ°è¿ç»­è¶…æ—¶æ¨æ–‡é™åˆ¶({max_consecutive_old}æ¡)")
                    break
            else:
                self.logger.info(f"å‘ç° {len(current_tweets)} æ¡æ¨æ–‡, {new_tweets_count} æ¡æ–°æ¨æ–‡, æ€»è®¡: {total_tweets}/{max_tweets}")
            
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(tweets) >= max_tweets:
                self.logger.info(f"å·²æ”¶é›†åˆ°ç›®æ ‡æ•°é‡ {max_tweets} æ¡æ¨æ–‡ï¼Œåœæ­¢æ”¶é›†")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡
            if new_tweets_count == 0:
                no_new_tweets_count += 1
                self.logger.warning(f"æœ¬æ¬¡æ»šåŠ¨æœªè·å–åˆ°æ–°æ¨æ–‡ ({no_new_tweets_count}/3)")
                if no_new_tweets_count >= 3:
                    self.logger.info("è¿ç»­3æ¬¡æ»šåŠ¨æœªè·å–åˆ°æ–°æ¨æ–‡ï¼Œåœæ­¢æ”¶é›†")
                    break
            else:
                no_new_tweets_count = 0  # é‡ç½®è®¡æ•°
            
            # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º
            await self._human_like_scroll()
            scroll_count += 1
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(*self.delay_range)
            await asyncio.sleep(delay)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†æ„å¤–å¯¼èˆª
            current_url = self.page.url
            if '/status/' in current_url and 'home' not in current_url:
                self.logger.warning("æ£€æµ‹åˆ°æ„å¤–å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ")
                recovery_success = await self._recover_from_accidental_navigation('https://x.com/home')
                if not recovery_success:
                    self.logger.error("æ— æ³•ä»æ„å¤–å¯¼èˆªä¸­æ¢å¤ï¼Œåœæ­¢æ”¶é›†")
                    break
        
        if cutoff_time is not None:
            self.logger.info(f"åŸºäºæ•°é‡çš„æ”¶é›†å®Œæˆï¼ˆå¸¦æ—¶é—´è¾¹ç•Œï¼‰ï¼šæ”¶é›†äº† {len(tweets)} æ¡æ¨æ–‡ï¼ˆ{hours_back}å°æ—¶å†…ï¼‰")
        else:
            self.logger.info(f"åŸºäºæ•°é‡çš„æ”¶é›†å®Œæˆï¼ˆçº¯æ•°é‡ç­–ç•¥ï¼‰ï¼šæ”¶é›†äº† {len(tweets)} æ¡æ¨æ–‡ï¼ˆFor Youé¡µé¢ï¼Œå¿½ç•¥æ—¶é—´ï¼‰")
        return tweets
    
    async def _scrape_timeline_time_first(self, timeline_type: str, hours_back: int) -> List[Dict[str, Any]]:
        """æ—¶é—´ä¼˜å…ˆç­–ç•¥ï¼šä¸“ç”¨äºFollowingé¡µé¢ï¼Œæ—¶é—´é™åˆ¶ä¼˜å…ˆäºæ¡æ•°é™åˆ¶"""
        tweets = []
        max_tweets = self.config.get('max_tweets_per_run', 100)
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        self.logger.info(f"Starting time-first tweet collection for {timeline_type} timeline...")
        self.logger.info(f"æ—¶é—´é™åˆ¶ä¼˜å…ˆ: {hours_back}å°æ—¶å†…çš„æ¨æ–‡ (ä» {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')} å¼€å§‹)")
        self.logger.info(f"æœ€å¤§æ•°é‡é™åˆ¶: {max_tweets} æ¡æ¨æ–‡ (æ—¶é—´é™åˆ¶ä¼˜å…ˆ)")
        self.logger.info(f"å½“å‰æœ¬åœ°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        scroll_count = 0
        collected_tweet_ids = set()  # ç”¨äºå»é‡
        no_new_tweets_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡çš„æ¬¡æ•°
        consecutive_old_tweets = 0  # è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•°
        max_consecutive_old = 3  # Followingé¡µé¢æ—¶é—´æ•æ„Ÿï¼Œè¿ç»­3æ¡å°±åœæ­¢
        
        while len(tweets) < max_tweets:
            self.logger.info(f"Scroll {scroll_count + 1} - Extracting tweets from current view...")
            
            # Extract tweets from current view
            current_tweets = await self._extract_tweets_from_page(timeline_type)
            
            # å»é‡å¤„ç†å’Œä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤
            new_tweets = []
            valid_tweets = []  # åœ¨æ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡
            old_tweets_in_batch = 0  # æœ¬æ‰¹æ¬¡ä¸­çš„è¶…æ—¶æ¨æ–‡æ•°é‡
            oldest_tweet_time = None
            
            for tweet in current_tweets:
                tweet_id = tweet.get('id', f"tweet_{random.randint(1000000, 9999999)}")
                if tweet_id not in collected_tweet_ids:
                    collected_tweet_ids.add(tweet_id)
                    new_tweets.append(tweet)
                    
                    # ä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤ï¼ˆFollowingé¡µé¢æ—¶é—´ä¼˜å…ˆç­–ç•¥ï¼‰
                    try:
                        tweet_time_str = tweet.get('created_at', '')
                        is_retweet = tweet.get('is_retweet', False)
                        tweet_time = self._parse_tweet_timestamp(tweet_time_str)
                        
                        # cutoff_time ç¡®ä¿æ˜¯ naive datetimeï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼‰
                        if cutoff_time.tzinfo is not None:
                            cutoff_time = cutoff_time.replace(tzinfo=None)
                        
                        # è·Ÿè¸ªæœ€æ—§çš„æ¨æ–‡æ—¶é—´
                        if oldest_tweet_time is None or tweet_time < oldest_tweet_time:
                            oldest_tweet_time = tweet_time
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                        if tweet_time >= cutoff_time:
                            if is_retweet:
                                self.logger.debug(f"Following: Retweet {tweet_id} within time range (retweet time: {tweet_time})")
                            else:
                                self.logger.debug(f"Following: Original tweet {tweet_id} within time range (created: {tweet_time})")
                            valid_tweets.append(tweet)
                        else:
                            old_tweets_in_batch += 1
                            if is_retweet:
                                self.logger.debug(f"Following: Retweet {tweet_id} too old (retweet time: {tweet_time}), not collecting")
                            else:
                                self.logger.debug(f"Following: Tweet {tweet_id} too old (created: {tweet_time}), not collecting")
                    except Exception as e:
                        # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼ŒåŒ…å«è¿™æ¡æ¨æ–‡ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                        self.logger.debug(f"Failed to parse tweet time: {e}, including tweet")
                        valid_tweets.append(tweet)
            
            # æ·»åŠ æœ‰æ•ˆæ¨æ–‡åˆ°ç»“æœåˆ—è¡¨
            tweets.extend(valid_tweets)
            
            new_tweets_count = len(new_tweets)
            valid_tweets_count = len(valid_tweets)
            total_tweets = len(tweets)
            
            # æ›´æ–°è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•° - æ—¶é—´ä¼˜å…ˆç­–ç•¥æ›´ä¸¥æ ¼
            if old_tweets_in_batch > 0 and valid_tweets_count == 0:
                consecutive_old_tweets += old_tweets_in_batch
            else:
                consecutive_old_tweets = 0  # é‡ç½®è®¡æ•°
            
            # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯
            if oldest_tweet_time:
                time_info = f", oldest: {oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                time_info = ""
            
            self.logger.info(f"å‘ç° {len(current_tweets)} æ¡æ¨æ–‡, {new_tweets_count} æ¡æ–°æ¨æ–‡, {valid_tweets_count} æ¡æœ‰æ•ˆæ¨æ–‡, æ€»è®¡: {total_tweets}/{max_tweets}{time_info}")
            
            if old_tweets_in_batch > 0:
                self.logger.info(f"æœ¬æ‰¹æ¬¡ {old_tweets_in_batch} æ¡æ¨æ–‡è¶…å‡ºæ—¶é—´é™åˆ¶, è¿ç»­è¶…æ—¶: {consecutive_old_tweets}/{max_consecutive_old}")
            
            # æ—¶é—´ä¼˜å…ˆç­–ç•¥ï¼šè¿ç»­å°‘é‡è¶…æ—¶æ¨æ–‡å°±åœæ­¢
            if consecutive_old_tweets >= max_consecutive_old:
                self.logger.info(f"ğŸ•’ æ—¶é—´é™åˆ¶è§¦å‘ï¼šè¿ç»­å‘ç° {consecutive_old_tweets} æ¡è¶…å‡º {hours_back} å°æ—¶æ—¶é—´é™åˆ¶çš„æ¨æ–‡")
                self.logger.info(f"ğŸ“Š Followingé¡µé¢çŠ¶æ€ï¼šæ—¶é—´è¾¹ç•Œ={cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}, æœ€æ—§æ¨æ–‡={oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S') if oldest_tweet_time else 'None'}")
                self.logger.info(f"ğŸ›‘ Followingé¡µé¢åœæ­¢æ”¶é›† - æ—¶é—´é™åˆ¶ä¼˜å…ˆç­–ç•¥(è¿ç»­{max_consecutive_old}æ¡è¶…æ—¶)")
                break
            
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æ•°é‡é™åˆ¶ï¼ˆæ¬¡è¦æ¡ä»¶ï¼‰
            if len(tweets) >= max_tweets:
                self.logger.info(f"ğŸ“Š æ•°é‡é™åˆ¶è§¦å‘ï¼šå·²æ”¶é›†åˆ°ç›®æ ‡æ•°é‡ {max_tweets} æ¡æ¨æ–‡ï¼Œåœæ­¢æ”¶é›†")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡
            if new_tweets_count == 0:
                no_new_tweets_count += 1
                self.logger.warning(f"æœ¬æ¬¡æ»šåŠ¨æœªè·å–åˆ°æ–°æ¨æ–‡ ({no_new_tweets_count}/3)")
                if no_new_tweets_count >= 3:
                    self.logger.info("è¿ç»­3æ¬¡æ»šåŠ¨æœªè·å–åˆ°æ–°æ¨æ–‡ï¼Œåœæ­¢æ”¶é›†")
                    break
            else:
                no_new_tweets_count = 0  # é‡ç½®è®¡æ•°
            
            # æ£€æŸ¥å¯¼èˆªçŠ¶æ€ï¼Œé˜²æ­¢æ„å¤–è·³è½¬åˆ°æ¨æ–‡è¯¦æƒ…é¡µ
            current_url = self.page.url
            if '/status/' in current_url and 'home' not in current_url:
                self.logger.warning("æ£€æµ‹åˆ°æ„å¤–å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ")
                recovery_success = await self._recover_from_accidental_navigation('https://x.com/home')
                if not recovery_success:
                    self.logger.error("æ— æ³•ä»æ„å¤–å¯¼èˆªä¸­æ¢å¤ï¼Œåœæ­¢æ”¶é›†")
                    break
            
            # Human-like scrolling
            await self._human_like_scroll()
            scroll_count += 1
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(*self.delay_range)
            await asyncio.sleep(delay)
        
        self.logger.info(f"æ—¶é—´ä¼˜å…ˆç­–ç•¥æ”¶é›†å®Œæˆï¼šæ”¶é›†äº† {len(tweets)} æ¡æ¨æ–‡ï¼ˆ{hours_back}å°æ—¶å†…ï¼ŒFollowingé¡µé¢ï¼‰")
        return tweets
    
    async def _scrape_timeline_by_time(self, timeline_type: str, hours_back: int) -> List[Dict[str, Any]]:
        """åŸºäºæ—¶é—´çš„æ”¶é›†ç­–ç•¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        tweets = []
        
        # Scroll and collect tweets with intelligent time-based stopping
        self.logger.info(f"Starting time-based tweet collection for {timeline_type} timeline...")
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        self.logger.info(f"Collecting tweets from {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')} onwards")
        
        scroll_count = 0
        collected_tweet_ids = set()  # ç”¨äºå»é‡
        no_new_tweets_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡çš„æ¬¡æ•°
        consecutive_old_tweets = 0  # è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•°
        max_consecutive_old = 15 if timeline_type == 'for_you' else 8  # For Youé¡µé¢æ›´å®½æ¾
        reached_time_limit = False
        
        while not reached_time_limit:
            self.logger.info(f"Scroll {scroll_count + 1} - Extracting tweets from current view...")
            
            # Extract tweets from current view
            current_tweets = await self._extract_tweets_from_page(timeline_type)
            
            # å»é‡å¤„ç†å’Œæ™ºèƒ½æ—¶é—´æ£€æŸ¥ï¼ˆæ”¯æŒè½¬å‘æ¨æ–‡æ—¶é—´åˆ¤æ–­ï¼‰
            new_tweets = []
            valid_tweets = []  # åœ¨æ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡
            old_tweets_in_batch = 0  # æœ¬æ‰¹æ¬¡ä¸­çš„è¶…æ—¶æ¨æ–‡æ•°é‡
            oldest_tweet_time = None
            
            for tweet in current_tweets:
                tweet_id = tweet.get('id', f"tweet_{random.randint(1000000, 9999999)}")
                if tweet_id not in collected_tweet_ids:
                    collected_tweet_ids.add(tweet_id)
                    new_tweets.append(tweet)
                    
                    # æ£€æŸ¥æ¨æ–‡æ—¶é—´ï¼ˆåŸºäºæ—¶é—´ç­–ç•¥ï¼‰
                    try:
                        tweet_time_str = tweet.get('created_at', '')
                        is_retweet = tweet.get('is_retweet', False)
                        tweet_time = self._parse_tweet_timestamp(tweet_time_str)
                        
                        # cutoff_time ç¡®ä¿æ˜¯ naive datetimeï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼‰
                        if cutoff_time.tzinfo is not None:
                            cutoff_time = cutoff_time.replace(tzinfo=None)
                        
                        # è·Ÿè¸ªæœ€æ—§çš„æ¨æ–‡æ—¶é—´
                        if oldest_tweet_time is None or tweet_time < oldest_tweet_time:
                            oldest_tweet_time = tweet_time
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                        if tweet_time >= cutoff_time:
                            if is_retweet:
                                self.logger.debug(f"Time-based: Retweet {tweet_id} within time range (retweet time: {tweet_time})")
                            else:
                                self.logger.debug(f"Time-based: Original tweet {tweet_id} within time range (created: {tweet_time})")
                            valid_tweets.append(tweet)
                        else:
                            old_tweets_in_batch += 1
                            if is_retweet:
                                self.logger.debug(f"Time-based: Retweet {tweet_id} too old (retweet time: {tweet_time}), not collecting")
                            else:
                                self.logger.debug(f"Time-based: Tweet {tweet_id} too old (created: {tweet_time}), not collecting")
                    except Exception as e:
                        # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼ŒåŒ…å«è¿™æ¡æ¨æ–‡ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                        self.logger.debug(f"Failed to parse tweet time: {e}, including tweet")
                        valid_tweets.append(tweet)
            
            # åªæ·»åŠ æ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡åˆ°æœ€ç»ˆç»“æœ
            tweets.extend(valid_tweets)
            
            new_tweets_count = len(new_tweets)
            valid_tweets_count = len(valid_tweets)
            total_tweets = len(tweets)
            
            # æ›´æ–°è¿ç»­è¶…æ—¶æ¨æ–‡è®¡æ•°
            if old_tweets_in_batch > 0 and valid_tweets_count == 0:
                consecutive_old_tweets += old_tweets_in_batch
            else:
                consecutive_old_tweets = 0  # é‡ç½®è®¡æ•°
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if oldest_tweet_time:
                time_info = f", oldest: {oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S')}"
                
                # æ™ºèƒ½åœæ­¢åˆ¤æ–­
                if consecutive_old_tweets >= max_consecutive_old:
                    reached_time_limit = True
                    self.logger.info(f"â° åŸºäºæ—¶é—´ç­–ç•¥è§¦å‘ï¼šè¿ç»­å‘ç° {consecutive_old_tweets} æ¡è¶…å‡ºæ—¶é—´é™åˆ¶çš„æ¨æ–‡")
                    self.logger.info(f"ğŸ“Š æ—¶é—´ç­–ç•¥çŠ¶æ€ï¼šæ—¶é—´è¾¹ç•Œ={cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}, æœ€æ—§æ¨æ–‡={oldest_tweet_time.strftime('%Y-%m-%d %H:%M:%S') if oldest_tweet_time else 'None'}")
                    self.logger.info(f"ğŸ›‘ åŸºäºæ—¶é—´ç­–ç•¥åœæ­¢æ”¶é›† - è¿ç»­{max_consecutive_old}æ¡è¶…æ—¶æ¨æ–‡")
            else:
                time_info = ""
            
            self.logger.info(f"å‘ç° {len(current_tweets)} æ¡æ¨æ–‡, {new_tweets_count} æ¡æ–°æ¨æ–‡, {valid_tweets_count} æ¡æœ‰æ•ˆæ¨æ–‡, æ€»è®¡: {total_tweets}{time_info}")
            
            if old_tweets_in_batch > 0:
                self.logger.info(f"æœ¬æ‰¹æ¬¡ {old_tweets_in_batch} æ¡æ¨æ–‡è¶…å‡ºæ—¶é—´é™åˆ¶, è¿ç»­è¶…æ—¶: {consecutive_old_tweets}/{max_consecutive_old}")
            
            # å¦‚æœè¾¾åˆ°è¿ç»­è¶…æ—¶é™åˆ¶ï¼Œåœæ­¢æ»šåŠ¨
            if reached_time_limit:
                self.logger.info(f"åœæ­¢æ”¶é›† - è¿ç»­è¶…å‡º {hours_back} å°æ—¶æ—¶é—´é™åˆ¶")
                break
            
            # Check if we got new tweets
            if new_tweets_count == 0:
                no_new_tweets_count += 1
                self.logger.info(f"No new tweets found ({no_new_tweets_count} times)")
                
                # å¦‚æœè¿ç»­3æ¬¡æ²¡æœ‰æ–°æ¨æ–‡ï¼Œåœæ­¢
                if no_new_tweets_count >= 3:
                    self.logger.info("No new tweets for 3 scrolls, stopping...")
                    break
            else:
                no_new_tweets_count = 0  # é‡ç½®è®¡æ•°å™¨
            
            # æ£€æŸ¥å¯¼èˆªçŠ¶æ€ï¼Œé˜²æ­¢æ„å¤–è·³è½¬åˆ°æ¨æ–‡è¯¦æƒ…é¡µ
            current_url = self.page.url
            if '/status/' in current_url and 'home' not in current_url:
                self.logger.warning("æ£€æµ‹åˆ°æ„å¤–å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ")
                recovery_success = await self._recover_from_accidental_navigation('https://x.com/home')
                if not recovery_success:
                    self.logger.error("æ— æ³•ä»æ„å¤–å¯¼èˆªä¸­æ¢å¤ï¼Œåœæ­¢æ”¶é›†")
                    break
            
            # Human-like scrolling
            await self._human_like_scroll()
            scroll_count += 1
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(*self.delay_range)
            await asyncio.sleep(delay)
        
        self.logger.info(f"åŸºäºæ—¶é—´çš„æ”¶é›†å®Œæˆï¼šæ”¶é›†äº† {len(tweets)} æ¡æ¨æ–‡")
        return tweets
    
    async def _extract_tweets_from_page(self, timeline_source: str = 'unknown') -> List[Dict[str, Any]]:
        """Extract tweet data from current page with improved stability.
        
        Args:
            timeline_source: Source timeline ('following', 'for_you', or 'unknown')
        """
        tweets = []
        
        try:
            # ç­‰å¾…é¡µé¢ç¨³å®š
            self.logger.debug("Waiting for page to stabilize before extraction...")
            await asyncio.sleep(1.0)  # é¢å¤–ç­‰å¾…ç¡®ä¿é¡µé¢ç¨³å®š
            
            self.logger.debug("Looking for tweet elements on page...")
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾æ¨æ–‡
            tweet_selectors = [
                'article[data-testid="tweet"]',
                'div[data-testid="tweet"]', 
                'article[role="article"]',
                'div[data-testid="cellInnerDiv"] article'
            ]
            
            tweet_elements = []
            for selector in tweet_selectors:
                try:
                    # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10ç§’
                    elements = await asyncio.wait_for(
                        self.page.locator(selector).all(), 
                        timeout=10.0
                    )
                    if elements:
                        tweet_elements = elements
                        self.logger.debug(f"Found tweet elements with selector: {selector}")
                        break
                except asyncio.TimeoutError:
                    self.logger.debug(f"Timeout finding elements with selector: {selector}")
                    continue
                except Exception as e:
                    self.logger.debug(f"Error with selector {selector}: {e}")
                    continue
            
            element_count = len(tweet_elements)
            self.logger.debug(f"Found {element_count} tweet elements total")
            
            # åˆ†æ‰¹å¤„ç†æ¨æ–‡å…ƒç´ ï¼Œå‡å°‘åŒæ—¶å¤„ç†çš„å‹åŠ›
            batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ¨æ–‡
            for batch_start in range(0, element_count, batch_size):
                batch_end = min(batch_start + batch_size, element_count)
                batch_elements = tweet_elements[batch_start:batch_end]
                
                self.logger.debug(f"Processing tweet batch {batch_start//batch_size + 1}: elements {batch_start+1}-{batch_end}")
                
                # å¹¶å‘å¤„ç†æ‰¹æ¬¡å†…çš„æ¨æ–‡
                batch_tasks = []
                for i, element in enumerate(batch_elements):
                    global_index = batch_start + i
                    task = self._extract_single_tweet_safe(element, global_index + 1, element_count, timeline_source)
                    batch_tasks.append(task)
                
                # ç­‰å¾…æ‰¹æ¬¡å®Œæˆï¼Œè®¾ç½®åˆç†è¶…æ—¶
                try:
                    batch_results = await asyncio.wait_for(
                        asyncio.gather(*batch_tasks, return_exceptions=True),
                        timeout=30.0  # æ•´ä¸ªæ‰¹æ¬¡30ç§’è¶…æ—¶
                    )
                    
                    # æ”¶é›†æœ‰æ•ˆç»“æœ
                    for result in batch_results:
                        if isinstance(result, dict) and result:
                            tweets.append(result)
                        elif isinstance(result, Exception):
                            self.logger.debug(f"Batch processing exception: {result}")
                            
                except asyncio.TimeoutError:
                    self.logger.warning(f"Batch {batch_start//batch_size + 1} processing timeout, continuing")
                    continue
                
                # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯
                if batch_end < element_count:
                    await asyncio.sleep(0.2)
            
            self.logger.debug(f"Successfully extracted {len(tweets)} tweets from page")
            
        except Exception as e:
            self.logger.error(f"Failed to extract tweets from page: {e}")
        
        return tweets
    
    async def _extract_single_tweet_safe(self, element, index: int, total: int, timeline_source: str = 'unknown') -> Optional[Dict[str, Any]]:
        """å®‰å…¨åœ°æå–å•ä¸ªæ¨æ–‡æ•°æ®ï¼ŒåŒ…å«å®¹é”™å¤„ç†
        
        Args:
            element: Tweet DOM element
            index: Current tweet index
            total: Total tweet count
            timeline_source: Source timeline ('following', 'for_you', or 'unknown')
        """
        try:
            self.logger.debug(f"Extracting data from tweet {index}/{total}...")
            
            # ç®€åŒ–çš„DOMè¿æ¥æ£€æŸ¥ï¼Œå‡å°‘è¶…æ—¶
            try:
                # ä½¿ç”¨æ›´ç®€å•çš„æ£€æŸ¥æ–¹å¼
                await element.is_visible()
            except Exception as e:
                self.logger.debug(f"Tweet {index} visibility check failed: {e}, trying anyway")
                # ä¸è·³è¿‡ï¼Œç»§ç»­å°è¯•æå–
            
            # æå–æ¨æ–‡æ•°æ®ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
            try:
                tweet_data = await asyncio.wait_for(
                    self._extract_tweet_data(element, timeline_source), 
                    timeout=15.0  # å¢åŠ åˆ°15ç§’è¶…æ—¶
                )
                
                if tweet_data:
                    self.logger.debug(f"Successfully extracted tweet {index}: {tweet_data.get('id', 'unknown')}")
                    return tweet_data
                else:
                    self.logger.debug(f"Tweet {index} data extraction returned None")
                    return None
                    
            except asyncio.TimeoutError:
                self.logger.warning(f"Timeout extracting tweet {index}, skipping")
                return None
                
        except Exception as e:
            self.logger.warning(f"Failed to extract tweet {index} data: {e}")
            return None
    
    async def _extract_tweet_data(self, tweet_element, timeline_source: str = 'unknown') -> Optional[Dict[str, Any]]:
        """Extract data from a single tweet element with timeout protection.
        
        Args:
            tweet_element: The tweet DOM element to extract data from
            timeline_source: Source timeline ('following', 'for_you', or 'unknown')
        """
        try:
            # æå–æ¨æ–‡çš„å®Œæ•´æ•°æ®ï¼ŒåŒ…æ‹¬é•¿æ¨æ–‡å¤„ç†ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰è¶…æ—¶ä¿æŠ¤
            
            # æå–æ¨æ–‡IDï¼ˆå¿«é€Ÿæ“ä½œï¼ŒçŸ­è¶…æ—¶ï¼‰
            try:
                tweet_id = await asyncio.wait_for(
                    self._extract_tweet_id(tweet_element), 
                    timeout=3.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Tweet ID extraction timeout, using fallback")
                import time
                tweet_id = f"tweet_{int(time.time() * 1000)}"
            
            # æå–æ¨æ–‡æ–‡æœ¬ï¼ˆå¯èƒ½éœ€è¦å¤„ç†é•¿æ¨æ–‡ï¼Œè¾ƒé•¿è¶…æ—¶ï¼‰
            try:
                tweet_text = await asyncio.wait_for(
                    self._extract_tweet_text(tweet_element), 
                    timeout=5.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Tweet text extraction timeout, using empty text")
                tweet_text = ""
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æ¨æ–‡ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            try:
                quoted_tweet = await asyncio.wait_for(
                    self._extract_quoted_tweet(tweet_element), 
                    timeout=2.0
                )
                if quoted_tweet:
                    tweet_text += f"\n\n[å¼•ç”¨æ¨æ–‡: {quoted_tweet}]"
            except asyncio.TimeoutError:
                self.logger.debug("Quoted tweet extraction timeout, skipping")
                quoted_tweet = None
            
            # æå–ä½œè€…ä¿¡æ¯ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            try:
                author = await asyncio.wait_for(
                    self._extract_author_info(tweet_element), 
                    timeout=3.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Author info extraction timeout, using default")
                author = {'name': 'Unknown', 'username': 'unknown', 'id': 'unknown', 'avatar_url': ''}
            
            # æå–æ—¶é—´æˆ³å’Œè½¬å‘çŠ¶æ€ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            try:
                created_at, is_retweet = await asyncio.wait_for(
                    self._extract_timestamp(tweet_element), 
                    timeout=2.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Timestamp extraction timeout, using current time")
                from datetime import datetime
                created_at = datetime.now().isoformat()
                is_retweet = False
            
            # æå–åª’ä½“URLï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            try:
                media_urls = await asyncio.wait_for(
                    self._extract_media_urls(tweet_element), 
                    timeout=2.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Media URLs extraction timeout, using empty list")
                media_urls = []
            
            # æå–äº’åŠ¨æ•°æ®ï¼ˆå¯èƒ½æ…¢ä¸€äº›ï¼‰
            try:
                engagement = await asyncio.wait_for(
                    self._extract_engagement_metrics(tweet_element), 
                    timeout=3.0
                )
            except asyncio.TimeoutError:
                self.logger.debug("Engagement metrics extraction timeout, using zeros")
                engagement = {'likes': 0, 'retweets': 0, 'replies': 0, 'quotes': 0}
            
            tweet_data = {
                'id': tweet_id,
                'text': tweet_text,
                'author': author,
                'created_at': created_at,
                'media_urls': media_urls,
                'engagement': engagement,
                'quoted_tweet': quoted_tweet,
                'is_retweet': is_retweet,  # æ·»åŠ è½¬å‘æ ‡è¯†
                'timeline_source': timeline_source  # æ·»åŠ æ—¶é—´çº¿æ¥æºæ ‡è¯†
            }
            
            return tweet_data
            
        except Exception as e:
            self.logger.warning(f"Failed to extract tweet data: {e}")
            return None
    
    async def _extract_tweet_id(self, element) -> str:
        """Extract tweet ID from element."""
        try:
            # å°è¯•å¤šç§æ–¹å¼è·å–æ¨æ–‡ID
            id_selectors = [
                # æŸ¥æ‰¾æ¨æ–‡é“¾æ¥
                'a[href*="/status/"]',
                '[data-testid="User-Name"] + div a[href*="/status/"]',
                'time[datetime] > a[href*="/status/"]',
                'time a[href*="/status/"]',
                '[data-testid="tweet"] a[href*="/status/"]',
                # æ—¶é—´å…ƒç´ é€šå¸¸åŒ…å«æ¨æ–‡é“¾æ¥
                'time[datetime]',
                # æ¨æ–‡å…ƒç´ æœ¬èº«å¯èƒ½æœ‰IDå±æ€§
                '[data-testid="tweet"]'
            ]
            
            for selector in id_selectors:
                elements = element.locator(selector)
                count = await elements.count()
                
                for i in range(count):
                    elem = elements.nth(i)
                    
                    # æ£€æŸ¥hrefå±æ€§
                    href = await elem.get_attribute('href')
                    if href and '/status/' in href:
                        parts = href.split('/status/')
                        if len(parts) > 1:
                            tweet_id = parts[1].split('?')[0].split('/')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œè·¯å¾„
                            if tweet_id and tweet_id.isdigit():
                                self.logger.debug(f"Extracted tweet ID: {tweet_id}")
                                return tweet_id
                    
                    # æ£€æŸ¥datetimeå±æ€§ï¼ˆæ—¶é—´å…ƒç´ ï¼‰
                    datetime_attr = await elem.get_attribute('datetime')
                    if datetime_attr:
                        # æ—¶é—´å…ƒç´ çš„çˆ¶å…ƒç´ é€šå¸¸æ˜¯é“¾æ¥
                        parent = elem.locator('..')
                        parent_href = await parent.get_attribute('href')
                        if parent_href and '/status/' in parent_href:
                            parts = parent_href.split('/status/')
                            if len(parts) > 1:
                                tweet_id = parts[1].split('?')[0].split('/')[0]
                                if tweet_id and tweet_id.isdigit():
                                    self.logger.debug(f"Extracted tweet ID from time element: {tweet_id}")
                                    return tweet_id
            
            # å¦‚æœè¿˜æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»é¡µé¢URLè·å–
            current_url = self.page.url if self.page else ""
            if '/status/' in current_url:
                parts = current_url.split('/status/')
                if len(parts) > 1:
                    tweet_id = parts[1].split('?')[0].split('/')[0]
                    if tweet_id and tweet_id.isdigit():
                        return tweet_id
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„IDä»¥ä¾¿è°ƒè¯•
            import time
            timestamp_id = f"tweet_{int(time.time() * 1000)}"
            self.logger.warning(f"Could not extract real tweet ID, using: {timestamp_id}")
            return timestamp_id
            
        except Exception as e:
            self.logger.warning(f"Failed to extract tweet ID: {e}")
            import time
            return f"tweet_{int(time.time() * 1000)}"
    
    async def _extract_tweet_text(self, element) -> str:
        """Extract tweet text content, handling long tweets."""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰"æŸ¥çœ‹æ›´å¤š"æˆ–"Show more"æŒ‰é’®
            await self._expand_long_tweet(element)
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ¨æ–‡æ–‡æœ¬
            text_selectors = [
                '[data-testid="tweetText"]',
                'div[data-testid="tweetText"]',
                'span[data-testid="tweetText"]',
                # æ¨æ–‡æ–‡æœ¬é€šå¸¸åœ¨æœ‰langå±æ€§çš„divä¸­
                'div[lang]',
                'div[lang] span',
                '[data-testid="tweet"] div[lang]',
                'div[data-testid="tweetText"] span',
                # æ›´å¹¿æ³›çš„æ–‡æœ¬é€‰æ‹©å™¨
                '[data-testid="tweet"] span[lang]',
                'article div[lang]',
                # å¤‡ç”¨é€‰æ‹©å™¨
                '[data-testid="tweet"] div:not([data-testid]) span',
                'article span:not([data-testid])'
            ]
            
            for selector in text_selectors:
                text_element = element.locator(selector)
                if await text_element.count() > 0:
                    # è·å–æ‰€æœ‰åŒ¹é…å…ƒç´ çš„æ–‡æœ¬å¹¶ç»„åˆ
                    all_elements = await text_element.all()
                    text_parts = []
                    for elem in all_elements:
                        text = await elem.inner_text()
                        if text.strip():
                            text_parts.append(text.strip())
                    
                    if text_parts:
                        full_text = ' '.join(text_parts)
                        self.logger.debug(f"Extracted tweet text (length: {len(full_text)}): {full_text[:100]}...")
                        return full_text
            
            # å¦‚æœä»¥ä¸Šéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ•´ä¸ªæ¨æ–‡åŒºåŸŸçš„æ–‡æœ¬
            tweet_content = element.locator('div[data-testid="tweetText"]')
            if await tweet_content.count() > 0:
                return await tweet_content.inner_text()
                
        except Exception as e:
            self.logger.warning(f"Failed to extract tweet text: {e}")
        
        return ""
    
    async def _expand_long_tweet(self, element) -> None:
        """Expand long tweets by clicking 'æ˜¾ç¤ºæ›´å¤š'/'Show more' buttons with case-insensitive matching and navigation protection."""
        try:
            # è®°å½•å½“å‰URLï¼Œç”¨äºæ£€æµ‹æ„å¤–å¯¼èˆª
            original_url = self.page.url
            
            # è°ƒè¯•ï¼šé¦–å…ˆæ£€æŸ¥æ¨æ–‡å…ƒç´ å†…çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹
            try:
                tweet_text = await element.inner_text()
                self.logger.debug(f"ğŸ” æ¨æ–‡å†…å®¹é¢„è§ˆ: {tweet_text[:200]}...")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å±•å¼€ç›¸å…³çš„æ–‡æœ¬
                if any(text in tweet_text.lower() for text in ['show more', 'æ˜¾ç¤ºæ›´å¤š', 'æŸ¥çœ‹æ›´å¤š']):
                    self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°åŒ…å«å±•å¼€æ–‡æœ¬çš„æ¨æ–‡ï¼Œå¼€å§‹æŸ¥æ‰¾æŒ‰é’®...")
                else:
                    self.logger.debug(f"ğŸ“ æ¨æ–‡ä¸åŒ…å«å±•å¼€æ–‡æœ¬ï¼Œè·³è¿‡æŒ‰é’®æœç´¢")
                    return
            except Exception as e:
                self.logger.debug(f"è·å–æ¨æ–‡æ–‡æœ¬å¤±è´¥: {e}")
            
            # é•¿æ¨æ–‡å±•å¼€æŒ‰é’®é€‰æ‹©å™¨ - 2025å¹´7æœˆæ›´æ–°ç‰ˆæœ¬ï¼ŒTwitterå·²ç§»é™¤role="button"
            show_more_selectors = [
                # === æ–°ç‰ˆæœ¬é€‰æ‹©å™¨ï¼šåŸºäºè°ƒè¯•å‘ç°çš„å®é™…DOMç»“æ„ ===
                # ä¸ä¾èµ–role="button"ï¼Œç›´æ¥åŒ¹é…spanå…ƒç´ 
                'span:text-is("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'span:text-is("Show more"):not(a):not(a *)',
                'span:text-is("show more"):not(a):not(a *)',
                'span:text-is("SHOW MORE"):not(a):not(a *)',
                'span:text-is("Show More"):not(a):not(a *)',
                
                # === åŸºäºCSSç±»çš„æ›´ç²¾ç¡®åŒ¹é… ===
                'span.css-1jxf684:text-is("æ˜¾ç¤ºæ›´å¤š")',
                'span.css-1jxf684:text-is("Show more")',
                'span.css-1jxf684:text-is("show more")',
                'span.css-1jxf684:text-is("SHOW MORE")',
                'span.css-1jxf684:text-is("Show More")',
                
                # === é€šè¿‡çˆ¶å…ƒç´ å®šä½ ===
                'div[lang] span:text-is("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'div[lang] span:text-is("Show more"):not(a):not(a *)',
                'div[lang] span:text-is("show more"):not(a):not(a *)',
                
                # === æ¨æ–‡å†…å®¹åŒºåŸŸå†… ===
                'div[data-testid="tweetText"] span:text-is("æ˜¾ç¤ºæ›´å¤š")',
                'div[data-testid="tweetText"] span:text-is("Show more")',
                'div[data-testid="tweetText"] span:text-is("show more")',
                
                # === ä½¿ç”¨has-textçš„æ¨¡ç³ŠåŒ¹é… ===
                'span:has-text("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'span:has-text("show more"):not(a):not(a *)',
                'div[data-testid="tweetText"] span:has-text("æ˜¾ç¤ºæ›´å¤š")',
                'div[data-testid="tweetText"] span:has-text("show more")',
                'div[lang] span:has-text("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'div[lang] span:has-text("show more"):not(a):not(a *)',
                
                # === å…¼å®¹æ—§ç‰ˆæœ¬çš„é€‰æ‹©å™¨ï¼ˆä¿ç•™ä»¥é˜²å›æ»šï¼‰ ===
                'span[role="button"]:text-is("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'span[role="button"]:text-is("Show more"):not(a):not(a *)',
                'button:text-is("æ˜¾ç¤ºæ›´å¤š"):not(a):not(a *)',
                'button:text-is("Show more"):not(a):not(a *)'
            ]
            
            expanded_count = 0
            self.logger.info(f"ğŸ” å¼€å§‹å°è¯• {len(show_more_selectors)} ä¸ªé€‰æ‹©å™¨æœç´¢å±•å¼€æŒ‰é’®...")
            
            for i, selector in enumerate(show_more_selectors):
                show_more_button = element.locator(selector)
                button_count = await show_more_button.count()
                if button_count > 0:
                    self.logger.info(f"ğŸ” é€‰æ‹©å™¨ {i+1} æ‰¾åˆ° {button_count} ä¸ªåŒ¹é…: {selector[:80]}...")
                else:
                    self.logger.debug(f"ğŸ” é€‰æ‹©å™¨ {i+1} æ— åŒ¹é…: {selector[:80]}...")
                
                if button_count > 0:
                    button_text = await show_more_button.first.inner_text() if button_count > 0 else "unknown"
                    self.logger.info(f"ğŸ” å‘ç°é•¿æ¨æ–‡å±•å¼€æŒ‰é’® '{button_text}' (é€‰æ‹©å™¨: {selector[:50]}...)")
                    try:
                        # ä½¿ç”¨æœ€å®‰å…¨çš„ç‚¹å‡»æ–¹å¼ï¼šå…ˆæ£€æŸ¥å…ƒç´ å±æ€§
                        button_element = show_more_button.first
                        
                        # éªŒè¯å…ƒç´ ä¸æ˜¯é“¾æ¥
                        tag_name = await button_element.evaluate("el => el.tagName.toLowerCase()")
                        has_href = await button_element.evaluate("el => el.hasAttribute('href')")
                        is_visible = await button_element.is_visible()
                        is_enabled = await button_element.is_enabled()
                        
                        if tag_name == 'a' or has_href:
                            self.logger.debug(f"Skipping selector {selector} - element is a link")
                            continue
                            
                        if not is_visible or not is_enabled:
                            self.logger.debug(f"Skipping selector {selector} - element not visible/enabled")
                            continue
                        
                        # ä½¿ç”¨å®‰å…¨çš„ç‚¹å‡»æ–¹å¼
                        self.logger.debug(f"Performing safe click on long tweet '{button_text}' button...")
                        await button_element.click(
                            timeout=3000, 
                            force=False,  # ä¸å¼ºåˆ¶ç‚¹å‡»ï¼Œç¡®ä¿å…ƒç´ å¯è§
                            no_wait_after=False  # ç­‰å¾…å¯¼èˆªå®Œæˆ
                        )
                        
                        # ç«‹å³æ£€æŸ¥å¯¼èˆªçŠ¶æ€
                        await asyncio.sleep(0.3)
                        current_url = self.page.url
                        if current_url != original_url:
                            self.logger.warning(f"Accidental navigation detected! From {original_url} to {current_url}")
                            # ç«‹å³è¿”å›åŸé¡µé¢
                            await self._recover_from_accidental_navigation(original_url)
                            return
                        
                        # ç­‰å¾…å†…å®¹å±•å¼€
                        await asyncio.sleep(1.0)
                        expanded_count += 1
                        self.logger.info(f"âœ… æˆåŠŸå±•å¼€é•¿æ¨æ–‡: '{button_text}' æŒ‰é’®ç‚¹å‡»æˆåŠŸ")
                        
                        # ç»§ç»­æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–å±•å¼€æŒ‰é’®
                        continue
                        
                    except Exception as e:
                        self.logger.debug(f"Failed to click long tweet expansion button '{button_text}' with selector {selector}: {e}")
                        # æ£€æŸ¥æ˜¯å¦å› ä¸ºæ„å¤–å¯¼èˆªè€Œå¤±è´¥
                        current_url = self.page.url
                        if current_url != original_url:
                            self.logger.warning(f"Navigation occurred during click attempt: {current_url}")
                            await self._recover_from_accidental_navigation(original_url)
                            return
                        continue
            
            if expanded_count > 0:
                self.logger.info(f"ğŸ¯ æœ¬æ¡æ¨æ–‡æ€»å…±å±•å¼€äº† {expanded_count} ä¸ªé•¿æ¨æ–‡æŒ‰é’®")
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æŒ‰é’®ï¼Œä½†æ¨æ–‡åŒ…å«å±•å¼€æ–‡æœ¬ï¼Œé‚£ä¹ˆè°ƒè¯•DOMç»“æ„
                try:
                    tweet_text = await element.inner_text()
                    if any(text in tweet_text.lower() for text in ['show more', 'æ˜¾ç¤ºæ›´å¤š', 'æŸ¥çœ‹æ›´å¤š']):
                        self.logger.warning("âš ï¸ æ¨æ–‡åŒ…å«å±•å¼€æ–‡æœ¬ä½†æœªæ‰¾åˆ°æŒ‰é’®ï¼Œå¼€å§‹è°ƒè¯•DOMç»“æ„...")
                        
                        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŒ‰é’®å…ƒç´ 
                        all_buttons = element.locator('button, [role="button"], span[role="button"]')
                        button_count = await all_buttons.count()
                        self.logger.info(f"ğŸ” æ¨æ–‡ä¸­æ€»å…±æ‰¾åˆ° {button_count} ä¸ªæŒ‰é’®/å¯ç‚¹å‡»å…ƒç´ ")
                        
                        for j in range(min(button_count, 5)):  # åªæ£€æŸ¥å‰5ä¸ª
                            try:
                                btn = all_buttons.nth(j)
                                btn_text = await btn.inner_text()
                                btn_html = await btn.inner_html()
                                self.logger.info(f"   æŒ‰é’® {j+1}: æ–‡æœ¬='{btn_text[:50]}' HTML='{btn_html[:100]}...'")
                            except:
                                pass
                    else:
                        self.logger.debug("ğŸ“„ æœ¬æ¡æ¨æ–‡æœªå‘ç°éœ€è¦å±•å¼€çš„é•¿æ¨æ–‡æŒ‰é’®")
                except Exception as e:
                    self.logger.debug(f"è°ƒè¯•DOMç»“æ„å¤±è´¥: {e}")
                        
        except Exception as e:
            self.logger.debug(f"Error in _expand_long_tweet: {e}")
            # æ£€æŸ¥æ˜¯å¦å› ä¸ºå¯¼èˆªé—®é¢˜å¯¼è‡´çš„é”™è¯¯
            try:
                current_url = self.page.url
                if '/status/' in current_url and '/home' not in current_url:
                    self.logger.warning(f"Detected navigation to tweet detail page: {current_url}")
                    await self._recover_from_accidental_navigation('https://x.com/home')
            except:
                pass
    
    async def _extract_quoted_tweet(self, element) -> Optional[str]:
        """Extract quoted tweet content if present."""
        try:
            # æŸ¥æ‰¾å¼•ç”¨æ¨æ–‡çš„å¸¸è§é€‰æ‹©å™¨
            quoted_tweet_selectors = [
                'div[data-testid="quote"]',
                'div[data-testid="quoteTweet"]',
                'div[role="blockquote"]',
                'blockquote',
                # å¼•ç”¨æ¨æ–‡é€šå¸¸åœ¨ç‰¹å®šçš„å®¹å™¨ä¸­
                'div[data-testid="tweet"] div[role="blockquote"]',
                'article div[data-testid="quote"]'
            ]
            
            for selector in quoted_tweet_selectors:
                quoted_element = element.locator(selector)
                if await quoted_element.count() > 0:
                    # æå–å¼•ç”¨æ¨æ–‡çš„æ–‡æœ¬
                    quoted_text = await quoted_element.inner_text()
                    if quoted_text.strip():
                        self.logger.debug(f"Found quoted tweet: {quoted_text[:100]}...")
                        return quoted_text.strip()
            
            return None
            
        except Exception as e:
            self.logger.debug(f"Error extracting quoted tweet: {e}")
            return None
    
    async def _extract_author_info(self, element) -> Dict[str, str]:
        """Extract author information."""
        try:
            # å°è¯•å¤šç§æ–¹å¼æå–ä½œè€…ä¿¡æ¯
            author_selectors = [
                '[data-testid="User-Name"]',
                '[data-testid="UserName"]', 
                'div[data-testid="User-Name"]',
                'span[data-testid="User-Name"]'
            ]
            
            username = "unknown"
            display_name = "Unknown"
            
            for selector in author_selectors:
                user_element = element.locator(selector)
                if await user_element.count() > 0:
                    # æå–ç”¨æˆ·åï¼ˆä»é“¾æ¥ï¼‰
                    username_links = user_element.locator('a[href^="/"]')
                    if await username_links.count() > 0:
                        for i in range(await username_links.count()):
                            link = username_links.nth(i)
                            href = await link.get_attribute('href')
                            if href and href.startswith('/') and not '/status/' in href:
                                username = href.strip('/').split('/')[0]
                                if username and username != 'home' and username != 'search':
                                    self.logger.debug(f"Extracted username: {username}")
                                    break
                    
                    # æå–æ˜¾ç¤ºåç§°ï¼ˆç¬¬ä¸€ä¸ªç²—ä½“æ–‡æœ¬é€šå¸¸æ˜¯æ˜¾ç¤ºåç§°ï¼‰
                    name_elements = user_element.locator('span')
                    if await name_elements.count() > 0:
                        for i in range(await name_elements.count()):
                            span = name_elements.nth(i)
                            text = await span.inner_text()
                            if text.strip() and not text.startswith('@') and len(text.strip()) > 1:
                                display_name = text.strip()
                                self.logger.debug(f"Extracted display name: {display_name}")
                                break
                    
                    # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆä¿¡æ¯å°±é€€å‡º
                    if username != "unknown" or display_name != "Unknown":
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if username == "unknown":
                # æŸ¥æ‰¾ä»»ä½•æŒ‡å‘ç”¨æˆ·æ¡£æ¡ˆçš„é“¾æ¥
                profile_links = element.locator('a[href^="/"]')
                if await profile_links.count() > 0:
                    for i in range(min(5, await profile_links.count())):  # åªæ£€æŸ¥å‰5ä¸ªé“¾æ¥
                        link = profile_links.nth(i)
                        href = await link.get_attribute('href')
                        if href and href.startswith('/') and not any(x in href for x in ['/status/', '/search', '/home', '/i/']):
                            potential_username = href.strip('/').split('/')[0]
                            if potential_username and len(potential_username) > 1:
                                username = potential_username
                                self.logger.debug(f"Extracted username from profile link: {username}")
                                break
            
            # æŸ¥æ‰¾å¤´åƒæ¥éªŒè¯ä½œè€…ä¿¡æ¯
            avatar_url = ""
            avatar_selectors = [
                'img[src*="profile_images"]',
                '[data-testid="UserAvatar-Container-"] img',
                'img[alt*="profile"]'
            ]
            
            for selector in avatar_selectors:
                avatar_element = element.locator(selector)
                if await avatar_element.count() > 0:
                    src = await avatar_element.first.get_attribute('src')
                    if src:
                        avatar_url = src
                        break
            
            return {
                'name': display_name,
                'username': username,
                'id': username,
                'avatar_url': avatar_url
            }
            
        except Exception as e:
            self.logger.warning(f"Failed to extract author info: {e}")
            return {
                'name': 'Unknown',
                'username': 'unknown', 
                'id': 'unknown',
                'avatar_url': ''
            }
    
    async def _extract_timestamp(self, element) -> tuple[str, bool]:
        """Extract tweet timestamp and determine if it's a retweet.
        
        Returns:
            tuple[str, bool]: (timestamp, is_retweet)
            - For original tweets: returns (original_time, False)
            - For retweets: returns (retweet_time, True) - prioritizes retweet time over original
        """
        try:
            # First check if this is a retweet by looking for retweet indicators
            is_retweet = await self._is_retweet(element)
            
            if is_retweet:
                # For retweets, try to get the retweet time (newer time) first
                retweet_time = await self._extract_retweet_timestamp(element)
                if retweet_time:
                    self.logger.debug(f"Extracted retweet timestamp: {retweet_time}")
                    return retweet_time, True
                else:
                    self.logger.debug("Failed to extract retweet time, falling back to original time")
            
            # Extract original tweet time (fallback for retweets or primary for original tweets)
            original_time = await self._extract_original_timestamp(element)
            if original_time:
                return original_time, is_retweet
                    
            # Final fallback to current time
            return datetime.now().isoformat(), is_retweet
            
        except Exception as e:
            self.logger.warning(f"Failed to extract timestamp: {e}")
            return datetime.now().isoformat(), False
    
    async def _is_retweet(self, element) -> bool:
        """Check if the tweet element is a retweet."""
        try:
            # Look for retweet indicators in the tweet
            retweet_indicators = [
                # Text indicators
                ':has-text("Retweeted")',
                ':has-text("è½¬æ¨äº†")',
                ':has-text("reposted")',
                ':has-text("è½¬å‘äº†")',
                # Icon indicators
                '[data-testid="retweet"]',
                'svg[aria-label*="Retweet"]',
                'svg[aria-label*="è½¬æ¨"]',
                # Structure indicators - retweets often have specific DOM patterns
                '[data-testid="socialContext"]',
                # User action indicators
                'span:has-text("retweeted")',
                'span:has-text("è½¬æ¨äº†")',
                # Look for "X retweeted" or "X è½¬æ¨äº†" patterns
                'div:has-text("retweeted")',
                'div:has-text("è½¬æ¨äº†")',
            ]
            
            for indicator in retweet_indicators:
                indicator_element = element.locator(indicator)
                if await indicator_element.count() > 0:
                    # Additional verification - make sure it's not just the retweet button
                    text_content = await indicator_element.first.inner_text()
                    if text_content and any(keyword in text_content.lower() for keyword in ['retweeted', 'è½¬æ¨äº†', 'reposted', 'è½¬å‘äº†']):
                        self.logger.debug(f"Retweet detected with indicator: {indicator}")
                        return True
            
            # Alternative method: check if there are multiple time elements (original + retweet)
            time_elements = element.locator('time[datetime]')
            time_count = await time_elements.count()
            if time_count > 1:
                self.logger.debug(f"Multiple time elements found ({time_count}), likely a retweet")
                return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"Error checking retweet status: {e}")
            return False
    
    async def _extract_retweet_timestamp(self, element) -> Optional[str]:
        """Extract the retweet timestamp (when the retweet action occurred)."""
        try:
            # Strategy 1: Look for time elements in retweet context areas
            retweet_time_selectors = [
                # Time in social context area (where "X retweeted" appears)
                '[data-testid="socialContext"] time[datetime]',
                '[data-testid="socialContext"] ~ * time[datetime]',
                # Time elements associated with retweet indicators
                ':has-text("retweeted") time[datetime]',
                ':has-text("è½¬æ¨äº†") time[datetime]',
                # Sometimes the newer time is the first one in DOM order
                'time[datetime]:first-child',
            ]
            
            for selector in retweet_time_selectors:
                time_element = element.locator(selector)
                if await time_element.count() > 0:
                    datetime_attr = await time_element.first.get_attribute('datetime')
                    if datetime_attr:
                        self.logger.debug(f"Found retweet time with selector: {selector}")
                        return datetime_attr
            
            # Strategy 2: If there are multiple time elements, assume the first/latest one is retweet time
            all_time_elements = element.locator('time[datetime]')
            time_count = await all_time_elements.count()
            
            if time_count > 1:
                # Get all timestamps and find the most recent one (likely the retweet time)
                timestamps = []
                for i in range(time_count):
                    time_elem = all_time_elements.nth(i)
                    datetime_attr = await time_elem.get_attribute('datetime')
                    if datetime_attr:
                        try:
                            parsed_time = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                            timestamps.append((datetime_attr, parsed_time))
                        except:
                            continue
                
                if timestamps:
                    # Sort by time and return the most recent (retweet time)
                    timestamps.sort(key=lambda x: x[1], reverse=True)
                    newest_timestamp = timestamps[0][0]
                    self.logger.debug(f"Selected newest timestamp as retweet time: {newest_timestamp}")
                    return newest_timestamp
            
            return None
            
        except Exception as e:
            self.logger.debug(f"Error extracting retweet timestamp: {e}")
            return None
    
    async def _extract_original_timestamp(self, element) -> Optional[str]:
        """Extract the original tweet timestamp."""
        try:
            # Look for time element - use more specific selectors to avoid duplicates
            time_selectors = [
                'time[datetime]',  # æœ‰datetimeå±æ€§çš„timeå…ƒç´ 
                'a time[datetime]',  # é“¾æ¥å†…çš„timeå…ƒç´ 
                'time:last-child',  # å¯¹äºè½¬å‘ï¼ŒåŸæ¨æ–‡æ—¶é—´é€šå¸¸åœ¨åé¢
                'time'  # å…œåº•é€‰æ‹©å™¨
            ]
            
            for selector in time_selectors:
                time_element = element.locator(selector)
                if await time_element.count() > 0:
                    try:
                        datetime_attr = await time_element.first.get_attribute('datetime')
                        if datetime_attr:
                            return datetime_attr
                    except Exception as e:
                        self.logger.debug(f"Time selector '{selector}' failed: {e}")
                        continue
                    
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨JavaScriptæå–
            timestamp = await element.evaluate("""
                (el) => {
                    const timeElements = el.querySelectorAll('time[datetime]');
                    if (timeElements.length > 0) {
                        // For retweets, the original time is usually the last one
                        return timeElements[timeElements.length - 1].getAttribute('datetime');
                    }
                    return null;
                }
            """)
            
            return timestamp
                    
        except Exception as e:
            self.logger.debug(f"Error extracting original timestamp: {e}")
            return None
    
    async def _extract_media_urls(self, element) -> List[str]:
        """Extract media URLs from tweet."""
        try:
            media_urls = []
            
            # Extract image URLs
            img_elements = element.locator('img[src*="pbs.twimg.com"]')
            img_count = await img_elements.count()
            for i in range(img_count):
                img_element = img_elements.nth(i)
                src = await img_element.get_attribute('src')
                if src and src not in media_urls:
                    media_urls.append(src)
            
            # Extract video thumbnails and URLs
            video_elements = element.locator('video source')
            video_count = await video_elements.count()
            for i in range(video_count):
                video_element = video_elements.nth(i)
                src = await video_element.get_attribute('src')
                if src and src not in media_urls:
                    media_urls.append(src)
            
            return media_urls
        except Exception as e:
            self.logger.warning(f"Failed to extract media URLs: {e}")
            return []
    
    async def _extract_engagement_metrics(self, element) -> Dict[str, int]:
        """Extract engagement metrics (likes, retweets, etc.)."""
        try:
            metrics = {
                'likes': 0,
                'retweets': 0,
                'replies': 0,
                'quotes': 0
            }
            
            # äº’åŠ¨æŒ‰é’®çš„å¤šç§é€‰æ‹©å™¨
            engagement_selectors = {
                'replies': [
                    '[data-testid="reply"]',
                    '[aria-label*="reply"]', 
                    '[aria-label*="Reply"]',
                    'button[aria-label*="å›å¤"]'
                ],
                'retweets': [
                    '[data-testid="retweet"]',
                    '[aria-label*="retweet"]',
                    '[aria-label*="Retweet"]', 
                    'button[aria-label*="è½¬æ¨"]'
                ],
                'likes': [
                    '[data-testid="like"]',
                    '[aria-label*="like"]',
                    '[aria-label*="Like"]',
                    'button[aria-label*="å–œæ¬¢"]'
                ],
                'quotes': [
                    '[data-testid="quote"]',
                    '[aria-label*="quote"]',
                    '[aria-label*="Quote"]'
                ]
            }
            
            for metric_type, selectors in engagement_selectors.items():
                for selector in selectors:
                    buttons = element.locator(selector)
                    if await buttons.count() > 0:
                        try:
                            # å°è¯•ä»æŒ‰é’®æ–‡æœ¬è·å–æ•°å­—
                            button_text = await buttons.first.inner_text()
                            if button_text.strip():
                                number = self._parse_number(button_text)
                                if number > 0:
                                    metrics[metric_type] = number
                                    self.logger.debug(f"Extracted {metric_type}: {number}")
                                    break
                            
                            # å°è¯•ä»aria-labelè·å–æ•°å­—
                            aria_label = await buttons.first.get_attribute('aria-label')
                            if aria_label:
                                # ä»aria-labelä¸­æå–æ•°å­—
                                import re
                                numbers = re.findall(r'\d+', aria_label)
                                if numbers:
                                    number = int(numbers[0])
                                    metrics[metric_type] = number
                                    self.logger.debug(f"Extracted {metric_type} from aria-label: {number}")
                                    break
                                    
                        except Exception as e:
                            self.logger.debug(f"Failed to extract {metric_type}: {e}")
                            continue
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«æ•°å­—çš„spanå…ƒç´ 
            if all(v == 0 for v in metrics.values()):
                # ä½¿ç”¨JavaScriptæ¥æŸ¥æ‰¾åŒ…å«æ•°å­—çš„spanå…ƒç´ ï¼Œå› ä¸ºCSSé€‰æ‹©å™¨ä¸æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼
                number_texts = await element.evaluate("""
                    (el) => {
                        const spans = el.querySelectorAll('span');
                        const numberTexts = [];
                        spans.forEach(span => {
                            const text = span.textContent?.trim();
                            if (text && /^\\d+/.test(text)) {
                                numberTexts.push(text);
                            }
                        });
                        return numberTexts;
                    }
                """)
                span_count = len(number_texts)
                
                for i, text in enumerate(number_texts[:4]):  # æœ€å¤šæ£€æŸ¥4ä¸ªæ•°å­—æ–‡æœ¬
                    number = self._parse_number(text)
                    
                    if number > 0:
                        # æ ¹æ®ä½ç½®æ¨æµ‹æ˜¯ä»€ä¹ˆç±»å‹çš„äº’åŠ¨
                        if i == 0:
                            metrics['replies'] = number
                        elif i == 1:
                            metrics['retweets'] = number
                        elif i == 2:
                            metrics['likes'] = number
                        elif i == 3:
                            metrics['quotes'] = number
            
            self.logger.debug(f"Final engagement metrics: {metrics}")
            return metrics
            
        except Exception as e:
            self.logger.warning(f"Failed to extract engagement metrics: {e}")
            return {
                'likes': 0,
                'retweets': 0,
                'replies': 0,
                'quotes': 0
            }
    
    def _parse_number(self, text: str) -> int:
        """Parse number from text (handles K, M suffixes)."""
        try:
            text = text.strip().replace(',', '')
            if 'K' in text:
                return int(float(text.replace('K', '')) * 1000)
            elif 'M' in text:
                return int(float(text.replace('M', '')) * 1000000)
            else:
                return int(text) if text.isdigit() else 0
        except:
            return 0
    
    def _parse_tweet_timestamp(self, timestamp_str: str) -> datetime:
        """
        Parse tweet timestamp and convert to local timezone for comparison.
        
        Twitter timestamps are usually in ISO format with UTC timezone (Z suffix).
        We need to convert them to local timezone for proper comparison.
        
        Args:
            timestamp_str: ISO format timestamp string (e.g., "2025-07-31T11:40:50.000Z")
            
        Returns:
            datetime object in local timezone (timezone-naive for comparison)
        """
        try:
            # Parse the ISO timestamp
            tweet_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            
            # Convert to local timezone
            local_tweet_time = tweet_time.astimezone()
            
            # Remove timezone info to make it naive for comparison with naive cutoff_time
            local_tweet_time_naive = local_tweet_time.replace(tzinfo=None)
            
            self.logger.debug(f"Parsed timestamp: {timestamp_str} -> UTC: {tweet_time} -> Local: {local_tweet_time_naive}")
            
            return local_tweet_time_naive
            
        except Exception as e:
            self.logger.warning(f"Failed to parse timestamp '{timestamp_str}': {e}")
            # Fallback to current time
            return datetime.now()
    
    def _adjust_scroll_params_by_mode(self):
        """æ ¹æ®æ»šåŠ¨æ¨¡å¼è°ƒæ•´å‚æ•°"""
        if self.scroll_mode == 'fast':
            # å¿«é€Ÿæ¨¡å¼ï¼šæ›´å¤§æ»šåŠ¨è·ç¦»ï¼Œæ›´çŸ­ç­‰å¾…æ—¶é—´
            self.scroll_distance_range = [3000, 4500]
            self.content_wait_range = [1, 2]
            self.bottom_wait_range = [1, 3]
            self.logger.info("ğŸš€ æ»šåŠ¨æ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å¼ - æå‡æ”¶é›†é€Ÿåº¦ï¼Œä½†å¯èƒ½å¢åŠ æ£€æµ‹é£é™©")
        elif self.scroll_mode == 'safe':
            # å®‰å…¨æ¨¡å¼ï¼šä¿å®ˆæ»šåŠ¨è·ç¦»ï¼Œæ›´é•¿ç­‰å¾…æ—¶é—´
            self.scroll_distance_range = [1200, 2000]
            self.content_wait_range = [3, 5]
            self.bottom_wait_range = [4, 8]
            self.logger.info("ğŸ›¡ï¸ æ»šåŠ¨æ¨¡å¼ï¼šå®‰å…¨æ¨¡å¼ - é™ä½æ£€æµ‹é£é™©ï¼Œä½†æ”¶é›†é€Ÿåº¦è¾ƒæ…¢")
        else:
            # å¹³è¡¡æ¨¡å¼ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
            self.logger.info("âš–ï¸ æ»šåŠ¨æ¨¡å¼ï¼šå¹³è¡¡æ¨¡å¼ - å…¼é¡¾é€Ÿåº¦å’Œå®‰å…¨æ€§")
    
    async def _human_like_scroll(self) -> None:
        """Perform configurable human-like scrolling optimized for Twitter's infinite scroll."""
        try:
            # è®°å½•æ»šåŠ¨å‰çš„é¡µé¢é«˜åº¦
            pre_scroll_height = await self.page.evaluate("document.documentElement.scrollHeight")
            
            # ä½¿ç”¨å¯é…ç½®çš„æ»šåŠ¨è·ç¦»
            scroll_distance = random.randint(*self.scroll_distance_range)
            scroll_info = f"æ»šåŠ¨è·ç¦»: {scroll_distance}px (æ¨¡å¼: {self.scroll_mode})"
            
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨é™„è¿‘è§¦å‘æ›´å¤šå†…å®¹åŠ è½½
            await self.page.evaluate(f"""
                window.scrollBy({{
                    top: {scroll_distance},
                    behavior: 'smooth'
                }});
            """)
            
            # ä½¿ç”¨å¯é…ç½®çš„å†…å®¹åŠ è½½ç­‰å¾…
            content_wait = random.uniform(*self.content_wait_range)
            self.logger.debug(f"{scroll_info}, ç­‰å¾…å†…å®¹åŠ è½½: {content_wait:.1f}s")
            await asyncio.sleep(content_wait)
            
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘é¡µé¢åº•éƒ¨ï¼Œå¦‚æœæ˜¯åˆ™é¢å¤–ç­‰å¾…
            is_near_bottom = await self.page.evaluate("""
                () => {
                    const windowHeight = window.innerHeight;
                    const documentHeight = document.documentElement.scrollHeight;
                    const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
                    return (scrollTop + windowHeight) > (documentHeight * 0.8);
                }
            """)
            
            if is_near_bottom:
                bottom_wait = random.uniform(*self.bottom_wait_range)
                self.logger.debug(f"æ¥è¿‘é¡µé¢åº•éƒ¨ï¼Œé¢å¤–ç­‰å¾…: {bottom_wait:.1f}s")
                await asyncio.sleep(bottom_wait)
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½å®Œæˆ
            await self._wait_for_new_content(pre_scroll_height)
            
        except Exception as e:
            self.logger.warning(f"Scrolling failed: {e}")
    
    async def _wait_for_new_content(self, previous_height: int, max_wait: float = 8.0) -> bool:
        """ç­‰å¾…æ–°å†…å®¹åŠ è½½å®Œæˆï¼Œè¿”å›æ˜¯å¦æœ‰æ–°å†…å®¹"""
        try:
            start_time = asyncio.get_event_loop().time()
            check_interval = 0.5
            
            while (asyncio.get_event_loop().time() - start_time) < max_wait:
                current_height = await self.page.evaluate("document.documentElement.scrollHeight")
                
                if current_height > previous_height:
                    self.logger.debug(f"æ£€æµ‹åˆ°æ–°å†…å®¹ï¼šé¡µé¢é«˜åº¦ {previous_height} -> {current_height}")
                    # æ–°å†…å®¹åŠ è½½åå†ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ¸²æŸ“å®Œæˆ
                    await asyncio.sleep(0.8)
                    return True
                
                await asyncio.sleep(check_interval)
            
            self.logger.debug(f"ç­‰å¾… {max_wait}s åæ— æ–°å†…å®¹åŠ è½½")
            return False
            
        except Exception as e:
            self.logger.debug(f"ç­‰å¾…æ–°å†…å®¹å¤±è´¥: {e}")
            return False
    
    async def cleanup(self) -> None:
        """Clean up browser resources."""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            
            self.logger.info("Twitter scraper cleanup completed")
            
        except Exception as e:
            self.logger.warning(f"Cleanup error: {e}")
    
    async def _recover_from_accidental_navigation(self, target_url: str) -> bool:
        """Recover from accidental navigation to tweet detail pages while preserving scroll position."""
        try:
            current_url = self.page.url
            self.logger.info(f"Attempting to recover from accidental navigation: {current_url}")
            
            # æ–¹æ³•1: ä½¿ç”¨æµè§ˆå™¨åé€€æŒ‰é’®ï¼ˆä¼˜å…ˆé€‰æ‹©ï¼Œä¿æŒæ»šåŠ¨ä½ç½®ï¼‰
            if '/status/' in current_url or '/photo/' in current_url:
                try:
                    self.logger.info("Using browser back button to preserve scroll position...")
                    await self.page.go_back(wait_until='domcontentloaded', timeout=10000)
                    await asyncio.sleep(3)  # ç»™é¡µé¢æ›´å¤šæ—¶é—´åŠ è½½
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¿”å›åˆ°ä¸»é¡µæ—¶é—´çº¿
                    recovered_url = self.page.url
                    if '/home' in recovered_url:
                        self.logger.info("âœ… Successfully recovered using browser back button (scroll position preserved)")
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ¿€æ´»Followingæ ‡ç­¾
                        if 'following' in target_url.lower():
                            await asyncio.sleep(2)
                            # éªŒè¯Followingæ ‡ç­¾æ˜¯å¦ä»ç„¶æ¿€æ´»
                            is_following_active = await self._verify_following_timeline()
                            if not is_following_active:
                                self.logger.info("Re-activating Following timeline after recovery...")
                                await self._navigate_to_following_timeline()
                        
                        return True
                    else:
                        self.logger.warning(f"Back button led to unexpected page: {recovered_url}")
                        
                except Exception as e:
                    self.logger.debug(f"Browser back button failed: {e}")
            
            # æ–¹æ³•2: é”®ç›˜å¿«æ·é”®è¿”å›ï¼ˆAlt+Left Arrow æˆ– Cmd+Left Arrowï¼‰
            try:
                self.logger.info("Trying keyboard shortcut to go back...")
                import platform
                if platform.system() == "Darwin":  # macOS
                    await self.page.keyboard.press("Meta+ArrowLeft")
                else:  # Windows/Linux
                    await self.page.keyboard.press("Alt+ArrowLeft")
                
                await asyncio.sleep(3)
                
                current_url_after_shortcut = self.page.url
                if '/home' in current_url_after_shortcut and '/status/' not in current_url_after_shortcut:
                    self.logger.info("âœ… Successfully recovered using keyboard shortcut (scroll position preserved)")
                    return True
                    
            except Exception as e:
                self.logger.debug(f"Keyboard shortcut failed: {e}")
            
            # æ–¹æ³•3: JavaScriptå†å²è®°å½•è¿”å›
            try:
                self.logger.info("Trying JavaScript history.back()...")
                await self.page.evaluate("window.history.back()")
                await asyncio.sleep(3)
                
                current_url_after_js = self.page.url
                if '/home' in current_url_after_js and '/status/' not in current_url_after_js:
                    self.logger.info("âœ… Successfully recovered using JavaScript history.back() (scroll position preserved)")
                    return True
                    
            except Exception as e:
                self.logger.debug(f"JavaScript history.back() failed: {e}")
            
            # æ–¹æ³•4: æœ€åæ‰‹æ®µ - ç›´æ¥å¯¼èˆªï¼ˆä¼šä¸¢å¤±æ»šåŠ¨ä½ç½®ï¼‰
            self.logger.warning("âš ï¸  All scroll-preserving methods failed, using direct navigation (will lose scroll position)")
            try:
                await self.page.goto(target_url, wait_until='domcontentloaded', timeout=15000)
                await asyncio.sleep(3)
                
                # éªŒè¯æ˜¯å¦æˆåŠŸå¯¼èˆª
                final_url = self.page.url
                if '/home' in final_url:
                    self.logger.info(f"Successfully recovered by navigating to {target_url} (scroll position lost)")
                    
                    # å¦‚æœç›®æ ‡æ˜¯Followingé¡µé¢ï¼Œéœ€è¦é‡æ–°åˆ‡æ¢åˆ°Followingæ ‡ç­¾
                    if 'following' in target_url.lower():
                        await asyncio.sleep(2)
                        await self._navigate_to_following_timeline()
                    
                    return True
                else:
                    self.logger.warning(f"Navigation recovery failed: ended up at {final_url}")
                    return False
                    
            except Exception as e:
                self.logger.error(f"Failed to recover navigation: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"Recovery attempt failed: {e}")
            return False
    
    async def _check_and_recover_navigation(self) -> bool:
        """Check if we're still on the correct page and recover if needed."""
        try:
            current_url = self.page.url
            
            # å¦‚æœåœ¨æ¨æ–‡è¯¦æƒ…é¡µï¼Œéœ€è¦æ¢å¤
            if '/status/' in current_url and '/home' not in current_url:
                self.logger.warning(f"Detected navigation to tweet detail page: {current_url}")
                
                # å°è¯•æ¢å¤åˆ°homeé¡µé¢
                recovery_success = await self._recover_from_accidental_navigation('https://x.com/home')
                
                if not recovery_success:
                    self.logger.error("Failed to recover from accidental navigation")
                    return False
                
                return True
            
            # å¦‚æœåœ¨homeé¡µé¢ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°Followingæ ‡ç­¾
            elif '/home' in current_url:
                return True
            
            # å…¶ä»–é¡µé¢ä¹Ÿå°è¯•æ¢å¤
            else:
                self.logger.warning(f"Unexpected page detected: {current_url}")
                return await self._recover_from_accidental_navigation('https://x.com/home')
                
        except Exception as e:
            self.logger.error(f"Navigation check failed: {e}")
            return False
    
    async def _navigate_to_following_timeline(self) -> bool:
        """
        Navigate to Following timeline with improved logic.
        Since both For You and Following use the same URL (https://x.com/home),
        we need to click the Following tab to switch the timeline.
        
        Returns:
            True if navigation successful, False otherwise
        """
        self.logger.info("Navigating to Following timeline...")
        
        try:
            # First ensure we're on the home page
            current_url = self.page.url
            if '/home' not in current_url:
                self.logger.info("Not on home page, navigating there first...")
                await self.page.goto('https://x.com/home', wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
            
            # Wait for the timeline tabs to load
            self.logger.info("Waiting for timeline navigation to load...")
            try:
                await self.page.wait_for_selector('[role="tablist"]', timeout=15000)
                await asyncio.sleep(3)  # Additional wait for JavaScript to render tabs
                self.logger.info("Timeline navigation loaded")
            except:
                self.logger.warning("Timeline navigation tabs not found, continuing...")
            
            # Strategy 1: Try direct tab selection with multiple approaches
            following_clicked = await self._try_click_following_tab()
            
            if following_clicked:
                # Verify we're on Following timeline by checking page content
                await asyncio.sleep(5)  # Wait for content to load
                
                # Check if the Following tab is active/selected
                is_following_active = await self._verify_following_timeline()
                if is_following_active:
                    self.logger.info("Successfully navigated to Following timeline")
                    return True
                else:
                    self.logger.warning("Following tab clicked but timeline may not have switched")
            
            # Strategy 2: If direct clicking failed, try JavaScript approach
            self.logger.info("Trying JavaScript-based Following navigation...")
            js_success = await self._javascript_click_following()
            
            if js_success:
                await asyncio.sleep(5)
                if await self._verify_following_timeline():
                    self.logger.info("JavaScript Following navigation successful")
                    return True
            
            # Strategy 3: Keyboard navigation as last resort
            self.logger.info("Trying keyboard navigation to Following...")
            keyboard_success = await self._keyboard_navigate_following()
            
            if keyboard_success:
                await asyncio.sleep(5)
                if await self._verify_following_timeline():
                    self.logger.info("Keyboard Following navigation successful")
                    return True
            
            self.logger.warning("All Following navigation strategies failed")
            return False
            
        except Exception as e:
            self.logger.error(f"Following timeline navigation failed: {e}")
            return False
    
    async def _try_click_following_tab(self) -> bool:
        """Try to click the Following tab using various selectors."""
        following_selectors = [
            # Most specific selectors first (support multiple languages)
            '[role="tablist"] [role="tab"]:has-text("Following")',
            '[role="tablist"] [role="tab"]:has-text("æ­£åœ¨å…³æ³¨")',
            '[role="tablist"] [role="tab"]:has-text("å…³æ³¨")',
            '[role="tablist"] div:has-text("Following")',
            '[role="tablist"] div:has-text("æ­£åœ¨å…³æ³¨")',
            '[role="tablist"] div:has-text("å…³æ³¨")',
            # Broader selectors
            '[role="tab"]:has-text("Following")',
            '[role="tab"]:has-text("æ­£åœ¨å…³æ³¨")',
            '[role="tab"]:has-text("å…³æ³¨")',
            'a:has-text("Following")',
            'a:has-text("å…³æ³¨")',
            'button:has-text("Following")',
            'button:has-text("å…³æ³¨")',
            # Navigation specific
            'nav [role="tab"]:has-text("Following")',
            'nav [role="tab"]:has-text("å…³æ³¨")',
            'div[data-testid="primaryColumn"] [role="tab"]:has-text("Following")',
            'div[data-testid="primaryColumn"] [role="tab"]:has-text("å…³æ³¨")',
            # Aria labels
            '[aria-label*="Following"]',
            '[aria-label*="æ­£åœ¨å…³æ³¨"]',
            '[aria-label*="å…³æ³¨"]'
        ]
        
        for selector in following_selectors:
            try:
                element = self.page.locator(selector)
                count = await element.count()
                
                if count > 0:
                    self.logger.info(f"Found Following tab with selector: {selector}")
                    await element.first.click()
                    self.logger.info("Following tab clicked successfully")
                    return True
                    
            except Exception as e:
                self.logger.debug(f"Selector '{selector}' failed: {e}")
                continue
        
        return False
    
    async def _javascript_click_following(self) -> bool:
        """Use JavaScript to find and click the Following tab."""
        try:
            result = await self.page.evaluate("""
                () => {
                    // First, let's debug what tabs are actually available
                    const allTabs = Array.from(document.querySelectorAll('[role="tab"]'));
                    const tabInfo = allTabs.map(tab => ({
                        text: tab.textContent?.trim(),
                        selected: tab.getAttribute('aria-selected'),
                        element: tab.tagName
                    }));
                    
                    // Look for Following tab more precisely (support multiple languages)
                    const followingTab = allTabs.find(tab => {
                        const text = tab.textContent?.trim();
                        return text === 'Following' || text === 'æ­£åœ¨å…³æ³¨' || text === 'å…³æ³¨';
                    });
                    
                    if (followingTab) {
                        // Try multiple click methods
                        try {
                            // Method 1: Direct click
                            followingTab.click();
                            return { 
                                success: true, 
                                method: 'direct_click',
                                tabInfo: tabInfo,
                                clicked_text: followingTab.textContent?.trim()
                            };
                        } catch (e1) {
                            try {
                                // Method 2: Mouse event
                                const clickEvent = new MouseEvent('click', { 
                                    bubbles: true, 
                                    cancelable: true,
                                    view: window
                                });
                                followingTab.dispatchEvent(clickEvent);
                                return { 
                                    success: true, 
                                    method: 'mouse_event',
                                    tabInfo: tabInfo,
                                    clicked_text: followingTab.textContent?.trim()
                                };
                            } catch (e2) {
                                try {
                                    // Method 3: Focus and enter
                                    followingTab.focus();
                                    const enterEvent = new KeyboardEvent('keydown', { key: 'Enter' });
                                    followingTab.dispatchEvent(enterEvent);
                                    return { 
                                        success: true, 
                                        method: 'keyboard_enter',
                                        tabInfo: tabInfo,
                                        clicked_text: followingTab.textContent?.trim()
                                    };
                                } catch (e3) {
                                    return { 
                                        success: false, 
                                        error: `All click methods failed: ${e1.message}, ${e2.message}, ${e3.message}`,
                                        tabInfo: tabInfo
                                    };
                                }
                            }
                        }
                    }
                    
                    return { 
                        success: false, 
                        error: 'Following tab not found',
                        tabInfo: tabInfo
                    };
                }
            """)
            
            # Log debug information
            if 'tabInfo' in result:
                self.logger.info(f"Available tabs: {result['tabInfo']}")
            
            if result.get('success'):
                self.logger.info(f"JavaScript Following click successful via {result.get('method')}, clicked: {result.get('clicked_text')}")
                return True
            else:
                self.logger.warning(f"JavaScript Following click failed: {result.get('error')}")
                return False
                
        except Exception as e:
            self.logger.error(f"JavaScript Following navigation error: {e}")
            return False
    
    async def _keyboard_navigate_following(self) -> bool:
        """Use keyboard navigation to reach the Following tab."""
        try:
            # Focus on the page first
            await self.page.keyboard.press('Tab')
            await asyncio.sleep(1)
            
            # Try to navigate to tabs area and then move right
            for _ in range(3):  # Try a few tab navigation attempts
                await self.page.keyboard.press('Tab')
                await asyncio.sleep(0.5)
            
            # Try arrow key navigation (assuming we're in the tab area)
            await self.page.keyboard.press('ArrowRight')
            await asyncio.sleep(1)
            await self.page.keyboard.press('Enter')
            await asyncio.sleep(2)
            
            return True  # Assume success, will be verified by caller
            
        except Exception as e:
            self.logger.error(f"Keyboard Following navigation error: {e}")
            return False
    
    async def _verify_following_timeline(self) -> bool:
        """Verify that we're currently viewing the Following timeline."""
        try:
            # Give page time to update after clicking
            await asyncio.sleep(2)
            
            # Strategy 1: Check if Following tab appears selected/active
            active_selectors = [
                '[role="tablist"] [role="tab"][aria-selected="true"]:has-text("Following")',
                '[role="tablist"] [role="tab"][aria-selected="true"]:has-text("æ­£åœ¨å…³æ³¨")',
                '[role="tablist"] [role="tab"][aria-selected="true"]:has-text("å…³æ³¨")',
                '[role="tablist"] [role="tab"].selected:has-text("Following")',
                '[role="tablist"] [role="tab"].selected:has-text("å…³æ³¨")',
                '[role="tab"][data-testid*="following"]',
            ]
            
            for selector in active_selectors:
                element = self.page.locator(selector)
                if await element.count() > 0:
                    self.logger.info(f"Following tab is active (selector: {selector})")
                    return True
            
            # Strategy 2: Use JavaScript to check DOM state with detailed debugging
            result = await self.page.evaluate("""
                () => {
                    const tabElements = Array.from(document.querySelectorAll('[role="tab"]'));
                    const tabStates = tabElements.map(tab => ({
                        text: tab.textContent?.trim(),
                        selected: tab.getAttribute('aria-selected'),
                        classes: tab.className,
                        href: tab.href || null
                    }));
                    
                    // Look for active Following tab (support multiple languages)
                    const followingTab = tabElements.find(tab => 
                        tab.textContent?.includes('Following') || 
                        tab.textContent?.includes('æ­£åœ¨å…³æ³¨') || 
                        tab.textContent?.includes('å…³æ³¨')
                    );
                    
                    const forYouTab = tabElements.find(tab => 
                        tab.textContent?.includes('For you') || 
                        tab.textContent?.includes('ä¸ºä½ æ¨è')
                    );
                    
                    return {
                        tabStates: tabStates,
                        followingFound: !!followingTab,
                        followingActive: followingTab ? (
                            followingTab.getAttribute('aria-selected') === 'true' ||
                            followingTab.classList.contains('selected') ||
                            followingTab.classList.contains('active')
                        ) : false,
                        forYouFound: !!forYouTab,
                        forYouActive: forYouTab ? (
                            forYouTab.getAttribute('aria-selected') === 'true' ||
                            forYouTab.classList.contains('selected') ||
                            forYouTab.classList.contains('active')
                        ) : false
                    };
                }
            """)
            
            self.logger.info(f"Tab verification result: {result}")
            
            # Only consider it successful if Following is active AND For You is NOT active
            if result.get('followingActive') and not result.get('forYouActive'):
                self.logger.info("Following timeline verified - Following active, For You inactive")
                return True
            elif result.get('forYouActive'):
                self.logger.warning("Still on For You timeline - Following navigation failed")
                return False
            else:
                self.logger.warning("Cannot determine active timeline state")
                return False
            
        except Exception as e:
            self.logger.warning(f"Following timeline verification error: {e}")
            return False
    
    async def _navigate_to_for_you_timeline(self) -> bool:
        """Navigate to For You timeline (usually the default)."""
        self.logger.info("Navigating to For You timeline...")
        
        try:
            # For You is typically the default timeline
            current_url = self.page.url
            if '/home' not in current_url:
                await self.page.goto('https://x.com/home', wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
            
            # For You is usually already selected, but let's try to click it to be sure
            for_you_selectors = [
                '[role="tablist"] [role="tab"]:has-text("For you")',
                '[role="tablist"] [role="tab"]:has-text("ä¸ºä½ æ¨è")',
                '[role="tab"]:has-text("For you")',
                '[role="tab"]:has-text("ä¸ºä½ æ¨è")'
            ]
            
            for selector in for_you_selectors:
                try:
                    element = self.page.locator(selector)
                    if await element.count() > 0:
                        await element.first.click()
                        self.logger.info("For You tab clicked")
                        await asyncio.sleep(3)
                        break
                except:
                    continue
            
            self.logger.info("For You timeline ready")
            return True
            
        except Exception as e:
            self.logger.warning(f"For You timeline navigation error: {e}")
            return True  # Continue anyway as it's usually the default
